#+TITLE: UD730 assignments
-*- eval: (auto-revert-mode 1); -*-
#+TODO: TODO IN-PROGRESS WAITING DONE
#+STARTUP: indent
#+OPTIONS: author:nil

* Table of Contents                                                            :TOC_3_gh:
- [[#assignment-1][Assignment 1]]
  - [[#problem-1][Problem 1]]
  - [[#problem-2][Problem 2]]
  - [[#problem-3][Problem 3]]
  - [[#problem-4][Problem 4]]
  - [[#problem-5][Problem 5]]
  - [[#problem-6][Problem 6]]
- [[#assignment-2][Assignment 2]]
  - [[#problem-1-1][Problem 1]]
  - [[#problem-2-1][Problem 2]]
- [[#assignment-3][Assignment 3]]
  - [[#problem-1-2][Problem 1]]
  - [[#problem-2-2][Problem 2]]
  - [[#problem-3-1][Problem 3]]
  - [[#problem-4-1][Problem 4]]
- [[#assignment-4][Assignment 4]]
  - [[#starter-code][Starter code]]
  - [[#problem-1-3][Problem 1]]
  - [[#problem-2-3][Problem 2]]
    - [[#experiments][experiments]]
- [[#assignment-5][Assignment 5]]
  - [[#starter-code-1][starter code]]
  - [[#problem][Problem]]
- [[#assignment-6][Assignment 6]]
  - [[#starter-code-2][starter code]]
  - [[#problem-1-4][Problem 1]]
  - [[#problem-2-4][Problem 2]]
    - [[#ab][a+b]]
    - [[#c][c]]
  - [[#problem-3-2][Problem 3]]

* Assignment 1
:PROPERTIES:
:CUSTOM_ID: assignment-1
:header-args: :session a1py
:END:

The objective of this assignment is to learn about simple data curation practices, and familiarize you with some of the data we'll be reusing later.
This notebook uses the [[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST]] dataset to be used with python experiments. This dataset is designed to look like the classic [[http://yann.lecun.com/exdb/mnist/][MNIST]] dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST.

#+NAME: imps1
#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import matplotlib.pyplot as plt
  import numpy as np
  import os
  import sys
  import tarfile
  from IPython.display import display, Image
  from scipy import ndimage
  from sklearn.linear_model import LogisticRegression
  from six.moves.urllib.request import urlretrieve
  from six.moves import cPickle as pickle
#+END_SRC

#+RESULTS:

First, we'll download the dataset to our local machine. The dataconsists of characters rendered in a variety of fonts on a 28x28 image.
The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labelled examples. Given thesesizes, it should be possible to train models quickly on any machine.

#+BEGIN_SRC python
  url = 'http://commondatastorage.googleapis.com/books1000/'
  last_percent_reported = None

  def download_progress_hook(count, blockSize, totalSize):
    """A hook to report the progress of a download. This is mostly intended for users with
    slow internet connections. Reports every 1% change in download progress.
    """
    global last_percent_reported
    percent = int(count * blockSize * 100 / totalSize)

    if last_percent_reported != percent:
      if percent % 5 == 0:
        sys.stdout.write("%s%%" % percent)
        sys.stdout.flush()
      else:
        sys.stdout.write(".")
        sys.stdout.flush()

      last_percent_reported = percent

  def maybe_download(filename, expected_bytes, force=False):
    """Download a file if not present, and make sure it's the right size."""
    if force or not os.path.exists(filename):
      print('Attempting to download:', filename) 
      filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)
      print('\nDownload Complete!')
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified', filename)
    else:
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)
  test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)
#+END_SRC

#+RESULTS:


Extract the dataset from the compressed .tar.gz file. This should give you a set of directories, labelled A through J.

#+BEGIN_SRC python
  num_classes = 10
  np.random.seed(133)

  def maybe_extract(filename, force=False):
    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz
    if os.path.isdir(root) and not force:
      # You may override by setting force=True.
      print('%s already present - Skipping extraction of %s.' % (root, filename))
    else:
      print('Extracting data for %s. This may take a while. Please wait.' % root)
      tar = tarfile.open(filename)
      sys.stdout.flush()
      tar.extractall(data_root)
      tar.close()
    data_folders = [
      os.path.join(root, d) for d in sorted(os.listdir(root))
      if os.path.isdir(os.path.join(root, d))]
    if len(data_folders) != num_classes:
      raise Exception(
        'Expected %d folders, one per class. Found %d instead.' % (
          num_classes, len(data_folders)))
    print(data_folders)
    return data_folders
  
  train_folders = maybe_extract(train_filename)
  test_folders = maybe_extract(test_filename)
#+END_SRC

#+RESULTS:

#+NAME: imps2
#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import matplotlib.pyplot as plt
  import numpy as np
  import os
  import sys
  import tarfile
  from IPython.display import display, Image
  from scipy import ndimage
  from sklearn.linear_model import LogisticRegression
  from six.moves.urllib.request import urlretrieve
  from six.moves import cPickle as pickle

  # Config the matplotlib backend as plotting inline in IPython
#+END_SRC

#+RESULTS:

** Problem 1
:PROPERTIES:
    :CUSTOM_ID: problem-1
    :END:

 Let's take a peek at some of the data to make sure it looks sensible.
 Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display.

[[file:notMNIST_large/A/a29ydW5pc2hpLnR0Zg==.png]]

 Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size.
 We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road.
 A few images might not be readable, we'll just skip them.

#+NAME: shapes
#+BEGIN_SRC python
   image_size = 28  # Pixel width and height.
   pixel_depth = 255.0  # Number of levels per pixel.
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  def load_letter(folder, min_num_images, standardize=True, dtype = float):
    """Load the data for a single letter label."""
    image_files = os.listdir(folder)
    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=dtype)
    print(folder)
    num_images = 0
    for image in image_files:
      image_file = os.path.join(folder, image)
      try:
        if standardize:
          image_data = (ndimage.imread(image_file).astype(dtype) - 
                        pixel_depth / 2) / pixel_depth
        else:
          image_data = ndimage.imread(image_file).astype(dtype)
        if image_data.shape != (image_size, image_size):
          raise Exception('Unexpected image shape: %s' % str(image_data.shape))
        dataset[num_images, :, :] = image_data
        num_images = num_images + 1
      except IOError as e:
        print('Could not read:', image_file, ':', e, '- it\'s ok, skipping.')
        pass
    dataset = dataset[0:num_images, :, :]
    if num_images < min_num_images:
      raise Exception('Many fewer images than expected: %d < %d' %
                      (num_images, min_num_images))
    print('Full dataset tensor:', dataset.shape)
    print('Mean:', np.mean(dataset))
    print('Standard deviation:', np.std(dataset))
    return dataset

  def maybe_pickle(data_folders, min_num_images_per_class, force=False, instance_name='', standardize = True, dtype = np.float32):
    dataset_names = []
    for folder in data_folders:
      set_filename = folder + instance_name + '.pickle'
      dataset_names.append(set_filename)
      if os.path.exists(set_filename) and not force:
        # You may override by setting force=True.
        print('%s already present - Skipping pickling.' % set_filename)
      else:
        print('Pickling %s.' % set_filename)
        dataset = load_letter(folder, min_num_images_per_class, standardize, dtype)
        try:
          with open(set_filename, 'wb') as f:
            pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)
        except Exception as e:
          print('Unable to save data to', set_filename, ':', e)
    return dataset_names
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  # train_datasets = maybe_pickle(train_folders, 45000)
  # test_datasets = maybe_pickle(test_folders, 1800)
#+END_SRC

#+RESULTS:
: None


#+BEGIN_SRC python
  # train_datasets = ["./notMNIST_large/"+nm for nm in sorted(filter(lambda n: n[-6:] == "pickle",os.listdir("./notMNIST_large")))]
  # test_datasets = ["./notMNIST_small/"+nm for nm in sorted(filter(lambda n: n[-6:] == "pickle",os.listdir("./notMNIST_small")))]
#+END_SRC

 #+RESULTS:

To avoid standardizing and use uint8 encoding:
#+BEGIN_SRC python
  train_datasets = maybe_pickle(train_folders, 45000, instance_name='int8', standardize=False,
                                dtype=np.uint8)
  test_datasets = maybe_pickle(test_folders, 1800, instance_name='int8', standardize=False,
                               dtype=np.uint8)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  train_datasets = ["./notMNIST_large/"+nm for nm in sorted(filter(lambda n: n[-10:] == "int8.pickle",os.listdir("./notMNIST_large")))]
  test_datasets = ["./notMNIST_small/"+nm for nm in sorted(filter(lambda n: n[-10:] == "int8.pickle",os.listdir("./notMNIST_small")))]
#+END_SRC

 #+RESULTS:


** Problem 2
:PROPERTIES:
:CUSTOM_ID: problem-2
:END:
#+BEGIN_SRC python
  try:
      os.stat('imgs')
  except:
      os.mkdir('imgs')       
#+END_SRC

#+RESULTS:
: os.stat_result(st_mode=16893, st_ino=5324802, st_dev=47, st_nlink=2, st_uid=1000, st_gid=1000, st_size=4096, st_atime=1496539788, st_mtime=1496539655, st_ctime=1496539655)

#+BEGIN_SRC python :results file
  letter_ix = np.random.randint(len(train_datasets))
  print(letter_ix)
  pickle_file = train_datasets[letter_ix]
  with open(pickle_file, 'rb') as f:
      letter_set = pickle.load(f)  # unpickle
      sample_idx = np.random.randint(len(letter_set))  # pick a random image index
      sample_image = letter_set[sample_idx, :, :]  # extract a 2D slice
      plt.figure()
      plt.imshow(sample_image)  # display it
      plt.savefig('imgs/sample.png')

  'imgs/sample.png'
#+END_SRC

#+RESULTS:
[[file:imgs/sample.png]]


** Problem 3
    :PROPERTIES:
    :CUSTOM_ID: problem-3
    :END:

 Another check: we expect the data to be balanced across classes. Verify
 that.

 #+BEGIN_SRC python
   for nm in (filter(lambda n: n[-6:] == "pickle",os.listdir("./notMNIST_large"))):
       f = open("./notMNIST_large/"+nm, 'rb')
       letter_set = pickle.load(f)
       print(letter_set.shape[0])
 #+END_SRC

 #+RESULTS:

 Merge and prune the training data as needed. Depending on your computer
 setup, you might not be able to fit it all in memory, and you can tune
 =train_size= as needed. The labels will be stored into a separate array
 of integers 0 through 9.

 Also create a validation dataset for hyperparameter tuning.

 #+BEGIN_SRC python
   def make_arrays(nb_rows, img_size):
     if nb_rows:
       dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)
       labels = np.ndarray(nb_rows, dtype=np.int32)
     else:
       dataset, labels = None, None
     return dataset, labels

   def merge_datasets(pickle_files, train_size, valid_size=0):
     num_classes = len(pickle_files)
     valid_dataset, valid_labels = make_arrays(valid_size, image_size)
     train_dataset, train_labels = make_arrays(train_size, image_size)
     vsize_per_class = valid_size // num_classes
     tsize_per_class = train_size // num_classes
     start_v, start_t = 0, 0
     end_v, end_t = vsize_per_class, tsize_per_class
     end_l = vsize_per_class+tsize_per_class
     for label, pickle_file in enumerate(pickle_files):       
       try:
         with open(pickle_file, 'rb') as f:
           letter_set = pickle.load(f)
           # let's shuffle the letters to have random validation and training set
           np.random.shuffle(letter_set)
           if valid_dataset is not None:
             valid_letter = letter_set[:vsize_per_class, :, :]
             valid_dataset[start_v:end_v, :, :] = valid_letter
             valid_labels[start_v:end_v] = label
             start_v += vsize_per_class
             end_v += vsize_per_class
           train_letter = letter_set[vsize_per_class:end_l, :, :]
           train_dataset[start_t:end_t, :, :] = train_letter
           train_labels[start_t:end_t] = label
           start_t += tsize_per_class
           end_t += tsize_per_class
       except Exception as e:
         print('Unable to process data from', pickle_file, ':', e)
         raise
     return valid_dataset, valid_labels, train_dataset, train_labels
#+END_SRC

#+BEGIN_SRC python
   train_size = 200000
   valid_size = 10000
   test_size = 10000
   valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(train_datasets,
                                                                             train_size, 
                                                                             valid_size)
#+END_SRC

#+BEGIN_SRC python
   _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
   print('Training:', train_dataset.shape, train_labels.shape)
   print('Validation:', valid_dataset.shape, valid_labels.shape)
   print('Testing:', test_dataset.shape, test_labels.shape)
 #+END_SRC

 #+RESULTS:
 : None

 Next, we'll randomize the data. It's important to have the labels well
 shuffled for the training and test distributions to match.

 #+BEGIN_SRC python
   def randomize(dataset, labels):
     permutation = np.random.permutation(labels.shape[0])
     shuffled_dataset = dataset[permutation,:,:]
     shuffled_labels = labels[permutation]
     return shuffled_dataset, shuffled_labels
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
   train_dataset, train_labels = randomize(train_dataset, train_labels)
   test_dataset, test_labels = randomize(test_dataset, test_labels)
   valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)
 #+END_SRC

 #+RESULTS:
 : None


** Problem 4
:PROPERTIES:
:CUSTOM_ID: problem-4
:END:

Convince yourself that the data is still good after shuffling!

#+BEGIN_SRC python :results output
  sample_idx = np.random.randint(len(train_dataset))  # pick a random image index
  sample_image = train_dataset[sample_idx, :, :]  # extract a 2D slice
  plt.figure()
  plt.imshow(sample_image)  # display it
  plt.savefig('./imgs/sample2.png')

  train_labels[sample_idx]
#+END_SRC

#+RESULTS:
: 
: >>> <matplotlib.figure.Figure object at 0x7f585fd7ef98>
: <matplotlib.image.AxesImage object at 0x7f585fcf2198>
: 3

[[./imgs/sample2.png]]     

 Finally, let's save the data for later reuse:

#+BEGIN_SRC python
  pickle_file = 'notMNIST.pickle'

  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_dataset': train_dataset,
      'train_labels': train_labels,
      'valid_dataset': valid_dataset,
      'valid_labels': valid_labels,
      'test_dataset': test_dataset,
      'test_labels': test_labels,
      }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  statinfo = os.stat(pickle_file)
  print('Compressed pickle size:', statinfo.st_size)
#+END_SRC

 #+RESULTS:
 : None

#+NAME: load_int8_pkl
#+BEGIN_SRC python 
  pickle_file = 'notMNISTint8.pickle'
  f = open(pickle_file, 'rb')
  pkl = pickle.load(f)
  test_labels = pkl["test_labels"]
  valid_labels = pkl["valid_labels"]
  valid_dataset = pkl["valid_dataset"]
  train_labels = pkl["train_labels"]
  test_dataset = pkl["test_dataset"]
  train_dataset = pkl["train_dataset"]
  f.close()
  image_size = 28  # Pixel width and height.
  pixel_depth = 255.0  # Number of levels per pixel.
#+END_SRC

 #+RESULTS:



** Problem 5
:PROPERTIES:
:CUSTOM_ID: problem-5
:END:

By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it. Measure how much overlap there is between training, validation and test samples.

Optional questions:

- What about near duplicates between datasets? (images that are almost identical)

- Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments.

#+NAME: radius
#+BEGIN_SRC python
  radius = 2**4
#+END_SRC

#+RESULTS: radius

**** broadcast + expand l2

***** get_edges

****** numpy

#+BEGIN_SRC python :var pre1=imps1 pre2=imps2 pre3=load_int8_pkl pre4=shapes pre5=radius
  from math import ceil
#+END_SRC

#+RESULTS:

test
#+BEGIN_SRC python
A = test_dataset.reshape([test_dataset.shape[0], -1])
r = (A*A).sum(axis=1)
r = r.reshape([-1,1])
D = r-2*np.matmul(A,A.T)+r.T
E = np.where(D<radius**2)
E = (np.vstack(E).T)[[E[0]<E[1]]]
#+END_SRC

#+RESULTS:
: 256

#+BEGIN_SRC python
  ix=np.random.randint(E.shape[0])
  np.sum(np.square(test_dataset[E[ix,1],:,:]-test_dataset[E[ix,0],:,:]))
#+END_SRC

#+RESULTS:
: 0.0

#+BEGIN_SRC python
  def get_edges(data):
    N = data.shape[0]
    data = data.reshape([N,-1])
    T = 2**15 # slice length
    def slice_edges(ix1,ix2):
      A=data[ix1*T:(ix1+1)*T,:]
      B=data[ix2*T:(ix2+1)*T,:]
      r_A = (A*A).sum(axis=1).reshape([-1,1])
      r_B = (B*B).sum(axis=1).reshape([-1,1])
      D = r_A-2*np.matmul(A,B.T)+r_B.T
      E = np.where(D<radius**2)
      return (np.vstack(E).T)[E[0]+ix1*T<E[1]+ix2*T,:]+np.array([[ix1,ix2]])*T
    E_all = np.empty(shape=(0,2), dtype=np.int32)
    for i in range(ceil(N/T)):
      for j in range(i,ceil(N/T)):
        E_new = slice_edges(i,j)
        E_all = np.vstack([E_all, E_new])
        print("finished iteration i:{}, j:{}. Found {} edges.".format(i,j,len(E_new)))
    return E_all
#+END_SRC


****** IN-PROGRESS tflow

#+BEGIN_SRC python :var pre1=imps1 pre2=imps2 pre3=load_int8_pkl pre4=shapes pre5=radius
  import tensorflow as tf
  from math import ceil
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  import pdb
  tf.reset_default_graph()  
  def get_edges(dataset):
    g = tf.Graph()
    N = dataset.shape[0]
    T = 2**16 # slice length
    with g.as_default():`
      data = tf.constant(dataset.reshape([N, -1]), dtype=tf.int32)
      slice_ix1 = tf.placeholder(dtype = tf.int32, shape=())
      slice_ix2 = tf.placeholder(dtype = tf.int32, shape=())
      A=data[slice_ix1*T:(slice_ix1+1)*T,:]
      B=data[slice_ix2*T:(slice_ix2+1)*T,:]
      r_A = tf.reduce_sum(A*A, 1)
      r_B = tf.reduce_sum(B*B, 1)
      # turn r into column vector
      r_A = tf.reshape(r_A, [-1, 1])
      r_B = tf.reshape(r_B, [-1, 1])
      D = r_A - 2*tf.matmul(A, tf.transpose(B)) + tf.transpose(r_B)
      E = tf.where(tf.less_equal(D,radius**2))
    sess = tf.Session(graph=g)
    all_edges = np.empty(shape=(0,2))
    for i in range(ceil(N/T)):
      for j in range(i,ceil(N/T)):
        edges = sess.run(E, feed_dict = {slice_ix1:i, slice_ix2:j})
        pdb.set_trace()
        all_edges = np.vstack([all_edges,
                               edges[edges[:,0]+i*T<edges[:,1]+j*T,:]+np.array([[i,j]])*T])
        print("finished iteration i:{}, j:{}. Found {} edges.".format(i,j,len(edges)))
    return all_edges
#+END_SRC

#+RESULTS:
: 1.06335e+07


***** post process

#+BEGIN_SRC python
  train_edges = get_edges(train_dataset)
  test_edges = get_edges(test_dataset)
  valid_edges = get_edges(valid_dataset)
#+END_SRC

#+RESULTS:

test
#+BEGIN_SRC python
  E = np.int32(train_edges)
  data = train_dataset
  ix=np.random.randint(E.shape[0])
  np.sum(np.square(data[E[ix,1],:,:]-data[E[ix,0],:,:]))
#+END_SRC

#+RESULTS:
: 0.0


#+BEGIN_SRC python
  pickle_file = 'edges_r_2p4.pickle'
  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_edges':train_edges,
      'test_edges':test_edges,
      'valid_edges':valid_edges
    }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise  
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  pickle_file = 'edges_r_2p4.pickle'
  f = open(pickle_file, 'rb')
  pkl = pickle.load(f)
  train_edges = pkl["train_edges"]
  test_edges = pkl["test_edges"]
  valid_edges = pkl["valid_edges"]
  f.close()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  from scipy.sparse import csc_matrix
  train_A = csc_matrix((np.ones(len(train_edges)),
                        (train_edges[:,0], train_edges[:,1])), 
                       shape=(len(train_dataset),len(train_dataset)))
  test_A = csc_matrix((np.ones(len(test_edges)),
                       (test_edges[:,0], test_edges[:,1])), 
                      shape=(len(test_dataset),len(test_dataset)))
  valid_A = csc_matrix((np.ones(len(valid_edges)),
                        (valid_edges[:,0], valid_edges[:,1])), 
                       shape=(len(valid_dataset),len(valid_dataset)))
#+END_SRC

#+RESULTS:

test
#+BEGIN_SRC python
A = train_A
data = train_dataset
coo_A = A.tocoo()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
ix = np.random.randint(A.nnz)
np.sum(np.square(data[coo_A.row[ix]]-data[coo_A.col[ix]]))
#+END_SRC

#+RESULTS:
: 0.0


#+BEGIN_SRC python
  from scipy.sparse.csgraph import connected_components
  import pandas as pd 

  def get_groups(A):
    n_comp, index_labels = connected_components(A, directed=False, return_labels=True)
    comp_labels, comp_first, comp_counts = np.unique(index_labels, return_index=True, 
                                                     return_inverse=False, 
                                                     return_counts=True)
    comp_labels = comp_labels[comp_counts>1]  # non-trivial components
    index_labels = np.vstack([np.arange(len(index_labels)), index_labels]).T
    # filter out trivial:
    index_labels = index_labels[np.in1d(index_labels[:,1], comp_labels),:]
    return pd.Series(index_labels[:,0]).groupby(index_labels[:,1]), comp_first
#+END_SRC

#+RESULTS:

tests
#+BEGIN_SRC python
  group_obj, group_firsts = get_groups(train_A)
  data = train_dataset
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  group_counts = group_obj.count()
  gkey = np.random.choice(group_counts.index, p=group_counts.values/sum(group_counts.values))
  print(gkey)
  ixs = np.random.choice(group_obj.get_group(gkey),2,replace=False)
  np.sum(np.square(data[ixs[0]]-data[ixs[1]]))
#+END_SRC

#+RESULTS:
: 0.0

#+name: plt-save
#+begin_src python :exports results :results verbatim
files = []
path='imgs/compare'
for i in range(len(ixs)):
    plt.figure(i)
    plt.imshow(data[ixs[i]])
    files.append('{0}_{1}.png'.format(path, i))
    plt.savefig(files[-1], bbox_inches='tight')

"\n".join(["[[file:{0}]]".format(f) for f in files])
#+end_src

#+RESULTS: plt-save
[[file:imgs/compare_0.png]]
[[file:imgs/compare_1.png]]

#+BEGIN_SRC python
  pickle_file = 'groups_firsts_r_2p4.pickle'
  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_groups_firsts':get_groups(train_A),
      'test_groups_firsts':get_groups(test_A),
      'valid_groups_firsts':get_groups(valid_A)
    }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise  
#+END_SRC

#+RESULTS:

load
#+BEGIN_SRC python
  from six.moves import cPickle as pickle
  pickle_file = 'groups_firsts_r_2p4.pickle'
  with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_groups_firsts = save['train_groups_firsts']
    test_groups_firsts = save['test_groups_firsts']
    valid_groups_firsts = save['valid_groups_firsts']
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :results file
  import numpy as np
  import matplotlib.pyplot as plt
  import os
  imgfile = 'imgs/copyhists.png'
  try:
    os.remove(filename)
  except OSError:
    pass  
  plt.figure(1)
  for i, grp in enumerate(zip(['train', 'test', 'valid'],
                              [train_groups_firsts, test_groups_firsts, valid_groups_firsts])):
    plt.subplot(3, 1, i+1)
    plt.hist(grp[1][0].count(), 50, range = (1, 10), log = True)

  plt.savefig(imgfile, bbox_inches='tight')
  
  imgfile
#+END_SRC

#+RESULTS:
[[file:imgs/copyhists.png]]
Lots of small groups of similar images at radius 2^4

#+BEGIN_SRC python :results output
  for a, b in zip(['train', 'test', 'valid'], [train_groups_firsts, test_groups_firsts, valid_groups_firsts]):
    print(a+': {}'.format(np.sort((b[0].count()))[-10:]))

#+END_SRC

#+RESULTS:
: 
: ... train: [  16   17   22   24   25   29   33   46   70 2085]
: test: [  2   2   2   2   2   2   2   2   2 142]
: valid: [  2   2   2   2   2   2   2   2   3 110]

1 very large group


**** IN-PROGRESS digitize and cluster

#+BEGIN_SRC python
train_dataset = train_dataset.reshape([train_dataset.shape[0],-1])
train_dist = sklearn.metrics.pairwise.pairwise_distances(train_dataset, train_dataset, n_jobs = 8)
valid_dataset = valid_dataset.reshape([valid_dataset.shape[0],-1])
test_dataset = test_dataset.reshape([test_dataset.shape[0],-1])
#+END_SRC

#+RESULTS:
| 10 | 28 | 28 |

_warning_: consumes lots of memory and should probably be done in sql.
#+BEGIN_SRC python
  from math import ceil
  import pandas as pd  # for groupby
  bins = np.arange(ceil((pixel_depth+1)/radius))*radius
  train_bins = pd.RangeIndex(train_dataset.shape[0]).groupby(
    pd.Series(map(tuple, np.digitize(train_dataset.reshape([train_dataset.shape[0],-1]), 
                                     bins, right=False)-1)))
  test_bins = pd.RangeIndex(test_dataset.shape[0]).groupby(
    pd.Series(map(tuple, np.digitize(test_dataset.reshape([test_dataset.shape[0],-1]), 
                                     bins, right=False)-1)))
  valid_bins = pd.RangeIndex(valid_dataset.shape[0]).groupby(
    pd.Series(map(tuple, np.digitize(valid_dataset.reshape([valid_dataset.shape[0],-1]), 
                                     bins, right=False)-1)))
#+END_SRC

#+RESULTS:

Based on Fixed-Radius Near Neighbor on the Line by Bucketing, for example as described [[www.cs.wustl.edu/~pless/546/lectures/Lecture2.pdf][here]].
#+BEGIN_SRC python
  from scipy.sparse import csc_matrix
  import pdb
  def get_adjmx(dataset, bins):
    A = csc_matrix((dataset.shape[0], dataset.shape[0]), dtype=bool)
    keys = bins.keys()
    def find_neighbors(bin_orig, vec_length, delta):
      if len(delta) == vec_length and np.sum(np.abs(delta)) != 0:      
        if tuple(bin_orig+delta) in keys:
          bin_new = bins[bin_orig+delta]
          for e_0 in bin_orig:
            for e_1 in bin_new:
              A[min(e_0,e_1),max(e_0,e_1)] = np.sum(np.abs(dataset[e_0,:]-dataset[e_1,:]))<=radius
      elif len(delta) < vec_length:
        for d in [0,-1,1]:
          find_neighbors(bin_orig, vec_length, np.concatenate([delta,[d]]))
    i = 0      
    for b in keys:
      if i % 1 == 0:
        print("on key #"+str(i))
      find_neighbors(b, len(b), [])
      i+=1
    return A

  A_train = get_adjmx(train_dataset, train_bins)
  A_valid = get_adjmx(valid_dataset, valid_bins)
  A_test = get_adjmx(test_dataset, test_bins)
#+END_SRC

... This takes too long. Probably has a bug.


#+BEGIN_SRC julia :session a1jl
using PyCall
@pyimport pickle
pickle_file = "notMNISTint8.pickle"
fid = open(pickle_file,"r")
data = pickle.load(fid)
close(fid)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC julia
convert(Array{UInt8,3},data["test_dataset"])
#+END_SRC


** Problem 6
:PROPERTIES:
:CUSTOM_ID: problem-6
:END:

Let's get an idea of what an off-the-shelf classifier can give you on
this data. It's always good to check that there is something to learn,
and that it's a problem that is not so trivial that a canned solution
solves it.

Train a simple model on this data using 50, 100, 1000 and 5000 training
samples. Hint: you can use the LogisticRegression model from
sklearn.linear\_model.

Optional question: train an off-the-shelf model on all the data!

#+BEGIN_SRC python
    import pickle
    pickle_file = 'eql_lsts.pickle'
    eql_lsts = np.load(pickle_file)
    apx_eql_lst = eql_lsts["apx_lst"]
#+END_SRC

#+BEGIN_SRC python
    pkl = np.load('notMNIST.pickle',mmap_mode='r')
    test_labels = pkl["test_labels"]
    valid_labels = pkl["valid_labels"]
    valid_dataset = pkl["valid_dataset"]
    train_labels = pkl["train_labels"]
    test_dataset = pkl["test_dataset"]
    train_dataset = pkl["train_dataset"]
#+END_SRC

#+BEGIN_SRC python
    import itertools
    import random
    import sklearn.linear_model
    bad_train_ix = map(lambda x: x[0], apx_eql_lst)
    good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                range(train_dataset.shape[0])))
#+END_SRC

#+BEGIN_SRC python
    def reservoir_sampling(iterable, r=1):
        "Random selection from itertools.permutations(iterable, r)"
        it = iter(iterable)
        R = [next(it) for i in range(r)]
        for i, item in enumerate(it, start=r+1):
          j = random.randrange(i)
          if j<r:
            R[j] = item
        return R
#+END_SRC

#+BEGIN_SRC python
    sample_size = 20000
    sample_train_ix = reservoir_sampling(good_train_ix, sample_size)
    logreg = sklearn.linear_model.LogisticRegression()
#+END_SRC

#+BEGIN_SRC python
    m = train_dataset.shape[1]*train_dataset.shape[2]
    X = train_dataset[sample_train_ix].reshape(sample_size,m)
    y = train_labels[sample_train_ix]
    M = logreg.fit(X,y)
#+END_SRC

#+BEGIN_SRC python
    X_hat = test_dataset.reshape(test_dataset.shape[0],m)
    y_hat = test_labels
    L = M.score(X_hat, y_hat)
    print(L)
#+END_SRC

#+BEGIN_EXAMPLE
    0.7081
#+END_EXAMPLE


* Assignment 2
:PROPERTIES:
:CUSTOM_ID: assignment-2
:header-args: :session a2py
:END:

Previously in =1_notmnist.ipynb=, we created a pickle with formatted
datasets for training, development and testing on the
[[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST
dataset]].

The goal of this assignment is to progressively train deeper and more
accurate models using TensorFlow.

#+BEGIN_SRC python
     # These are all the modules we'll be using later. Make sure you can import them
     # before proceeding further.
     from __future__ import print_function
     import numpy as np
     import tensorflow as tf
     from six.moves import cPickle as pickle
     from six.moves import range
#+END_SRC

#+RESULTS:

First reload the data we generated in =1_notmnist.ipynb=.

#+BEGIN_SRC python
     pickle_file = 'notMNIST.pickle'

     with open(pickle_file, 'rb') as f:
       save = pickle.load(f)
       train_dataset = save['train_dataset']
       train_labels = save['train_labels']
       valid_dataset = save['valid_dataset']
       valid_labels = save['valid_labels']
       test_dataset = save['test_dataset']
       test_labels = save['test_labels']
       del save  # hint to help gc free up memory
       print('Training set', train_dataset.shape, train_labels.shape)
       print('Validation set', valid_dataset.shape, valid_labels.shape)
       print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+BEGIN_EXAMPLE
  Training set (200000, 28, 28) (200000,)
  Validation set (10000, 28, 28) (10000,)
  Test set (10000, 28, 28) (10000,)
#+END_EXAMPLE

Reformat into a shape that's more adapted to the models we're going to
train:

-  data as a flat matrix,
-  labels as float 1-hot encodings.

#+BEGIN_SRC python
  image_size = 28
  num_labels = 10

  def reformat(dataset, labels):
    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels
  train_dataset, train_labels = reformat(train_dataset, train_labels)
  valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
  test_dataset, test_labels = reformat(test_dataset, test_labels)
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+BEGIN_EXAMPLE
     Training set (200000, 784) (200000, 10)
     Validation set (10000, 784) (10000, 10)
     Test set (10000, 784) (10000, 10)
#+END_EXAMPLE

We're first going to train a multinomial logistic regression using
simple gradient descent.

TensorFlow works like this:

-  First you describe the computation that you want to see performed:
what the inputs, the variables, and the operations look like. These
get created as nodes over a computation graph. This description is
all contained within the block below:

with graph.as\_default():\\
...

-  Then you can run the operations on this graph as many times as you
want by calling =session.run()=, providing it outputs to fetch from
the graph that get returned. This runtime operation is all contained
in the block below:

with tf.Session(graph=graph) as session:\\
...

Let's load all the data into TensorFlow and build the computation graph
corresponding to our training:

#+BEGIN_SRC python
     # With gradient descent training, even this much data is prohibitive.
     # Subset the training data for faster turnaround.
     train_subset = 10000

     graph = tf.Graph()
     with graph.as_default():

       # Input data.
       # Load the training, validation and test data into constants that are
       # attached to the graph.
       tf_train_dataset = tf.constant(train_dataset[:train_subset, :])
       tf_train_labels = tf.constant(train_labels[:train_subset])
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       # These are the parameters that we are going to be training. The weight
       # matrix will be initialized using random values following a (truncated)
       # normal distribution. The biases get initialized to zero.
       weights = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_labels]))
       biases = tf.Variable(tf.zeros([num_labels]))
      
       # Training computation.
       # We multiply the inputs with the weight matrix, and add biases. We compute
       # the softmax and cross-entropy (it's one operation in TensorFlow, because
       # it's very common, and it can be optimized). We take the average of this
       # cross-entropy across all training examples: that's our loss.
       logits = tf.matmul(tf_train_dataset, weights) + biases
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
      
       # Optimizer.
       # We are going to find the minimum of this loss using gradient descent.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       # These are not part of training, but merely here so that we can report
       # accuracy figures as we train.
       train_prediction = tf.nn.softmax(logits)
       valid_prediction = tf.nn.softmax(
         tf.matmul(tf_valid_dataset, weights) + biases)
       test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)
#+END_SRC

Let's run this computation and iterate:

#+BEGIN_SRC python
     num_steps = 801
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
#+END_SRC

#+BEGIN_SRC python
  with tf.Session(graph=graph) as session:
    # This is a one-time operation which ensures the parameters get initialized as
    # we described in the graph: random weights for the matrix, zeros for the
    # biases. 
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(num_steps):
      # Run the computations. We tell .run() that we want to run the optimizer,
      # and get the loss value and the training predictions returned as numpy
      # arrays.
      _, l, predictions = session.run([optimizer, loss, train_prediction])
      if (step % 100 == 0):
        print('Loss at step %d: %f' % (step, l))
        print('Training accuracy: %.1f%%' % accuracy(
          predictions, train_labels[:train_subset, :]))
        # Calling .eval() on valid_prediction is basically like calling run(), but
        # just to get that one numpy array. Note that it recomputes all its graph
        # dependencies.
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+BEGIN_EXAMPLE
  Initialized
  Loss at step 0: 22.018156
  Training accuracy: 6.6%
  Validation accuracy: 8.6%
  Loss at step 100: 2.022280
  Training accuracy: 75.2%
  Validation accuracy: 73.7%
  Loss at step 200: 1.623059
  Training accuracy: 78.1%
  Validation accuracy: 75.7%
  Loss at step 300: 1.408173
  Training accuracy: 79.2%
  Validation accuracy: 76.5%
  Loss at step 400: 1.262911
  Training accuracy: 80.0%
  Validation accuracy: 76.6%
  Loss at step 500: 1.154954
  Training accuracy: 80.7%
  Validation accuracy: 76.8%
  Loss at step 600: 1.070023
  Training accuracy: 81.2%
  Validation accuracy: 77.0%
  Loss at step 700: 1.000842
  Training accuracy: 81.7%
  Validation accuracy: 77.1%
  Loss at step 800: 0.943042
  Training accuracy: 82.2%
  Validation accuracy: 77.2%
  Test accuracy: 67.9%
#+END_EXAMPLE

Let's now switch to stochastic gradient descent training instead, which
is much faster.

The graph will be similar, except that instead of holding all the
training data into a constant node, we create a =Placeholder= node which
will be fed actual data at every call of =session.run()=.

#+BEGIN_SRC python
  batch_size = 128
  graph = tf.Graph()
  with graph.as_default():

    # Input data. For the training data, we use a placeholder that will be fed
    # at run time with a training minibatch.
    tf_train_dataset = tf.placeholder(tf.float32,
                                      shape=(batch_size, image_size * image_size))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)

    # Variables.
    weights = tf.Variable(
      tf.truncated_normal([image_size * image_size, num_labels]))
    biases = tf.Variable(tf.zeros([num_labels]))

    # Training computation.
    logits = tf.matmul(tf_train_dataset, weights) + biases
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))

    # Optimizer.
    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(
      tf.matmul(tf_valid_dataset, weights) + biases)
    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)
#+END_SRC

Let's run it:

#+BEGIN_SRC python
  num_steps = 3001

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print("Initialized")
    for step in range(num_steps):
      # Pick an offset within the training data, which has been randomized.
      # Note: we could use better randomization across epochs.
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      # Generate a minibatch.
      batch_data = train_dataset[offset:(offset + batch_size), :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      # Prepare a dictionary telling the session where to feed the minibatch.
      # The key of the dictionary is the placeholder node of the graph to be fed,
      # and the value is the numpy array to feed to it.
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
      _, l, predictions = session.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 500 == 0):
        print("Minibatch loss at step %d: %f" % (step, l))
        print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
        print("Validation accuracy: %.1f%%" % accuracy(
          valid_prediction.eval(), valid_labels))
    print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+BEGIN_EXAMPLE
     Initialized
     Minibatch loss at step 0: 16.320572
     Minibatch accuracy: 13.3%
     Validation accuracy: 14.4%
     Minibatch loss at step 500: 1.573464
     Minibatch accuracy: 73.4%
     Validation accuracy: 78.2%
     Minibatch loss at step 1000: 0.979085
     Minibatch accuracy: 81.2%
     Validation accuracy: 79.5%
     Minibatch loss at step 1500: 0.816317
     Minibatch accuracy: 81.2%
     Validation accuracy: 79.8%
     Minibatch loss at step 2000: 1.051737
     Minibatch accuracy: 79.7%
     Validation accuracy: 79.6%
     Minibatch loss at step 2500: 0.977313
     Minibatch accuracy: 76.6%
     Validation accuracy: 80.3%
     Minibatch loss at step 3000: 0.891709
     Minibatch accuracy: 76.6%
     Validation accuracy: 80.3%
     Test accuracy: 69.5%
#+END_EXAMPLE



** Problem 1
:PROPERTIES:
:CUSTOM_ID: problem
:END:

Turn the logistic regression example with SGD into a 1-hidden layer
neural network with rectified linear units
[[https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu][nn.relu()]]
and 1024 hidden nodes. This model should improve your validation / test
accuracy.



#+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)

       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     num_steps = 3001
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
         # Generate a minibatch.
         batch_data = train_dataset[offset:(offset + batch_size), :]
         batch_labels = train_labels[offset:(offset + batch_size), :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Initialized
     Minibatch loss at step 0: 413.522797
     Minibatch accuracy: 9.4%
     Validation accuracy: 37.4%
     Minibatch loss at step 500: 17.026733
     Minibatch accuracy: 79.7%
     Validation accuracy: 82.2%
     Minibatch loss at step 1000: 3.879690
     Minibatch accuracy: 82.0%
     Validation accuracy: 83.4%
     Minibatch loss at step 1500: 7.037672
     Minibatch accuracy: 85.2%
     Validation accuracy: 82.5%
     Minibatch loss at step 2000: 2.705339
     Minibatch accuracy: 84.4%
     Validation accuracy: 83.2%
     Minibatch loss at step 2500: 3.578274
     Minibatch accuracy: 75.8%
     Validation accuracy: 84.1%
     Minibatch loss at step 3000: 5.462776
     Minibatch accuracy: 77.3%
     Validation accuracy: 83.7%
     Test accuracy: 71.8%
 #+END_EXAMPLE


** Problem 2
:PROPERTIES:
:CUSTOM_ID: redo-previous-proper-sgd-randomize-order
:END:

#+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = map(lambda x: x[0], apx_eql_lst)
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 6001
     # ix list for actual SGD
     def fisher_yates_sampling(iterable):
       "l - random selection from permutations(iterable)"
       l = copy.deepcopy(iterable)
       n = len(l)
       for i in range(n-1):
         j = random.randrange(n-i)
         t = l[i]
         l[i] = l[i+j]
         l[i+j] = l[i]
       return l
     rand_train_ix = fisher_yates_sampling(good_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)

       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = fisher_yates_sampling(good_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Minibatch loss at step 3000: 1.604837
     Minibatch accuracy: 91.4%
     Validation accuracy: 84.7%
     Test accuracy: 72.5%
     Minibatch loss at step 2500: 3.163114
     Minibatch accuracy: 91.4%
     Validation accuracy: 83.7%
     Minibatch loss at step 2000: 7.948224
     Minibatch accuracy: 81.2%
     Validation accuracy: 83.3%
     Minibatch loss at step 1500: 2.736376
     Minibatch accuracy: 89.8%
     Validation accuracy: 82.7%
     Minibatch loss at step 1000: 11.304239
     Minibatch accuracy: 84.4%
     Validation accuracy: 82.5%
     Minibatch loss at step 500: 17.010756
     Minibatch accuracy: 85.9%
     Validation accuracy: 82.1%
     Initialized
     Minibatch loss at step 0: 339.549652
     Minibatch accuracy: 7.8%
     Validation accuracy: 26.5%
 #+END_EXAMPLE



* Assignment 3
:PROPERTIES:
:CUSTOM_ID: assignment-3
:header-args: :session a3py
:END:

 Previously in =2_fullyconnected.ipynb=, you trained a logistic
 regression and a neural network model.

 The goal of this assignment is to explore regularization techniques.

 #+BEGIN_SRC python
     # These are all the modules we'll be using later. Make sure you can import them
     # before proceeding further.
     from __future__ import print_function
     import numpy as np
     import tensorflow as tf
     from six.moves import cPickle as pickle
 #+END_SRC

 #+RESULTS:

 First reload the data we generated in /notmist.ipynb/.

 #+BEGIN_SRC python
   pickle_file = 'notMNIST.pickle'

   with open(pickle_file, 'rb') as f:
     save = pickle.load(f)
     train_dataset = save['train_dataset']
     train_labels = save['train_labels']
     valid_dataset = save['valid_dataset']
     valid_labels = save['valid_labels']
     test_dataset = save['test_dataset']
     test_labels = save['test_labels']
     del save  # hint to help gc free up memory
     print('Training set', train_dataset.shape, train_labels.shape)
     print('Validation set', valid_dataset.shape, valid_labels.shape)
     print('Test set', test_dataset.shape, test_labels.shape)
 #+END_SRC

 #+BEGIN_EXAMPLE
   Training set (200000, 28, 28) (200000,)
       Validation set (10000, 28, 28) (10000,)
       Test set (10000, 28, 28) (10000,)
 #+END_EXAMPLE

 Reformat into a shape that's more adapted to the models we're going to
 train:

 -  data as a flat matrix,
 -  labels as float 1-hot encodings.

 #+BEGIN_SRC python
   image_size = 28
   num_labels = 10

   def reformat(dataset, labels):
     dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
     # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]
     labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
     return dataset, labels
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
   train_dataset, train_labels = reformat(train_dataset, train_labels)
   valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
   test_dataset, test_labels = reformat(test_dataset, test_labels)
   print('Training set', train_dataset.shape, train_labels.shape)
   print('Validation set', valid_dataset.shape, valid_labels.shape)
   print('Test set', test_dataset.shape, test_labels.shape)
 #+END_SRC

 #+RESULTS:

 #+BEGIN_EXAMPLE
     Training set (200000, 784) (200000, 10)
     Validation set (10000, 784) (10000, 10)
     Test set (10000, 784) (10000, 10)
 #+END_EXAMPLE

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+RESULTS:



** Problem 1
    :PROPERTIES:
    :CUSTOM_ID: problem-1
    :END:

 Introduce and tune L2 regularization for both logistic and neural
 network models. Remember that L2 amounts to adding a penalty on the norm
 of the weights to the loss. In TensorFlow, you can compute the L2 loss
 for a tensor =t= using =nn.l2_loss(t)=. The right amount of
 regularization should improve your validation / test accuracy.



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = map(lambda x: x[0], apx_eql_lst)
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 2001
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = random_permutation(good_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024*16
     beta = .01
     keep_prob = 0.5
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)


       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = (tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+
               beta*(tf.nn.l2_loss(weights0)+tf.nn.l2_loss(weights1)))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = random_permutation(good_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Initialized
     Minibatch loss at step 0: 7347.640625
     Minibatch accuracy: 5.5%
     Validation accuracy: 37.0%
     Minibatch loss at step 500: 0.876382
     Minibatch accuracy: 96.1%
     Validation accuracy: 78.7%
     Minibatch loss at step 1000: 0.620260
     Minibatch accuracy: 92.2%
     Validation accuracy: 78.6%
     Minibatch loss at step 1500: 0.658640
     Minibatch accuracy: 88.3%
     Validation accuracy: 78.3%
     Minibatch loss at step 2000: 0.555534
     Minibatch accuracy: 93.8%
     Validation accuracy: 76.1%
     Test accuracy: 66.2%
 #+END_EXAMPLE



** Problem 2
    :PROPERTIES:
    :CUSTOM_ID: problem-2
    :END:

 Let's demonstrate an extreme case of overfitting. Restrict your training
 data to just a few batches. What happens?



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = map(lambda x: x[0], apx_eql_lst)
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
     num_train = 800
     good_train_ix = good_train_ix[:num_train]
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 2001
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = fisher_yates_sampling(good_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024
     beta = .01
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)

       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = (tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+
               beta*(tf.nn.l2_loss(weights0)+tf.nn.l2_loss(weights1)))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = fisher_yates_sampling(good_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Test accuracy: 68.6%
     Validation accuracy: 79.3%
     Minibatch loss at step 2000: 0.490076
     Minibatch accuracy: 96.9%
     Validation accuracy: 80.4%
     Minibatch loss at step 1500: 0.414823
     Minibatch accuracy: 99.2%
     Validation accuracy: 81.4%
     Minibatch loss at step 1000: 0.400443
     Minibatch accuracy: 100.0%
     Validation accuracy: 81.8%
     Minibatch loss at step 500: 0.780712
     Minibatch accuracy: 97.7%
     Validation accuracy: 30.3%
     Initialized
     Minibatch loss at step 0: 6528.056152
     Minibatch accuracy: 13.3%
 #+END_EXAMPLE



** Problem 3
    :PROPERTIES:
    :CUSTOM_ID: problem-3
    :END:

 Introduce Dropout on the hidden layer of the neural network. Remember:
 Dropout should only be introduced during training, not evaluation,
 otherwise your evaluation results would be stochastic as well.
 TensorFlow provides =nn.dropout()= for that, but you have to make sure
 it's only inserted during training.

 What happens to our extreme overfitting case?



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = list(map(lambda x: x[0], apx_eql_lst))
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     train_fraction = .01
     num_train = round(train_fraction*len(good_train_ix))
     actual_train_ix = good_train_ix[:num_train]
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 1001
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = random_permutation(actual_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     keep_prob = .5                  # 0<keep_prob<=1
     num_hidden = 1024*4
     beta = .01
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)
       hidden_drop = tf.nn.dropout(hidden_dataset,keep_prob)*(1/keep_prob)

       # Training computation.
       logits = tf.matmul(hidden_drop, weights1) + biases1
       loss = (tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+
               beta*(tf.nn.l2_loss(weights0)+tf.nn.l2_loss(weights1)))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = random_permutation(actual_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Test accuracy: 69.6%
     Minibatch loss at step 2000: 0.485670
     Minibatch accuracy: 94.5%
     Validation accuracy: 80.7%
     Minibatch loss at step 1500: 0.527620
     Minibatch accuracy: 96.9%
     Validation accuracy: 82.2%
     Minibatch loss at step 1000: 1.110798
     Minibatch accuracy: 95.3%
     Validation accuracy: 83.2%
     Minibatch loss at step 500: 98.461121
     Minibatch accuracy: 99.2%
     Validation accuracy: 82.6%
     Validation accuracy: 35.0%
     Initialized
     Minibatch loss at step 0: 14592.014648
     Minibatch accuracy: 12.5%
 #+END_EXAMPLE



** Problem 4
    :PROPERTIES:
    :CUSTOM_ID: problem-4
    :END:

 Try to get the best performance you can using a multi-layer model! The
 best reported test accuracy using a deep network is
 [[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595][97.1%]].

 One avenue you can explore is to add multiple layers.

 Another one is to use learning rate decay:

 #+BEGIN_EXAMPLE
     global_step = tf.Variable(0)  # count the number of steps taken.
     learning_rate = tf.train.exponential_decay(0.5, global_step, ...)
     optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)
 #+END_EXAMPLE



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = list(map(lambda x: x[0], apx_eql_lst))
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     train_fraction = 1
     num_train = round(train_fraction*len(good_train_ix))
     actual_train_ix = good_train_ix[:num_train]
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = random_permutation(actual_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     parameters = {
       'num_steps':6501,
       'batch_size':128,
       'keep_prob':.8,                  # 0<keep_prob<=1
       'learning_rate':[
         0.001,          # Base learning rate.
         128,           # Current index into the dataset (multiply by batch size).
         num_train,     # Decay steps.
         0.8           # Decay rate.
         ],
       'beta':.01,   # regularization parameter
       'num_hidden':[2^10,2^10],
       'layer_fcn':[tf.nn.relu,tf.nn.relu] ,
       'num_hidden_layers':2,
       'momentum':.9,
       'opt_fcn':tf.train.MomentumOptimizer # AdamOptimizer,MomentumOptimizer,GradientDescentOptimizer
     }
     assert parameters['num_hidden_layers'] == len(parameters['layer_fcn']) == len(parameters['num_hidden'])
     graph = tf.Graph()
     with graph.as_default():
       batch = tf.Variable(0)
       learning_rate = tf.train.exponential_decay(
         parameters['learning_rate'][0],
         parameters['learning_rate'][1]*batch,
         *parameters['learning_rate'][2:],
         staircase=True)
       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(parameters['batch_size'], image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(parameters['batch_size'], num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables
       weights = [tf.Variable(
         tf.truncated_normal([image_size * image_size, parameters['num_hidden'][0]]))]
       biases = [tf.Variable(tf.zeros([parameters['num_hidden'][0]]))]
       hidden_dataset = tf.nn.dropout(
         parameters['layer_fcn'][0](tf.add(tf.matmul(tf_train_dataset, weights[0]), biases[0])), 
         parameters['keep_prob'])*(1/parameters['keep_prob'])

       for l in range(1,parameters['num_hidden_layers']):
         weights += [tf.Variable(tf.truncated_normal([parameters['num_hidden'][l-1], parameters['num_hidden'][l]]))]
         biases += [tf.Variable(tf.zeros([parameters['num_hidden'][l]]))]
         hidden_dataset = tf.nn.dropout(
           parameters['layer_fcn'][l](tf.add(tf.matmul(hidden_dataset, weights[l]), biases[l])),
           parameters['keep_prob'])*(1/parameters['keep_prob'])


       weights += [tf.Variable(tf.truncated_normal([parameters['num_hidden'][-1], num_labels]))]
       biases += [tf.Variable(tf.zeros([num_labels]))]
     
       # Training computation.
       logits = tf.matmul(hidden_dataset, weights[-1]) + biases[-1]
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
       for i in range(parameters['num_hidden_layers']+1):
         loss += parameters['beta']*tf.nn.l2_loss(weights[i])

       # Optimizer
       # optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
       optimizer = parameters['opt_fcn'](
         learning_rate,
         parameters['momentum']).minimize(loss, global_step=batch)
       # optimizer = parameters['opt_fcn'](
       #   learning_rate=parameters['learning_rate'], 
       #   global_step=parameters['global_step']).minimize(loss)
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = parameters['layer_fcn'][0](tf.matmul(tf_valid_dataset, weights[0]) + 
                                   biases[0])
       test_hidden = parameters['layer_fcn'][0](tf.matmul(tf_test_dataset, weights[0]) + 
                                  biases[0])
       for l in range(1,parameters['num_hidden_layers']):
         valid_hidden = parameters['layer_fcn'][l](tf.matmul(valid_hidden, weights[l]) +
                                     biases[l])
         test_hidden = parameters['layer_fcn'][l](tf.matmul(test_hidden, weights[l]) + 
                                    biases[l])

       valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, weights[-1]) + 
                                        biases[-1])
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights[-1]) + 
                                       biases[-1])
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     test_accuracy = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(parameters['num_steps']):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * parameters['batch_size']) % (len(rand_train_ix) - parameters['batch_size'])
         if offset < last_offset:
           rand_train_ix = random_permutation(actual_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + parameters['batch_size'])], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + parameters['batch_size'])], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       test_accuracy = accuracy(test_prediction.eval(), test_labels)
       print("Test accuracy: %.1f%%" % test_accuracy)
 #+END_SRC

 #+BEGIN_EXAMPLE
     Test accuracy: 25.1%
     Minibatch loss at step 6500: 11.521459
     Minibatch accuracy: 28.9%
     Validation accuracy: 28.7%
     Minibatch loss at step 6000: 12.083858
     Minibatch accuracy: 21.9%
     Validation accuracy: 27.6%
     Minibatch loss at step 5500: 12.607504
     Minibatch accuracy: 21.1%
     Validation accuracy: 26.5%
     Minibatch loss at step 5000: 13.112435
     Minibatch accuracy: 28.1%
     Validation accuracy: 25.1%
     Minibatch loss at step 4500: 13.792360
     Minibatch accuracy: 22.7%
     Validation accuracy: 23.3%
     Minibatch loss at step 4000: 14.609591
     Minibatch accuracy: 18.8%
     Validation accuracy: 22.4%
     Minibatch loss at step 3500: 15.401945
     Minibatch accuracy: 18.8%
     Validation accuracy: 21.5%
     Minibatch loss at step 3000: 16.365429
     Minibatch accuracy: 21.1%
     Validation accuracy: 20.2%
     Minibatch loss at step 2500: 17.581362
     Minibatch accuracy: 17.2%
     Validation accuracy: 19.9%
     Minibatch loss at step 2000: 18.865662
     Minibatch accuracy: 14.8%
     Validation accuracy: 18.9%
     Minibatch loss at step 1500: 20.277245
     Minibatch accuracy: 19.5%
     Validation accuracy: 18.5%
     Minibatch loss at step 1000: 22.221041
     Minibatch accuracy: 25.0%
     Validation accuracy: 17.8%
     Minibatch loss at step 500: 24.418331
     Minibatch accuracy: 11.7%
     Validation accuracy: 17.3%
     Initialized
     Minibatch loss at step 0: 65.115540
     Minibatch accuracy: 11.7%
     Validation accuracy: 8.8%
 #+END_EXAMPLE

 #+BEGIN_SRC python
     with open("results.txt", "a") as myfile:
       myfile.write(str(parameters))
       myfile.write("\n"+str(test_accuracy))
 #+END_SRC


* Assignment 4
:PROPERTIES:
:header-args: :session a4py
:END:

** Starter code

Previously in =2_fullyconnected.ipynb= and =3_regularization.ipynb=, we
trained fully connected networks to classify
[[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST]]
characters.

The goal of this assignment is make the neural network convolutional.

#+BEGIN_SRC python :session a4aspy
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import numpy as np
  import tensorflow as tf
  from six.moves import cPickle as pickle
  from six.moves import range
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output
  pickle_file = 'notMNIST.pickle'

  with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_dataset = save['train_dataset']
    train_labels = save['train_labels']
    valid_dataset = save['valid_dataset']
    valid_labels = save['valid_labels']
    test_dataset = save['test_dataset']
    test_labels = save['test_labels']
    del save  # hint to help gc free up memory
#+END_SRC

#+RESULTS:
    
#+BEGIN_SRC python :session a4aspy :results output
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+RESULTS:
: Training set (200000, 28, 28) (200000,)
: Validation set (10000, 28, 28) (10000,)
: Test set (10000, 28, 28) (10000,)


Reformat into a TensorFlow-friendly shape: - convolutions need the image
data formatted as a cube (width by height by #channels) - labels as
float 1-hot encodings.

#+BEGIN_SRC python :session a4aspy :results output
  image_size = 28
  num_labels = 10
  num_channels = 1 # grayscale

  import numpy as np

  def reformat(dataset, labels):
    dataset = dataset.reshape(
      (-1, image_size, image_size, num_channels)).astype(np.float32)
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels
#+END_SRC

#+RESULTS:
  
#+BEGIN_SRC python :session a4aspy :results output
  train_dataset, train_labels = reformat(train_dataset, train_labels)
  valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
  test_dataset, test_labels = reformat(test_dataset, test_labels)
#+END_SRC

#+RESULTS:
   
#+BEGIN_SRC python :session a4aspy :results output
   print('Training set', train_dataset.shape, train_labels.shape)
   print('Validation set', valid_dataset.shape, valid_labels.shape)
   print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+RESULTS:
: Training set (200000, 28, 28, 1) (200000, 10)
: Validation set (10000, 28, 28, 1) (10000, 10)
: Test set (10000, 28, 28, 1) (10000, 10)


#+BEGIN_SRC python :session a4aspy :results none
  def accuracy(predictions, labels):
    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
            / predictions.shape[0])
#+END_SRC

#+RESULTS:

Let's build a small network with two convolutional layers, followed by
one fully connected layer. Convolutional networks are more expensive
computationally, so we'll limit its depth and number of fully connected
nodes.

#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      shape = hidden.get_shape().as_list()
      reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
      hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(learning_rate=sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    test_prediction = tf.nn.softmax(model(tf_test_dataset))
#+END_SRC

#+RESULTS:

#+NAME: run_graph
#+BEGIN_SRC python :var nsteps = 1001 :var keep_prob = 1 :session a4aspy :results output
  num_steps = nsteps
  lr = 1e-3
  lr_decay = 0.999995
  with tf.Session(graph=graph) as sess:
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(1,num_steps):
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      lr *= lr_decay
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, 
                   sy_learn_rate: lr, sy_keep_prob: keep_prob}
      _, l, predictions = sess.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 50 == 0):
        print('Minibatch loss at step %d: %f' % (step, l))
        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+RESULTS: run_graph
#+begin_example

>>> >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2017-06-15 07:45:35.562943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 1.861184
Minibatch accuracy: 68.8%
Validation accuracy: 41.9%
Minibatch loss at step 100: 5.072113
Minibatch accuracy: 31.2%
Validation accuracy: 51.2%
Minibatch loss at step 150: 4.068310
Minibatch accuracy: 37.5%
Validation accuracy: 57.7%
Minibatch loss at step 200: 2.123676
Minibatch accuracy: 50.0%
Validation accuracy: 57.4%
Minibatch loss at step 250: 1.832294
Minibatch accuracy: 56.2%
Validation accuracy: 61.3%
Minibatch loss at step 300: 0.926931
Minibatch accuracy: 75.0%
Validation accuracy: 67.6%
Minibatch loss at step 350: 0.491697
Minibatch accuracy: 81.2%
Validation accuracy: 68.0%
Minibatch loss at step 400: 0.317087
Minibatch accuracy: 87.5%
Validation accuracy: 70.7%
Minibatch loss at step 450: 0.814103
Minibatch accuracy: 81.2%
Validation accuracy: 72.2%
Minibatch loss at step 500: 1.340266
Minibatch accuracy: 68.8%
Validation accuracy: 73.4%
Minibatch loss at step 550: 0.328524
Minibatch accuracy: 87.5%
Validation accuracy: 73.7%
Minibatch loss at step 600: 0.437495
Minibatch accuracy: 87.5%
Validation accuracy: 74.5%
Minibatch loss at step 650: 0.908238
Minibatch accuracy: 75.0%
Validation accuracy: 75.5%
Minibatch loss at step 700: 0.803429
Minibatch accuracy: 75.0%
Validation accuracy: 75.7%
Minibatch loss at step 750: 1.014526
Minibatch accuracy: 81.2%
Validation accuracy: 76.5%
Minibatch loss at step 800: 0.558631
Minibatch accuracy: 81.2%
Validation accuracy: 77.7%
Minibatch loss at step 850: 0.429926
Minibatch accuracy: 87.5%
Validation accuracy: 78.2%
Minibatch loss at step 900: 0.420720
Minibatch accuracy: 81.2%
Validation accuracy: 78.2%
Minibatch loss at step 950: 0.818006
Minibatch accuracy: 68.8%
Validation accuracy: 79.2%
Minibatch loss at step 1000: 0.752457
Minibatch accuracy: 81.2%
Validation accuracy: 79.0%
Test accuracy: 85.7%
#+end_example



** Problem 1 

The convolutional model above uses convolutions with stride 2 to reduce
the dimensionality. Replace the strides by a max pooling operation
(=nn.max_pool()=) of stride 2 and kernel size 2.

#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      hidden = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      hidden = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = hidden.get_shape().as_list()
      reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
      hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(learning_rate=sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    test_prediction = tf.nn.softmax(model(tf_test_dataset))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
08:09:56.957957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 2.900611
Minibatch accuracy: 68.8%
Validation accuracy: 47.1%
Minibatch loss at step 100: 1.479307
Minibatch accuracy: 56.2%
Validation accuracy: 54.6%
Minibatch loss at step 150: 4.690967
Minibatch accuracy: 31.2%
Validation accuracy: 58.4%
Minibatch loss at step 200: 1.523903
Minibatch accuracy: 62.5%
Validation accuracy: 61.9%
Minibatch loss at step 250: 1.892011
Minibatch accuracy: 50.0%
Validation accuracy: 66.5%
Minibatch loss at step 300: 1.033208
Minibatch accuracy: 75.0%
Validation accuracy: 67.8%
Minibatch loss at step 350: 1.215178
Minibatch accuracy: 75.0%
Validation accuracy: 69.8%
Minibatch loss at step 400: 0.688676
Minibatch accuracy: 62.5%
Validation accuracy: 70.1%
Minibatch loss at step 450: 1.180127
Minibatch accuracy: 75.0%
Validation accuracy: 73.0%
Minibatch loss at step 500: 1.524866
Minibatch accuracy: 68.8%
Validation accuracy: 73.9%
Minibatch loss at step 550: 0.290829
Minibatch accuracy: 87.5%
Validation accuracy: 75.3%
Minibatch loss at step 600: 0.348496
Minibatch accuracy: 87.5%
Validation accuracy: 75.7%
Minibatch loss at step 650: 1.442225
Minibatch accuracy: 68.8%
Validation accuracy: 74.8%
Minibatch loss at step 700: 0.850661
Minibatch accuracy: 68.8%
Validation accuracy: 75.8%
Minibatch loss at step 750: 1.095397
Minibatch accuracy: 62.5%
Validation accuracy: 77.4%
Minibatch loss at step 800: 0.330389
Minibatch accuracy: 87.5%
Validation accuracy: 78.0%
Minibatch loss at step 850: 0.832846
Minibatch accuracy: 81.2%
Validation accuracy: 78.1%
Minibatch loss at step 900: 0.486584
Minibatch accuracy: 87.5%
Validation accuracy: 77.4%
Minibatch loss at step 950: 1.308142
Minibatch accuracy: 68.8%
Validation accuracy: 78.9%
Minibatch loss at step 1000: 0.909435
Minibatch accuracy: 75.0%
Validation accuracy: 79.4%
Minibatch loss at step 1050: 1.192717
Minibatch accuracy: 56.2%
Validation accuracy: 80.1%
Minibatch loss at step 1100: 0.584909
Minibatch accuracy: 81.2%
Validation accuracy: 80.2%
Minibatch loss at step 1150: 0.142735
Minibatch accuracy: 100.0%
Validation accuracy: 79.5%
Minibatch loss at step 1200: 0.088882
Minibatch accuracy: 100.0%
Validation accuracy: 80.1%
Minibatch loss at step 1250: 0.535772
Minibatch accuracy: 81.2%
Validation accuracy: 79.1%
Minibatch loss at step 1300: 0.890280
Minibatch accuracy: 81.2%
Validation accuracy: 81.3%
Minibatch loss at step 1350: 0.338932
Minibatch accuracy: 87.5%
Validation accuracy: 80.3%
Minibatch loss at step 1400: 0.214247
Minibatch accuracy: 93.8%
Validation accuracy: 81.0%
Minibatch loss at step 1450: 1.065812
Minibatch accuracy: 62.5%
Validation accuracy: 81.4%
Minibatch loss at step 1500: 0.647235
Minibatch accuracy: 75.0%
Validation accuracy: 82.0%
Minibatch loss at step 1550: 0.544307
Minibatch accuracy: 87.5%
Validation accuracy: 81.8%
Minibatch loss at step 1600: 0.329639
Minibatch accuracy: 87.5%
Validation accuracy: 81.3%
Minibatch loss at step 1650: 0.874878
Minibatch accuracy: 68.8%
Validation accuracy: 81.2%
Minibatch loss at step 1700: 1.150976
Minibatch accuracy: 62.5%
Validation accuracy: 81.8%
Minibatch loss at step 1750: 0.856514
Minibatch accuracy: 81.2%
Validation accuracy: 81.4%
Minibatch loss at step 1800: 0.306168
Minibatch accuracy: 93.8%
Validation accuracy: 81.8%
Minibatch loss at step 1850: 0.579684
Minibatch accuracy: 81.2%
Validation accuracy: 81.7%
Minibatch loss at step 1900: 0.453013
Minibatch accuracy: 87.5%
Validation accuracy: 82.7%
Minibatch loss at step 1950: 0.422659
Minibatch accuracy: 87.5%
Validation accuracy: 83.0%
Minibatch loss at step 2000: 0.557044
Minibatch accuracy: 75.0%
Validation accuracy: 82.8%
Minibatch loss at step 2050: 0.886053
Minibatch accuracy: 75.0%
Validation accuracy: 83.4%
Minibatch loss at step 2100: 0.506418
Minibatch accuracy: 87.5%
Validation accuracy: 82.9%
Minibatch loss at step 2150: 0.170724
Minibatch accuracy: 100.0%
Validation accuracy: 82.1%
Minibatch loss at step 2200: 0.240701
Minibatch accuracy: 93.8%
Validation accuracy: 83.3%
Minibatch loss at step 2250: 0.123136
Minibatch accuracy: 100.0%
Validation accuracy: 83.7%
Minibatch loss at step 2300: 0.540390
Minibatch accuracy: 81.2%
Validation accuracy: 83.4%
Minibatch loss at step 2350: 0.372324
Minibatch accuracy: 93.8%
Validation accuracy: 82.9%
Minibatch loss at step 2400: 0.453190
Minibatch accuracy: 81.2%
Validation accuracy: 83.9%
Minibatch loss at step 2450: 1.779238
Minibatch accuracy: 62.5%
Validation accuracy: 83.5%
Minibatch loss at step 2500: 0.705045
Minibatch accuracy: 68.8%
Validation accuracy: 84.0%
Minibatch loss at step 2550: 1.292835
Minibatch accuracy: 68.8%
Validation accuracy: 83.6%
Minibatch loss at step 2600: 0.377887
Minibatch accuracy: 87.5%
Validation accuracy: 83.2%
Minibatch loss at step 2650: 1.297781
Minibatch accuracy: 68.8%
Validation accuracy: 83.7%
Minibatch loss at step 2700: 0.512231
Minibatch accuracy: 81.2%
Validation accuracy: 83.9%
Minibatch loss at step 2750: 1.071017
Minibatch accuracy: 81.2%
Validation accuracy: 84.2%
Minibatch loss at step 2800: 0.298933
Minibatch accuracy: 81.2%
Validation accuracy: 84.4%
Minibatch loss at step 2850: 1.001704
Minibatch accuracy: 62.5%
Validation accuracy: 83.7%
Minibatch loss at step 2900: 0.356880
Minibatch accuracy: 93.8%
Validation accuracy: 84.4%
Minibatch loss at step 2950: 0.796455
Minibatch accuracy: 62.5%
Validation accuracy: 84.3%
Minibatch loss at step 3000: 0.895969
Minibatch accuracy: 68.8%
Validation accuracy: 84.5%
Minibatch loss at step 3050: 0.272711
Minibatch accuracy: 93.8%
Validation accuracy: 84.1%
Minibatch loss at step 3100: 0.733070
Minibatch accuracy: 81.2%
Validation accuracy: 84.0%
Minibatch loss at step 3150: 0.692155
Minibatch accuracy: 81.2%
Validation accuracy: 84.1%
Minibatch loss at step 3200: 0.413291
Minibatch accuracy: 87.5%
Validation accuracy: 84.3%
Minibatch loss at step 3250: 0.643606
Minibatch accuracy: 75.0%
Validation accuracy: 84.4%
Minibatch loss at step 3300: 0.573235
Minibatch accuracy: 87.5%
Validation accuracy: 85.2%
Minibatch loss at step 3350: 0.737073
Minibatch accuracy: 68.8%
Validation accuracy: 85.1%
Minibatch loss at step 3400: 0.278904
Minibatch accuracy: 93.8%
Validation accuracy: 84.3%
Minibatch loss at step 3450: 0.902674
Minibatch accuracy: 81.2%
Validation accuracy: 85.0%
Minibatch loss at step 3500: 0.291990
Minibatch accuracy: 87.5%
Validation accuracy: 84.5%
Minibatch loss at step 3550: 0.689245
Minibatch accuracy: 75.0%
Validation accuracy: 85.0%
Minibatch loss at step 3600: 0.756285
Minibatch accuracy: 68.8%
Validation accuracy: 85.4%
Minibatch loss at step 3650: 0.357790
Minibatch accuracy: 87.5%
Validation accuracy: 84.7%
Minibatch loss at step 3700: 1.061131
Minibatch accuracy: 62.5%
Validation accuracy: 84.6%
Minibatch loss at step 3750: 0.835744
Minibatch accuracy: 75.0%
Validation accuracy: 85.7%
Minibatch loss at step 3800: 0.615569
Minibatch accuracy: 87.5%
Validation accuracy: 85.9%
Minibatch loss at step 3850: 0.258741
Minibatch accuracy: 93.8%
Validation accuracy: 85.5%
Minibatch loss at step 3900: 0.446685
Minibatch accuracy: 81.2%
Validation accuracy: 85.8%
Minibatch loss at step 3950: 0.727674
Minibatch accuracy: 68.8%
Validation accuracy: 85.2%
Minibatch loss at step 4000: 0.505325
Minibatch accuracy: 87.5%
Validation accuracy: 85.4%
Minibatch loss at step 4050: 0.447677
Minibatch accuracy: 87.5%
Validation accuracy: 86.2%
Minibatch loss at step 4100: 0.272943
Minibatch accuracy: 93.8%
Validation accuracy: 85.3%
Minibatch loss at step 4150: 0.159460
Minibatch accuracy: 93.8%
Validation accuracy: 85.2%
Minibatch loss at step 4200: 0.271813
Minibatch accuracy: 93.8%
Validation accuracy: 85.3%
Minibatch loss at step 4250: 0.403566
Minibatch accuracy: 87.5%
Validation accuracy: 85.1%
Minibatch loss at step 4300: 0.862058
Minibatch accuracy: 75.0%
Validation accuracy: 85.2%
Minibatch loss at step 4350: 0.403553
Minibatch accuracy: 93.8%
Validation accuracy: 86.0%
Minibatch loss at step 4400: 0.576407
Minibatch accuracy: 81.2%
Validation accuracy: 85.5%
Minibatch loss at step 4450: 0.401695
Minibatch accuracy: 87.5%
Validation accuracy: 85.7%
Minibatch loss at step 4500: 0.616162
Minibatch accuracy: 81.2%
Validation accuracy: 83.4%
Minibatch loss at step 4550: 0.346180
Minibatch accuracy: 87.5%
Validation accuracy: 86.1%
Minibatch loss at step 4600: 0.267765
Minibatch accuracy: 93.8%
Validation accuracy: 86.2%
Minibatch loss at step 4650: 0.444128
Minibatch accuracy: 87.5%
Validation accuracy: 85.7%
Minibatch loss at step 4700: 0.482445
Minibatch accuracy: 87.5%
Validation accuracy: 86.0%
Minibatch loss at step 4750: 0.698112
Minibatch accuracy: 68.8%
Validation accuracy: 85.8%
Minibatch loss at step 4800: 0.499284
Minibatch accuracy: 81.2%
Validation accuracy: 85.4%
Minibatch loss at step 4850: 0.811626
Minibatch accuracy: 81.2%
Validation accuracy: 86.4%
Minibatch loss at step 4900: 0.526118
Minibatch accuracy: 75.0%
Validation accuracy: 86.0%
Minibatch loss at step 4950: 0.191419
Minibatch accuracy: 93.8%
Validation accuracy: 85.8%
Minibatch loss at step 5000: 0.389479
Minibatch accuracy: 87.5%
Validation accuracy: 85.9%
Test accuracy: 91.7%
#+end_example


** Problem 2
Try to get the best performance you can using a convolutional net. Look for example at the classic [[http://yann.lecun.com/exdb/lenet/][LeNet5]] architecture, adding Dropout, and/or adding learning rate decay.

*** experiments

**** 3
learning rate_decay + dropout+ regularization
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  SEED = np.random.randint(low=np.iinfo(np.uint32).min, 
                           high=np.iinfo(np.uint32).max, size=1)[0]
  beta = 5e-28
  graph = tf.Graph()

  with graph.as_default():
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    sy_keep_prob = tf.placeholder(tf.float32)
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1, seed = SEED))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1, seed=SEED))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1, seed=SEED))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1, seed=SEED))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, sy_keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.div(tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), sy_keep_prob, seed=SEED),sy_keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    regularizers = (tf.nn.l2_loss(layer1_weights)+ 
                    tf.nn.l2_loss(layer2_weights)+ 
                    tf.nn.l2_loss(layer3_weights)+ 
                    tf.nn.l2_loss(layer4_weights))
    loss += beta*regularizers
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001, keep_prob=.5) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
 2017-06-15 09:39:23.291681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 4.247050
Minibatch accuracy: 37.5%
Validation accuracy: 27.4%
Minibatch loss at step 100: 4.601756
Minibatch accuracy: 18.8%
Validation accuracy: 35.9%
Minibatch loss at step 150: 3.145462
Minibatch accuracy: 12.5%
Validation accuracy: 44.5%
Minibatch loss at step 200: 3.035318
Minibatch accuracy: 18.8%
Validation accuracy: 46.6%
Minibatch loss at step 250: 2.477204
Minibatch accuracy: 31.2%
Validation accuracy: 51.1%
Minibatch loss at step 300: 1.430561
Minibatch accuracy: 43.8%
Validation accuracy: 55.5%
Minibatch loss at step 350: 1.477067
Minibatch accuracy: 56.2%
Validation accuracy: 61.2%
Minibatch loss at step 400: 1.614931
Minibatch accuracy: 43.8%
Validation accuracy: 65.2%
Minibatch loss at step 450: 1.169079
Minibatch accuracy: 62.5%
Validation accuracy: 66.7%
Minibatch loss at step 500: 1.250844
Minibatch accuracy: 62.5%
Validation accuracy: 68.4%
Minibatch loss at step 550: 0.673903
Minibatch accuracy: 68.8%
Validation accuracy: 69.3%
Minibatch loss at step 600: 0.872271
Minibatch accuracy: 75.0%
Validation accuracy: 70.5%
Minibatch loss at step 650: 1.511239
Minibatch accuracy: 43.8%
Validation accuracy: 71.8%
Minibatch loss at step 700: 1.044477
Minibatch accuracy: 50.0%
Validation accuracy: 70.9%
Minibatch loss at step 750: 1.326239
Minibatch accuracy: 56.2%
Validation accuracy: 73.6%
Minibatch loss at step 800: 0.657693
Minibatch accuracy: 81.2%
Validation accuracy: 74.0%
Minibatch loss at step 850: 0.522227
Minibatch accuracy: 87.5%
Validation accuracy: 74.5%
Minibatch loss at step 900: 0.841039
Minibatch accuracy: 81.2%
Validation accuracy: 75.1%
Minibatch loss at step 950: 1.378965
Minibatch accuracy: 56.2%
Validation accuracy: 75.7%
Minibatch loss at step 1000: 0.972474
Minibatch accuracy: 62.5%
Validation accuracy: 75.6%
Minibatch loss at step 1050: 1.275453
Minibatch accuracy: 50.0%
Validation accuracy: 77.0%
Minibatch loss at step 1100: 0.984924
Minibatch accuracy: 81.2%
Validation accuracy: 77.4%
Minibatch loss at step 1150: 0.632606
Minibatch accuracy: 81.2%
Validation accuracy: 78.3%
Minibatch loss at step 1200: 0.348040
Minibatch accuracy: 87.5%
Validation accuracy: 76.6%
Minibatch loss at step 1250: 1.205865
Minibatch accuracy: 75.0%
Validation accuracy: 77.1%
Minibatch loss at step 1300: 1.500824
Minibatch accuracy: 62.5%
Validation accuracy: 78.6%
Minibatch loss at step 1350: 0.564847
Minibatch accuracy: 87.5%
Validation accuracy: 78.8%
Minibatch loss at step 1400: 1.520104
Minibatch accuracy: 75.0%
Validation accuracy: 78.6%
Minibatch loss at step 1450: 1.583120
Minibatch accuracy: 68.8%
Validation accuracy: 78.7%
Minibatch loss at step 1500: 0.723130
Minibatch accuracy: 75.0%
Validation accuracy: 79.1%
Minibatch loss at step 1550: 1.007327
Minibatch accuracy: 75.0%
Validation accuracy: 78.4%
Minibatch loss at step 1600: 0.972170
Minibatch accuracy: 62.5%
Validation accuracy: 78.7%
Minibatch loss at step 1650: 1.221925
Minibatch accuracy: 43.8%
Validation accuracy: 78.9%
Minibatch loss at step 1700: 1.277078
Minibatch accuracy: 43.8%
Validation accuracy: 77.3%
Minibatch loss at step 1750: 1.305301
Minibatch accuracy: 81.2%
Validation accuracy: 79.1%
Minibatch loss at step 1800: 1.140792
Minibatch accuracy: 68.8%
Validation accuracy: 80.2%
Minibatch loss at step 1850: 0.916527
Minibatch accuracy: 68.8%
Validation accuracy: 79.4%
Minibatch loss at step 1900: 0.319315
Minibatch accuracy: 93.8%
Validation accuracy: 80.2%
Minibatch loss at step 1950: 0.566165
Minibatch accuracy: 75.0%
Validation accuracy: 80.6%
Minibatch loss at step 2000: 0.734313
Minibatch accuracy: 75.0%
Validation accuracy: 80.3%
Minibatch loss at step 2050: 0.861633
Minibatch accuracy: 75.0%
Validation accuracy: 79.8%
Minibatch loss at step 2100: 0.872185
Minibatch accuracy: 75.0%
Validation accuracy: 81.3%
Minibatch loss at step 2150: 0.436945
Minibatch accuracy: 81.2%
Validation accuracy: 80.5%
Minibatch loss at step 2200: 0.475010
Minibatch accuracy: 81.2%
Validation accuracy: 80.8%
Minibatch loss at step 2250: 0.455218
Minibatch accuracy: 81.2%
Validation accuracy: 81.5%
Minibatch loss at step 2300: 0.766944
Minibatch accuracy: 75.0%
Validation accuracy: 81.7%
Minibatch loss at step 2350: 0.863824
Minibatch accuracy: 68.8%
Validation accuracy: 81.7%
Minibatch loss at step 2400: 0.604405
Minibatch accuracy: 81.2%
Validation accuracy: 81.2%
Minibatch loss at step 2450: 1.457657
Minibatch accuracy: 50.0%
Validation accuracy: 79.8%
Minibatch loss at step 2500: 1.104749
Minibatch accuracy: 62.5%
Validation accuracy: 81.8%
Minibatch loss at step 2550: 1.174053
Minibatch accuracy: 68.8%
Validation accuracy: 82.1%
Minibatch loss at step 2600: 0.624880
Minibatch accuracy: 75.0%
Validation accuracy: 81.6%
Minibatch loss at step 2650: 1.463562
Minibatch accuracy: 56.2%
Validation accuracy: 80.8%
Minibatch loss at step 2700: 0.774168
Minibatch accuracy: 68.8%
Validation accuracy: 82.5%
Minibatch loss at step 2750: 1.511965
Minibatch accuracy: 56.2%
Validation accuracy: 82.4%
Minibatch loss at step 2800: 0.246308
Minibatch accuracy: 93.8%
Validation accuracy: 82.2%
Minibatch loss at step 2850: 1.437925
Minibatch accuracy: 68.8%
Validation accuracy: 81.4%
Minibatch loss at step 2900: 1.087213
Minibatch accuracy: 62.5%
Validation accuracy: 82.3%
Minibatch loss at step 2950: 1.135118
Minibatch accuracy: 75.0%
Validation accuracy: 82.2%
Minibatch loss at step 3000: 0.933039
Minibatch accuracy: 75.0%
Validation accuracy: 83.0%
Minibatch loss at step 3050: 0.496922
Minibatch accuracy: 87.5%
Validation accuracy: 81.6%
Minibatch loss at step 3100: 0.832652
Minibatch accuracy: 75.0%
Validation accuracy: 82.0%
Minibatch loss at step 3150: 0.647313
Minibatch accuracy: 75.0%
Validation accuracy: 82.7%
Minibatch loss at step 3200: 0.759036
Minibatch accuracy: 75.0%
Validation accuracy: 83.3%
Minibatch loss at step 3250: 0.980326
Minibatch accuracy: 68.8%
Validation accuracy: 81.7%
Minibatch loss at step 3300: 0.746954
Minibatch accuracy: 62.5%
Validation accuracy: 82.2%
Minibatch loss at step 3350: 0.861651
Minibatch accuracy: 81.2%
Validation accuracy: 82.4%
Minibatch loss at step 3400: 0.470646
Minibatch accuracy: 75.0%
Validation accuracy: 81.9%
Minibatch loss at step 3450: 1.103467
Minibatch accuracy: 56.2%
Validation accuracy: 82.9%
Minibatch loss at step 3500: 0.320315
Minibatch accuracy: 87.5%
Validation accuracy: 82.1%
Minibatch loss at step 3550: 1.318188
Minibatch accuracy: 62.5%
Validation accuracy: 81.7%
Minibatch loss at step 3600: 0.733523
Minibatch accuracy: 62.5%
Validation accuracy: 82.2%
Minibatch loss at step 3650: 0.467246
Minibatch accuracy: 81.2%
Validation accuracy: 82.5%
Minibatch loss at step 3700: 0.993713
Minibatch accuracy: 62.5%
Validation accuracy: 80.3%
Minibatch loss at step 3750: 0.973567
Minibatch accuracy: 87.5%
Validation accuracy: 83.3%
Minibatch loss at step 3800: 0.563805
Minibatch accuracy: 81.2%
Validation accuracy: 83.7%
Minibatch loss at step 3850: 0.605255
Minibatch accuracy: 81.2%
Validation accuracy: 83.3%
Minibatch loss at step 3900: 0.619934
Minibatch accuracy: 81.2%
Validation accuracy: 83.0%
Minibatch loss at step 3950: 1.044772
Minibatch accuracy: 68.8%
Validation accuracy: 83.5%
Minibatch loss at step 4000: 0.891935
Minibatch accuracy: 56.2%
Validation accuracy: 83.8%
Minibatch loss at step 4050: 0.806958
Minibatch accuracy: 81.2%
Validation accuracy: 83.7%
Minibatch loss at step 4100: 0.429208
Minibatch accuracy: 81.2%
Validation accuracy: 83.4%
Minibatch loss at step 4150: 0.307217
Minibatch accuracy: 87.5%
Validation accuracy: 84.3%
Minibatch loss at step 4200: 0.383367
Minibatch accuracy: 93.8%
Validation accuracy: 83.3%
Minibatch loss at step 4250: 0.518670
Minibatch accuracy: 81.2%
Validation accuracy: 83.5%
Minibatch loss at step 4300: 1.212439
Minibatch accuracy: 81.2%
Validation accuracy: 84.3%
Minibatch loss at step 4350: 0.709060
Minibatch accuracy: 75.0%
Validation accuracy: 84.6%
Minibatch loss at step 4400: 0.325387
Minibatch accuracy: 93.8%
Validation accuracy: 84.6%
Minibatch loss at step 4450: 0.746855
Minibatch accuracy: 75.0%
Validation accuracy: 83.1%
Minibatch loss at step 4500: 0.570571
Minibatch accuracy: 81.2%
Validation accuracy: 84.2%
Minibatch loss at step 4550: 0.623103
Minibatch accuracy: 75.0%
Validation accuracy: 84.4%
Minibatch loss at step 4600: 0.710920
Minibatch accuracy: 68.8%
Validation accuracy: 84.2%
Minibatch loss at step 4650: 0.657528
Minibatch accuracy: 87.5%
Validation accuracy: 84.4%
Minibatch loss at step 4700: 0.913127
Minibatch accuracy: 62.5%
Validation accuracy: 84.0%
Minibatch loss at step 4750: 0.738293
Minibatch accuracy: 81.2%
Validation accuracy: 84.0%
Minibatch loss at step 4800: 0.820862
Minibatch accuracy: 81.2%
Validation accuracy: 84.7%
Minibatch loss at step 4850: 0.734651
Minibatch accuracy: 68.8%
Validation accuracy: 84.7%
Minibatch loss at step 4900: 1.284054
Minibatch accuracy: 68.8%
Validation accuracy: 84.6%
Minibatch loss at step 4950: 0.473267
Minibatch accuracy: 81.2%
Validation accuracy: 84.9%
Minibatch loss at step 5000: 0.622454
Minibatch accuracy: 68.8%
Validation accuracy: 84.5%
Test accuracy: 90.4%
#+end_example


**** 2

learning rate_decay + dropout 1
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    sy_keep_prob = tf.placeholder(tf.float32)
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, sy_keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.div(tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), sy_keep_prob),sy_keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001, keep_prob=.5) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
 2017-06-15 09:31:31.720257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 1.868340
Minibatch accuracy: 25.0%
Validation accuracy: 19.3%
Minibatch loss at step 100: 2.491972
Minibatch accuracy: 25.0%
Validation accuracy: 16.3%
Minibatch loss at step 150: 2.421418
Minibatch accuracy: 18.8%
Validation accuracy: 15.0%
Minibatch loss at step 200: 2.552014
Minibatch accuracy: 12.5%
Validation accuracy: 15.9%
Minibatch loss at step 250: 2.219394
Minibatch accuracy: 25.0%
Validation accuracy: 18.4%
Minibatch loss at step 300: 2.048537
Minibatch accuracy: 12.5%
Validation accuracy: 20.2%
Minibatch loss at step 350: 2.132483
Minibatch accuracy: 25.0%
Validation accuracy: 24.1%
Minibatch loss at step 400: 2.160150
Minibatch accuracy: 12.5%
Validation accuracy: 24.9%
Minibatch loss at step 450: 3.029988
Minibatch accuracy: 12.5%
Validation accuracy: 23.0%
Minibatch loss at step 500: 2.291509
Minibatch accuracy: 18.8%
Validation accuracy: 25.3%
Minibatch loss at step 550: 2.146008
Minibatch accuracy: 18.8%
Validation accuracy: 24.2%
Minibatch loss at step 600: 2.225887
Minibatch accuracy: 12.5%
Validation accuracy: 26.8%
Minibatch loss at step 650: 2.287751
Minibatch accuracy: 6.2%
Validation accuracy: 24.4%
Minibatch loss at step 700: 2.191564
Minibatch accuracy: 6.2%
Validation accuracy: 25.7%
Minibatch loss at step 750: 2.458062
Minibatch accuracy: 25.0%
Validation accuracy: 32.0%
Minibatch loss at step 800: 1.455145
Minibatch accuracy: 56.2%
Validation accuracy: 31.2%
Minibatch loss at step 850: 1.523927
Minibatch accuracy: 43.8%
Validation accuracy: 35.2%
Minibatch loss at step 900: 1.448095
Minibatch accuracy: 43.8%
Validation accuracy: 34.5%
Minibatch loss at step 950: 2.200823
Minibatch accuracy: 12.5%
Validation accuracy: 39.3%
Minibatch loss at step 1000: 2.055672
Minibatch accuracy: 12.5%
Validation accuracy: 37.9%
Minibatch loss at step 1050: 1.998952
Minibatch accuracy: 25.0%
Validation accuracy: 37.1%
Minibatch loss at step 1100: 2.184813
Minibatch accuracy: 25.0%
Validation accuracy: 35.5%
Minibatch loss at step 1150: 1.700816
Minibatch accuracy: 50.0%
Validation accuracy: 39.0%
Minibatch loss at step 1200: 1.727975
Minibatch accuracy: 31.2%
Validation accuracy: 41.0%
Minibatch loss at step 1250: 1.804940
Minibatch accuracy: 18.8%
Validation accuracy: 39.0%
Minibatch loss at step 1300: 1.738092
Minibatch accuracy: 37.5%
Validation accuracy: 45.6%
Minibatch loss at step 1350: 1.788531
Minibatch accuracy: 37.5%
Validation accuracy: 40.2%
Minibatch loss at step 1400: 1.873891
Minibatch accuracy: 31.2%
Validation accuracy: 39.4%
Minibatch loss at step 1450: 2.233442
Minibatch accuracy: 12.5%
Validation accuracy: 33.6%
Minibatch loss at step 1500: 1.529142
Minibatch accuracy: 50.0%
Validation accuracy: 41.5%
Minibatch loss at step 1550: 2.086451
Minibatch accuracy: 12.5%
Validation accuracy: 43.5%
Minibatch loss at step 1600: 1.694917
Minibatch accuracy: 43.8%
Validation accuracy: 43.5%
Minibatch loss at step 1650: 2.215186
Minibatch accuracy: 12.5%
Validation accuracy: 42.9%
Minibatch loss at step 1700: 2.049239
Minibatch accuracy: 25.0%
Validation accuracy: 46.1%
Minibatch loss at step 1750: 2.325694
Minibatch accuracy: 31.2%
Validation accuracy: 48.3%
Minibatch loss at step 1800: 1.955550
Minibatch accuracy: 18.8%
Validation accuracy: 44.5%
Minibatch loss at step 1850: 2.122712
Minibatch accuracy: 25.0%
Validation accuracy: 46.0%
Minibatch loss at step 1900: 3.525293
Minibatch accuracy: 56.2%
Validation accuracy: 49.0%
Minibatch loss at step 1950: 1.769775
Minibatch accuracy: 37.5%
Validation accuracy: 46.7%
Minibatch loss at step 2000: 1.658909
Minibatch accuracy: 31.2%
Validation accuracy: 50.0%
Minibatch loss at step 2050: 1.398658
Minibatch accuracy: 43.8%
Validation accuracy: 50.0%
Minibatch loss at step 2100: 1.706336
Minibatch accuracy: 31.2%
Validation accuracy: 54.1%
Minibatch loss at step 2150: 1.517875
Minibatch accuracy: 37.5%
Validation accuracy: 44.2%
Minibatch loss at step 2200: 1.972822
Minibatch accuracy: 25.0%
Validation accuracy: 43.0%
Minibatch loss at step 2250: 1.748646
Minibatch accuracy: 18.8%
Validation accuracy: 49.4%
Minibatch loss at step 2300: 1.821078
Minibatch accuracy: 18.8%
Validation accuracy: 43.1%
Minibatch loss at step 2350: 1.457965
Minibatch accuracy: 56.2%
Validation accuracy: 51.2%
Minibatch loss at step 2400: 1.607498
Minibatch accuracy: 37.5%
Validation accuracy: 54.3%
Minibatch loss at step 2450: 2.106126
Minibatch accuracy: 25.0%
Validation accuracy: 54.3%
Minibatch loss at step 2500: 2.163781
Minibatch accuracy: 12.5%
Validation accuracy: 55.3%
Minibatch loss at step 2550: 1.766758
Minibatch accuracy: 37.5%
Validation accuracy: 57.4%
Minibatch loss at step 2600: 1.667016
Minibatch accuracy: 31.2%
Validation accuracy: 51.7%
Minibatch loss at step 2650: 1.942839
Minibatch accuracy: 18.8%
Validation accuracy: 52.8%
Minibatch loss at step 2700: 1.667981
Minibatch accuracy: 50.0%
Validation accuracy: 52.8%
Minibatch loss at step 2750: 2.108427
Minibatch accuracy: 25.0%
Validation accuracy: 54.7%
Minibatch loss at step 2800: 2.137871
Minibatch accuracy: 12.5%
Validation accuracy: 57.0%
Minibatch loss at step 2850: 1.423964
Minibatch accuracy: 43.8%
Validation accuracy: 55.0%
Minibatch loss at step 2900: 1.178312
Minibatch accuracy: 62.5%
Validation accuracy: 60.3%
Minibatch loss at step 2950: 1.497456
Minibatch accuracy: 43.8%
Validation accuracy: 64.3%
Minibatch loss at step 3000: 1.899452
Minibatch accuracy: 25.0%
Validation accuracy: 65.9%
Minibatch loss at step 3050: 1.557965
Minibatch accuracy: 43.8%
Validation accuracy: 61.8%
Minibatch loss at step 3100: 2.167020
Minibatch accuracy: 43.8%
Validation accuracy: 60.1%
Minibatch loss at step 3150: 1.574410
Minibatch accuracy: 31.2%
Validation accuracy: 65.0%
Minibatch loss at step 3200: 1.563329
Minibatch accuracy: 31.2%
Validation accuracy: 63.9%
Minibatch loss at step 3250: 1.821066
Minibatch accuracy: 31.2%
Validation accuracy: 61.9%
Minibatch loss at step 3300: 1.937753
Minibatch accuracy: 12.5%
Validation accuracy: 65.5%
Minibatch loss at step 3350: 1.285227
Minibatch accuracy: 43.8%
Validation accuracy: 66.4%
Minibatch loss at step 3400: 1.164702
Minibatch accuracy: 50.0%
Validation accuracy: 60.1%
Minibatch loss at step 3450: 1.756230
Minibatch accuracy: 37.5%
Validation accuracy: 67.9%
Minibatch loss at step 3500: 1.453190
Minibatch accuracy: 50.0%
Validation accuracy: 69.2%
Minibatch loss at step 3550: 5.874414
Minibatch accuracy: 25.0%
Validation accuracy: 64.8%
Minibatch loss at step 3600: 1.309912
Minibatch accuracy: 31.2%
Validation accuracy: 63.2%
Minibatch loss at step 3650: 1.374123
Minibatch accuracy: 50.0%
Validation accuracy: 72.3%
Minibatch loss at step 3700: 2.032023
Minibatch accuracy: 12.5%
Validation accuracy: 71.4%
Minibatch loss at step 3750: 1.890442
Minibatch accuracy: 31.2%
Validation accuracy: 72.5%
Minibatch loss at step 3800: 1.468653
Minibatch accuracy: 43.8%
Validation accuracy: 70.6%
Minibatch loss at step 3850: 1.072326
Minibatch accuracy: 68.8%
Validation accuracy: 73.0%
Minibatch loss at step 3900: 1.276820
Minibatch accuracy: 37.5%
Validation accuracy: 73.2%
Minibatch loss at step 3950: 1.458550
Minibatch accuracy: 56.2%
Validation accuracy: 74.3%
Minibatch loss at step 4000: 1.425846
Minibatch accuracy: 50.0%
Validation accuracy: 75.0%
Minibatch loss at step 4050: 1.352317
Minibatch accuracy: 37.5%
Validation accuracy: 73.2%
Minibatch loss at step 4100: 1.841503
Minibatch accuracy: 37.5%
Validation accuracy: 69.4%
Minibatch loss at step 4150: 1.434784
Minibatch accuracy: 37.5%
Validation accuracy: 69.5%
Minibatch loss at step 4200: 1.577219
Minibatch accuracy: 43.8%
Validation accuracy: 73.9%
Minibatch loss at step 4250: 1.199322
Minibatch accuracy: 62.5%
Validation accuracy: 74.3%
Minibatch loss at step 4300: 3.041095
Minibatch accuracy: 31.2%
Validation accuracy: 74.4%
Minibatch loss at step 4350: 1.320103
Minibatch accuracy: 50.0%
Validation accuracy: 74.6%
Minibatch loss at step 4400: 1.233494
Minibatch accuracy: 50.0%
Validation accuracy: 74.9%
Minibatch loss at step 4450: 1.534841
Minibatch accuracy: 31.2%
Validation accuracy: 76.6%
Minibatch loss at step 4500: 1.474233
Minibatch accuracy: 31.2%
Validation accuracy: 74.2%
Minibatch loss at step 4550: 1.027656
Minibatch accuracy: 62.5%
Validation accuracy: 77.2%
Minibatch loss at step 4600: 1.420190
Minibatch accuracy: 43.8%
Validation accuracy: 77.1%
Minibatch loss at step 4650: 0.945686
Minibatch accuracy: 68.8%
Validation accuracy: 75.6%
Minibatch loss at step 4700: 1.448259
Minibatch accuracy: 50.0%
Validation accuracy: 76.8%
Minibatch loss at step 4750: 1.097918
Minibatch accuracy: 62.5%
Validation accuracy: 78.6%
Minibatch loss at step 4800: 1.283625
Minibatch accuracy: 56.2%
Validation accuracy: 76.1%
Minibatch loss at step 4850: 1.620434
Minibatch accuracy: 62.5%
Validation accuracy: 77.8%
Minibatch loss at step 4900: 1.379436
Minibatch accuracy: 50.0%
Validation accuracy: 73.8%
Minibatch loss at step 4950: 0.794228
Minibatch accuracy: 68.8%
Validation accuracy: 76.8%
Minibatch loss at step 5000: 1.118136
Minibatch accuracy: 68.8%
Validation accuracy: 77.3%
Test accuracy: 83.8%
#+end_example


**** 1

only dropout
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    sy_keep_prob = tf.placeholder(tf.float32)
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, sy_keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1],
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.div(tf.nn.dropout(
        tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), 
        sy_keep_prob), sy_keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(learning_rate=sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001, keep_prob=.5) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
 2017-06-15 09:29:31.169213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 11.566952
Minibatch accuracy: 6.2%
Validation accuracy: 21.2%
Minibatch loss at step 100: 3.870863
Minibatch accuracy: 12.5%
Validation accuracy: 19.8%
Minibatch loss at step 150: 2.895633
Minibatch accuracy: 31.2%
Validation accuracy: 18.5%
Minibatch loss at step 200: 2.107081
Minibatch accuracy: 31.2%
Validation accuracy: 18.0%
Minibatch loss at step 250: 1.950364
Minibatch accuracy: 25.0%
Validation accuracy: 20.6%
Minibatch loss at step 300: 1.999945
Minibatch accuracy: 31.2%
Validation accuracy: 20.1%
Minibatch loss at step 350: 2.289965
Minibatch accuracy: 6.2%
Validation accuracy: 24.4%
Minibatch loss at step 400: 2.300172
Minibatch accuracy: 12.5%
Validation accuracy: 25.6%
Minibatch loss at step 450: 3.869735
Minibatch accuracy: 12.5%
Validation accuracy: 26.2%
Minibatch loss at step 500: 2.144319
Minibatch accuracy: 18.8%
Validation accuracy: 29.3%
Minibatch loss at step 550: 1.584909
Minibatch accuracy: 43.8%
Validation accuracy: 29.3%
Minibatch loss at step 600: 2.151093
Minibatch accuracy: 12.5%
Validation accuracy: 27.3%
Minibatch loss at step 650: 2.215356
Minibatch accuracy: 18.8%
Validation accuracy: 29.4%
Minibatch loss at step 700: 2.065422
Minibatch accuracy: 31.2%
Validation accuracy: 28.2%
Minibatch loss at step 750: 2.016208
Minibatch accuracy: 43.8%
Validation accuracy: 30.5%
Minibatch loss at step 800: 1.894511
Minibatch accuracy: 31.2%
Validation accuracy: 32.8%
Minibatch loss at step 850: 1.741181
Minibatch accuracy: 31.2%
Validation accuracy: 33.9%
Minibatch loss at step 900: 1.899962
Minibatch accuracy: 37.5%
Validation accuracy: 36.7%
Minibatch loss at step 950: 1.929227
Minibatch accuracy: 37.5%
Validation accuracy: 33.0%
Minibatch loss at step 1000: 1.671435
Minibatch accuracy: 37.5%
Validation accuracy: 36.7%
Minibatch loss at step 1050: 2.124148
Minibatch accuracy: 25.0%
Validation accuracy: 37.7%
Minibatch loss at step 1100: 1.898282
Minibatch accuracy: 18.8%
Validation accuracy: 37.7%
Minibatch loss at step 1150: 1.952750
Minibatch accuracy: 31.2%
Validation accuracy: 36.0%
Minibatch loss at step 1200: 1.512580
Minibatch accuracy: 50.0%
Validation accuracy: 43.4%
Minibatch loss at step 1250: 1.629997
Minibatch accuracy: 43.8%
Validation accuracy: 40.1%
Minibatch loss at step 1300: 1.712072
Minibatch accuracy: 62.5%
Validation accuracy: 46.1%
Minibatch loss at step 1350: 2.116345
Minibatch accuracy: 50.0%
Validation accuracy: 40.9%
Minibatch loss at step 1400: 1.630803
Minibatch accuracy: 43.8%
Validation accuracy: 39.9%
Minibatch loss at step 1450: 1.668554
Minibatch accuracy: 43.8%
Validation accuracy: 43.1%
Minibatch loss at step 1500: 1.650850
Minibatch accuracy: 31.2%
Validation accuracy: 44.3%
Minibatch loss at step 1550: 1.749657
Minibatch accuracy: 31.2%
Validation accuracy: 48.2%
Minibatch loss at step 1600: 1.506760
Minibatch accuracy: 37.5%
Validation accuracy: 47.2%
Minibatch loss at step 1650: 1.655661
Minibatch accuracy: 37.5%
Validation accuracy: 47.4%
Minibatch loss at step 1700: 2.183458
Minibatch accuracy: 12.5%
Validation accuracy: 43.5%
Minibatch loss at step 1750: 1.683201
Minibatch accuracy: 37.5%
Validation accuracy: 46.4%
Minibatch loss at step 1800: 1.963095
Minibatch accuracy: 12.5%
Validation accuracy: 47.1%
Minibatch loss at step 1850: 1.611518
Minibatch accuracy: 31.2%
Validation accuracy: 39.4%
Minibatch loss at step 1900: 1.658273
Minibatch accuracy: 50.0%
Validation accuracy: 48.3%
Minibatch loss at step 1950: 1.590845
Minibatch accuracy: 25.0%
Validation accuracy: 47.9%
Minibatch loss at step 2000: 2.187780
Minibatch accuracy: 18.8%
Validation accuracy: 52.0%
Minibatch loss at step 2050: 1.394493
Minibatch accuracy: 43.8%
Validation accuracy: 50.4%
Minibatch loss at step 2100: 2.159330
Minibatch accuracy: 18.8%
Validation accuracy: 49.3%
Minibatch loss at step 2150: 1.423224
Minibatch accuracy: 37.5%
Validation accuracy: 50.3%
Minibatch loss at step 2200: 1.767830
Minibatch accuracy: 31.2%
Validation accuracy: 51.1%
Minibatch loss at step 2250: 1.687594
Minibatch accuracy: 37.5%
Validation accuracy: 52.3%
Minibatch loss at step 2300: 1.296938
Minibatch accuracy: 50.0%
Validation accuracy: 51.9%
Minibatch loss at step 2350: 2.348637
Minibatch accuracy: 18.8%
Validation accuracy: 53.7%
Minibatch loss at step 2400: 1.952907
Minibatch accuracy: 18.8%
Validation accuracy: 57.0%
Minibatch loss at step 2450: 2.153319
Minibatch accuracy: 37.5%
Validation accuracy: 56.1%
Minibatch loss at step 2500: 2.116488
Minibatch accuracy: 25.0%
Validation accuracy: 60.0%
Minibatch loss at step 2550: 1.489463
Minibatch accuracy: 50.0%
Validation accuracy: 60.7%
Minibatch loss at step 2600: 1.149191
Minibatch accuracy: 43.8%
Validation accuracy: 56.6%
Minibatch loss at step 2650: 2.170089
Minibatch accuracy: 25.0%
Validation accuracy: 63.5%
Minibatch loss at step 2700: 1.525531
Minibatch accuracy: 37.5%
Validation accuracy: 59.3%
Minibatch loss at step 2750: 1.509228
Minibatch accuracy: 37.5%
Validation accuracy: 61.9%
Minibatch loss at step 2800: 1.147282
Minibatch accuracy: 56.2%
Validation accuracy: 62.5%
Minibatch loss at step 2850: 1.438620
Minibatch accuracy: 50.0%
Validation accuracy: 63.5%
Minibatch loss at step 2900: 1.439854
Minibatch accuracy: 43.8%
Validation accuracy: 64.3%
Minibatch loss at step 2950: 1.930614
Minibatch accuracy: 25.0%
Validation accuracy: 62.6%
Minibatch loss at step 3000: 1.589028
Minibatch accuracy: 43.8%
Validation accuracy: 62.4%
Minibatch loss at step 3050: 1.181139
Minibatch accuracy: 50.0%
Validation accuracy: 59.9%
Minibatch loss at step 3100: 1.706787
Minibatch accuracy: 50.0%
Validation accuracy: 64.2%
Minibatch loss at step 3150: 1.876871
Minibatch accuracy: 56.2%
Validation accuracy: 68.5%
Minibatch loss at step 3200: 1.360961
Minibatch accuracy: 37.5%
Validation accuracy: 69.2%
Minibatch loss at step 3250: 2.285524
Minibatch accuracy: 50.0%
Validation accuracy: 70.5%
Minibatch loss at step 3300: 1.441857
Minibatch accuracy: 50.0%
Validation accuracy: 70.5%
Minibatch loss at step 3350: 1.000102
Minibatch accuracy: 68.8%
Validation accuracy: 69.4%
Minibatch loss at step 3400: 0.899509
Minibatch accuracy: 68.8%
Validation accuracy: 71.0%
Minibatch loss at step 3450: 1.446827
Minibatch accuracy: 50.0%
Validation accuracy: 73.5%
Minibatch loss at step 3500: 1.269377
Minibatch accuracy: 50.0%
Validation accuracy: 67.3%
Minibatch loss at step 3550: 1.662549
Minibatch accuracy: 31.2%
Validation accuracy: 70.9%
Minibatch loss at step 3600: 1.274959
Minibatch accuracy: 56.2%
Validation accuracy: 72.7%
Minibatch loss at step 3650: 0.897081
Minibatch accuracy: 62.5%
Validation accuracy: 74.3%
Minibatch loss at step 3700: 1.394020
Minibatch accuracy: 50.0%
Validation accuracy: 74.4%
Minibatch loss at step 3750: 1.613516
Minibatch accuracy: 50.0%
Validation accuracy: 74.4%
Minibatch loss at step 3800: 0.905103
Minibatch accuracy: 68.8%
Validation accuracy: 72.7%
Minibatch loss at step 3850: 1.036171
Minibatch accuracy: 75.0%
Validation accuracy: 76.2%
Minibatch loss at step 3900: 0.679254
Minibatch accuracy: 81.2%
Validation accuracy: 74.8%
Minibatch loss at step 3950: 1.198567
Minibatch accuracy: 43.8%
Validation accuracy: 76.8%
Minibatch loss at step 4000: 1.513273
Minibatch accuracy: 43.8%
Validation accuracy: 74.3%
Minibatch loss at step 4050: 1.243160
Minibatch accuracy: 56.2%
Validation accuracy: 74.3%
Minibatch loss at step 4100: 0.809120
Minibatch accuracy: 62.5%
Validation accuracy: 75.2%
Minibatch loss at step 4150: 0.775453
Minibatch accuracy: 75.0%
Validation accuracy: 76.8%
Minibatch loss at step 4200: 0.922803
Minibatch accuracy: 75.0%
Validation accuracy: 76.6%
Minibatch loss at step 4250: 0.865568
Minibatch accuracy: 68.8%
Validation accuracy: 76.1%
Minibatch loss at step 4300: 1.376431
Minibatch accuracy: 56.2%
Validation accuracy: 77.6%
Minibatch loss at step 4350: 1.052437
Minibatch accuracy: 56.2%
Validation accuracy: 77.3%
Minibatch loss at step 4400: 0.973835
Minibatch accuracy: 50.0%
Validation accuracy: 78.2%
Minibatch loss at step 4450: 0.995051
Minibatch accuracy: 56.2%
Validation accuracy: 78.1%
Minibatch loss at step 4500: 1.235685
Minibatch accuracy: 50.0%
Validation accuracy: 78.3%
Minibatch loss at step 4550: 1.097849
Minibatch accuracy: 43.8%
Validation accuracy: 78.2%
Minibatch loss at step 4600: 1.154400
Minibatch accuracy: 62.5%
Validation accuracy: 77.8%
Minibatch loss at step 4650: 1.348891
Minibatch accuracy: 50.0%
Validation accuracy: 76.6%
Minibatch loss at step 4700: 0.996796
Minibatch accuracy: 62.5%
Validation accuracy: 79.4%
Minibatch loss at step 4750: 0.857328
Minibatch accuracy: 56.2%
Validation accuracy: 79.3%
Minibatch loss at step 4800: 0.978085
Minibatch accuracy: 62.5%
Validation accuracy: 79.3%
Minibatch loss at step 4850: 1.388877
Minibatch accuracy: 50.0%
Validation accuracy: 78.6%
Minibatch loss at step 4900: 1.505564
Minibatch accuracy: 50.0%
Validation accuracy: 78.1%
Minibatch loss at step 4950: 0.628969
Minibatch accuracy: 75.0%
Validation accuracy: 76.9%
Minibatch loss at step 5000: 0.963439
Minibatch accuracy: 56.2%
Validation accuracy: 79.2%
Test accuracy: 85.4%
#+end_example



* Assignment 5
:PROPERTIES:
:header-args: :session a5py :results output
:END:

** starter code
:PROPERTIES:
:ATTACH_DIR_INHERIT: t
:END:

The goal of this assignment is to train a Word2Vec skip-gram model over
[[http://mattmahoney.net/dc/textdata][Text8]] data.

#+BEGIN_SRC python :results none
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  # %matplotlib inline
  from __future__ import print_function
  import collections
  import math
  import numpy as np
  import os
  import random
  import tensorflow as tf
  import zipfile
  from matplotlib import pylab
  from six.moves import range
  from six.moves.urllib.request import urlretrieve
  from sklearn.manifold import TSNE
#+END_SRC

Download the data from the source website if necessary.

#+BEGIN_SRC python :results output
  Url = 'http://mattmahoney.net/dc/'

  def maybe_download(filename, expected_bytes):
    """Download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
      filename, _ = urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified %s' % filename)
    else:
      print(statinfo.st_size)
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  filename = maybe_download('text8.zip', 31344016)
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... >>> Found and verified text8.zip

Read the data into a string.

#+BEGIN_SRC python
     def read_data(filename):
       """Extract the first file enclosed in a zip file as a list of words"""
       with zipfile.ZipFile(filename) as f:
         data = tf.compat.as_str(f.read(f.namelist()[0])).split()
       return data
      
     words = read_data(filename)
     print('Data size %d' % len(words))
#+END_SRC

#+RESULTS:
: 
: ... ... ... ... >>> >>> Data size 17005207

Build the dictionary and replace rare words with UNK token.

#+BEGIN_SRC python
     vocabulary_size = 50000

     def build_dataset(words):
       count = [['UNK', -1]]
       count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
       dictionary = dict()
       for word, _ in count:
         dictionary[word] = len(dictionary)
       data = list()
       unk_count = 0
       for word in words:
         if word in dictionary:
           index = dictionary[word]
         else:
           index = 0  # dictionary['UNK']
           unk_count = unk_count + 1
         data.append(index)
       count[0][1] = unk_count
       reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) 
       return data, count, dictionary, reverse_dictionary

     data, count, dictionary, reverse_dictionary = build_dataset(words)
     print('Most common words (+UNK)', count[:5])
     print('Sample data', data[:10])
     del words  # Hint to reduce memory.
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> >>> Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
: Sample data [5236, 3084, 12, 6, 195, 2, 3134, 46, 59, 156]

Function to generate a training batch for the skip-gram model.

#+BEGIN_SRC python
  data_index = 0

  def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips <= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1 # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
      buffer.append(data[data_index])
      data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
      target = skip_window  # target label at the center of the buffer
      targets_to_avoid = [ skip_window ]
      for j in range(num_skips):
        while target in targets_to_avoid:
          target = random.randint(0, span - 1)
        targets_to_avoid.append(target)
        batch[i * num_skips + j] = buffer[skip_window]
        labels[i * num_skips + j, 0] = buffer[target]
      buffer.append(data[data_index])
      data_index = (data_index + 1) % len(data)
    return batch, labels

  print('data:', [reverse_dictionary[di] for di in data[:8]])

  for num_skips, skip_window in [(2, 1), (4, 2)]:
      data_index = 0
      batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)
      print('\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))
      print('    batch:', [reverse_dictionary[bi] for bi in batch])
      print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])
#+END_SRC

#+RESULTS:
#+begin_example

>>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']
>>> ... ... ... ... ... ... 
with num_skips = 2 and skip_window = 1:
    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']
    labels: ['as', 'anarchism', 'a', 'originated', 'as', 'term', 'of', 'a']

with num_skips = 4 and skip_window = 2:
    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']
    labels: ['term', 'anarchism', 'a', 'originated', 'of', 'as', 'originated', 'term']
#+end_example

Train a skip-gram model.

#+BEGIN_SRC python
  batch_size = 128
  embedding_size = 128 # Dimension of the embedding vector.
  skip_window = 1 # How many words to consider left and right.
  num_skips = 2 # How many times to reuse an input to generate a label.
  # We pick a random validation set to sample nearest neighbors. here we limit the
  # validation samples to the words that have a low numeric ID, which by
  # construction are also the most frequent. 
  valid_size = 16 # Random set of words to evaluate similarity on.
  valid_window = 100 # Only pick dev samples in the head of the distribution.
  valid_examples = np.array(random.sample(range(valid_window), valid_size))
  num_sampled = 64 # Number of negative examples to sample.
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)
    # Variables.
    embeddings = tf.Variable(
      tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
      tf.truncated_normal([vocabulary_size, embedding_size],
                           stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))
    # Model.
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
      tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,
                                 labels=train_labels, num_sampled=num_sampled, 
                                 num_classes=vocabulary_size))
    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities 
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  num_steps = 100001

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    average_loss = 0
    for step in range(num_steps):
      batch_data, batch_labels = generate_batch(
        batch_size, num_skips, skip_window)
      feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
      _, l = session.run([optimizer, loss], feed_dict=feed_dict)
      average_loss += l
      if step % 2000 == 0:
        if step > 0:
          average_loss = average_loss / 2000
        # The average loss is an estimate of the loss over the last 2000 batches.
        print('Average loss at step %d: %f' % (step, average_loss))
        average_loss = 0
      # note that this is expensive (~20% slowdown if computed every 500 steps)
      if step % 10000 == 0:
        sim = similarity.eval()
        for i in range(valid_size):
          valid_word = reverse_dictionary[valid_examples[i]]
          top_k = 8 # number of nearest neighbors
          nearest = (-sim[i, :]).argsort()[1:top_k+1]
          log = 'Nearest to %s:' % valid_word
          for k in range(top_k):
            close_word = reverse_dictionary[nearest[k]]
            log = '%s %s,' % (log, close_word)
          print(log)
    final_embeddings = normalized_embeddings.eval()
#+END_SRC

#+RESULTS:
#+begin_example
2017-06-15 18:08:02.513700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-06-15 18:08:02.516168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.66GiB
2017-06-15 18:08:02.516249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-06-15 18:08:02.516276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-06-15 18:08:02.516305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2017-06-15 18:08:03.777980: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-15 18:08:03.778053: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-15 18:08:03.781478: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x5603ba66c610 executing computations on platform Host. Devices:
2017-06-15 18:08:03.781535: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-06-15 18:08:03.782140: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-15 18:08:03.782195: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-15 18:08:03.784474: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x5603ba680b70 executing computations on platform CUDA. Devices:
2017-06-15 18:08:03.784521: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Quadro M2000M, Compute Capability 5.0
Initialized
Average loss at step 0: 8.152785
Nearest to zero: icrm, greeneville, fastened, bolland, grays, combi, svetlana, incurred,
Nearest to he: sennett, cheat, litt, arcadia, soares, majapahit, postpositions, leonidas,
Nearest to people: infantryman, southside, consumerism, philibert, ranges, bitstream, kauravas, confess,
Nearest to six: wicket, emcee, peas, sie, marini, wf, bewildering, lifeboats,
Nearest to many: jackson, exists, karo, composes, khufu, sucked, backer, passionately,
Nearest to however: blok, px, sinless, ise, emblems, venona, austerity, coughing,
Nearest to that: karabakh, jarrett, betrays, luo, mannheim, grove, reoccupied, swapping,
Nearest to also: imperfections, macduff, originator, stow, hitters, arnold, analog, porpoises,
Nearest to during: outpatient, marcia, improvisation, visions, panspermia, refugee, unstructured, gpp,
Nearest to world: churchyard, fluoxetine, tort, malls, taoiseach, unconvinced, tx, dearly,
Nearest to two: onwards, ordovician, emulator, published, mauritanian, root, musk, gated,
Nearest to some: ddd, unintentional, calorie, recension, jewellery, automorphism, class, digimon,
Nearest to not: digitally, netware, hurdler, parade, scans, popularity, letters, received,
Nearest to most: guayaquil, mencius, hata, hypnos, felt, motorist, designed, additional,
Nearest to between: naghten, highbury, january, weigh, sacrilegious, adf, thing, blasts,
Nearest to i: shuffle, fists, broadcasting, edgard, environmentally, cigarettes, mujibur, soulful,
Average loss at step 2000: 4.360054
Average loss at step 4000: 3.866475
Average loss at step 6000: 3.788148
Average loss at step 8000: 3.689236
Average loss at step 10000: 3.612638
Nearest to zero: nine, eight, seven, six, four, five, three, two,
Nearest to he: it, they, she, there, who, sennett, arcadia, cheat,
Nearest to people: infantryman, countries, reintroducing, tibeto, ranges, southside, influential, fanclub,
Nearest to six: seven, eight, four, three, five, nine, zero, two,
Nearest to many: some, all, backer, composes, several, aman, these, heavier,
Nearest to however: but, px, blok, tavern, venona, although, miscegenation, jogaila,
Nearest to that: which, it, geq, bara, getz, wears, makeshift, scrambled,
Nearest to also: still, schirra, chydenius, ads, bech, which, now, arnold,
Nearest to during: unstructured, biometric, at, in, outpatient, improvisation, marcia, nder,
Nearest to world: taoiseach, eschewed, malls, churchyard, meaningful, certainly, smallville, dilapidated,
Nearest to two: three, four, five, seven, six, nine, eight, one,
Nearest to some: many, these, plate, parsed, crystallized, shaku, jewellery, rupiah,
Nearest to not: it, novi, they, conspiracies, parade, enhancing, dusty, sarcophagus,
Nearest to most: motorist, mencius, uneven, macross, additional, blending, crumbling, designed,
Nearest to between: in, presidency, canned, naghten, worldwide, with, adf, elitism,
Nearest to i: t, personally, broadcasting, taiwan, shower, cranes, notices, webern,
Average loss at step 12000: 3.605409
Average loss at step 14000: 3.572850
Average loss at step 16000: 3.413008
Average loss at step 18000: 3.460469
Average loss at step 20000: 3.538633
Nearest to zero: five, six, four, seven, three, eight, nine, two,
Nearest to he: it, they, she, who, there, which, complementing, lanterns,
Nearest to people: countries, infantryman, ranges, reintroducing, influential, klement, those, shanks,
Nearest to six: five, nine, eight, four, seven, three, zero, two,
Nearest to many: some, several, all, these, other, composes, aman, various,
Nearest to however: but, although, that, miscegenation, jogaila, tavern, again, blok,
Nearest to that: which, but, however, murat, this, mooted, governance, bara,
Nearest to also: now, which, still, not, often, chydenius, never, schirra,
Nearest to during: at, unstructured, in, assay, refugee, subsidy, after, with,
Nearest to world: taoiseach, eschewed, u, cleese, malls, churchyard, speech, meaningful,
Nearest to two: three, five, four, six, zero, one, eight, seven,
Nearest to some: many, these, several, all, their, other, his, jewellery,
Nearest to not: novi, they, also, generally, it, to, still, overlapped,
Nearest to most: escherichia, modern, motorist, analysis, colliery, baroness, macross, uneven,
Nearest to between: with, rogue, presidency, downing, in, canned, cf, within,
Nearest to i: ii, iii, astride, notices, t, shower, cranes, we,
Average loss at step 22000: 3.508200
Average loss at step 24000: 3.490601
Average loss at step 26000: 3.481171
Average loss at step 28000: 3.480548
Average loss at step 30000: 3.503050
Nearest to zero: five, six, four, eight, seven, three, nine, two,
Nearest to he: it, she, they, there, who, lanterns, weaknesses, polygamous,
Nearest to people: countries, klement, laying, reintroducing, those, ranges, infantryman, michelangelo,
Nearest to six: four, eight, five, nine, seven, three, two, zero,
Nearest to many: some, several, these, composes, all, alois, various, values,
Nearest to however: but, although, while, miscegenation, jogaila, that, though, px,
Nearest to that: which, this, but, what, mooted, however, geq, mclaughlin,
Nearest to also: now, still, always, often, which, never, schirra, ads,
Nearest to during: in, after, at, until, before, involve, unstructured, subsidy,
Nearest to world: taoiseach, cleese, u, repealing, eschewed, atmosphere, speech, churchyard,
Nearest to two: four, three, one, six, seven, five, eight, nine,
Nearest to some: many, these, several, all, their, this, various, adaptable,
Nearest to not: they, novi, generally, to, it, tagging, still, gently,
Nearest to most: escherichia, baroness, more, many, analysis, reviewers, modern, motorist,
Nearest to between: with, in, adf, downing, within, presidency, autodesk, from,
Nearest to i: ii, you, iii, t, kamala, we, shower, astride,
Average loss at step 32000: 3.500849
Average loss at step 34000: 3.492524
Average loss at step 36000: 3.458225
Average loss at step 38000: 3.302418
Average loss at step 40000: 3.430410
Nearest to zero: five, seven, nine, eight, six, three, four, two,
Nearest to he: she, it, they, there, who, i, eventually, soon,
Nearest to people: countries, klement, those, actions, epilogue, others, members, reintroducing,
Nearest to six: seven, four, five, eight, nine, three, two, one,
Nearest to many: some, several, these, various, both, all, alois, most,
Nearest to however: but, although, that, which, though, and, while, where,
Nearest to that: which, however, this, ambient, what, but, where, indentured,
Nearest to also: often, still, now, which, never, usually, always, not,
Nearest to during: in, before, after, within, at, heysel, involve, subsidy,
Nearest to world: u, cleese, repealing, speech, lattices, malls, felix, taoiseach,
Nearest to two: three, four, five, six, seven, one, eight, nine,
Nearest to some: many, these, several, this, any, their, the, certain,
Nearest to not: they, never, it, still, generally, also, to, novi,
Nearest to most: more, baroness, extremely, escherichia, many, scouring, forties, annual,
Nearest to between: with, adf, within, wabash, from, sennacherib, maxine, pontifex,
Nearest to i: you, we, t, ii, he, iii, they, amherst,
Average loss at step 42000: 3.433557
Average loss at step 44000: 3.456737
Average loss at step 46000: 3.453549
Average loss at step 48000: 3.351593
Average loss at step 50000: 3.383822
Nearest to zero: seven, four, five, six, eight, nine, three, two,
Nearest to he: she, it, they, there, who, eventually, but, this,
Nearest to people: countries, actions, players, others, members, klement, nestorian, those,
Nearest to six: eight, seven, four, nine, three, five, two, zero,
Nearest to many: some, several, these, both, various, most, electrophilic, other,
Nearest to however: but, although, though, where, that, while, when, and,
Nearest to that: which, however, this, where, but, what, geq, mooted,
Nearest to also: now, which, often, still, always, usually, never, generally,
Nearest to during: in, at, after, within, until, when, including, before,
Nearest to world: cleese, felix, repealing, u, unit, flocked, malls, comprehensive,
Nearest to two: three, four, one, six, five, seven, eight, zero,
Nearest to some: many, several, these, various, both, their, most, the,
Nearest to not: never, generally, still, they, now, novi, dusty, always,
Nearest to most: more, extremely, many, some, less, baroness, uneven, crusading,
Nearest to between: with, within, in, adf, from, gospels, among, sennacherib,
Nearest to i: you, we, ii, t, iii, they, x, ebenezer,
Average loss at step 52000: 3.434259
Average loss at step 54000: 3.428692
Average loss at step 56000: 3.439087
Average loss at step 58000: 3.398797
Average loss at step 60000: 3.392078
Nearest to zero: five, four, eight, six, seven, nine, three, two,
Nearest to he: she, it, they, there, who, soon, eventually, we,
Nearest to people: countries, players, those, others, students, members, actions, men,
Nearest to six: eight, four, five, seven, nine, three, zero, two,
Nearest to many: some, several, these, various, other, all, both, most,
Nearest to however: but, although, though, when, that, which, and, where,
Nearest to that: which, what, this, however, it, mooted, there, indentured,
Nearest to also: now, still, usually, often, never, sometimes, generally, always,
Nearest to during: after, before, in, when, until, including, at, within,
Nearest to world: lattices, u, cleese, felix, malls, churchyard, coolidge, unit,
Nearest to two: three, four, one, six, five, seven, eight, zero,
Nearest to some: many, several, these, all, any, various, most, the,
Nearest to not: they, never, still, usually, technically, generally, you, now,
Nearest to most: more, many, some, baroness, electress, less, extremely, among,
Nearest to between: with, within, among, remaining, from, gospels, in, sennacherib,
Nearest to i: you, we, ii, organizes, t, iii, they, ebenezer,
Average loss at step 62000: 3.246887
Average loss at step 64000: 3.257387
Average loss at step 66000: 3.400697
Average loss at step 68000: 3.395861
Average loss at step 70000: 3.359366
Nearest to zero: five, four, seven, eight, six, nine, three, two,
Nearest to he: she, it, they, there, eventually, who, soon, we,
Nearest to people: countries, players, students, women, others, men, philosophers, members,
Nearest to six: eight, seven, four, five, nine, three, two, zero,
Nearest to many: some, several, these, various, both, all, most, each,
Nearest to however: but, although, though, where, while, that, when, which,
Nearest to that: which, however, what, this, but, where, portillo, mooted,
Nearest to also: now, still, which, often, usually, never, schirra, generally,
Nearest to during: after, before, in, until, throughout, within, at, while,
Nearest to world: u, lattices, churchyard, cleese, coolidge, felix, exact, malls,
Nearest to two: three, six, four, seven, one, five, eight, zero,
Nearest to some: many, several, these, all, various, most, each, any,
Nearest to not: never, still, now, generally, they, technically, usually, normally,
Nearest to most: more, many, some, less, extremely, electress, uneven, baroness,
Nearest to between: within, with, from, among, in, gospels, sennacherib, adf,
Nearest to i: you, we, ii, organizes, ebenezer, they, cranes, tatiana,
Average loss at step 72000: 3.372637
Average loss at step 74000: 3.349423
Average loss at step 76000: 3.315401
Average loss at step 78000: 3.355518
Average loss at step 80000: 3.373379
Nearest to zero: five, seven, eight, six, four, three, nine, two,
Nearest to he: she, it, they, there, who, we, soon, originally,
Nearest to people: students, countries, men, women, players, others, those, members,
Nearest to six: five, seven, eight, four, three, nine, two, zero,
Nearest to many: some, several, these, various, both, all, most, those,
Nearest to however: but, although, though, that, where, while, which, forgery,
Nearest to that: which, however, where, what, this, levitt, hortense, mooted,
Nearest to also: now, still, often, never, which, usually, always, sometimes,
Nearest to during: after, before, in, until, throughout, when, despite, although,
Nearest to world: u, lattices, cleese, felix, aircrew, repealing, malls, speech,
Nearest to two: three, four, six, five, seven, one, eight, zero,
Nearest to some: many, several, these, various, most, certain, all, both,
Nearest to not: still, normally, generally, technically, never, usually, always, they,
Nearest to most: more, some, many, among, less, extremely, especially, electress,
Nearest to between: within, with, among, in, from, rogue, sennacherib, downing,
Nearest to i: you, we, ii, organizes, they, iii, cloak, t,
Average loss at step 82000: 3.410528
Average loss at step 84000: 3.412772
Average loss at step 86000: 3.388072
Average loss at step 88000: 3.350503
Average loss at step 90000: 3.366962
Nearest to zero: seven, five, eight, six, four, nine, three, two,
Nearest to he: she, it, they, there, soon, who, originally, later,
Nearest to people: students, players, men, women, countries, those, members, jews,
Nearest to six: seven, eight, five, four, nine, three, zero, two,
Nearest to many: some, several, these, various, all, most, certain, numerous,
Nearest to however: but, although, though, that, where, which, nevertheless, while,
Nearest to that: which, however, what, mooted, instead, portillo, neurologic, accidentally,
Nearest to also: now, often, still, which, never, generally, always, actually,
Nearest to during: after, before, until, in, although, while, under, throughout,
Nearest to world: actual, lattices, psilocybin, eschewed, churchyard, aircrew, cleese, recreation,
Nearest to two: three, four, five, seven, six, one, eight, nine,
Nearest to some: many, several, these, most, various, certain, any, all,
Nearest to not: normally, still, never, generally, technically, we, also, tagging,
Nearest to most: more, less, many, some, all, among, particularly, nasir,
Nearest to between: with, within, among, from, jamal, ensuing, romano, into,
Nearest to i: you, we, g, iii, ii, frac, they, t,
Average loss at step 92000: 3.398437
Average loss at step 94000: 3.258491
Average loss at step 96000: 3.357522
Average loss at step 98000: 3.241308
Average loss at step 100000: 3.355222
Nearest to zero: five, four, seven, eight, six, nine, three, two,
Nearest to he: she, it, they, there, we, who, soon, never,
Nearest to people: players, students, countries, women, men, children, others, klement,
Nearest to six: seven, eight, four, five, nine, three, two, zero,
Nearest to many: several, some, various, these, numerous, all, few, certain,
Nearest to however: but, although, though, that, especially, where, nevertheless, chicks,
Nearest to that: which, what, however, hawick, actually, hortense, gzip, how,
Nearest to also: now, still, never, often, actually, bonhomme, sometimes, which,
Nearest to during: after, in, before, until, following, although, throughout, at,
Nearest to world: tendon, lattices, actual, cleese, hellenistic, duffy, eschewed, coolidge,
Nearest to two: three, four, five, six, one, eight, seven, zero,
Nearest to some: many, several, these, any, certain, various, all, the,
Nearest to not: never, still, they, normally, always, generally, now, technically,
Nearest to most: more, less, nasir, extremely, particularly, especially, among, many,
Nearest to between: with, within, among, sennacherib, adf, hooking, arecibo, cereal,
Nearest to i: we, you, ii, they, iii, frac, t, never,
#+end_example


#+BEGIN_SRC python
  num_points = 400

  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
  two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  def plot(embeddings, labels):
    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'
    pylab.figure(figsize=(15,15))  # in inches
    for i, label in enumerate(labels):
      x, y = embeddings[i,:]
      pylab.scatter(x, y)
      pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',
                     ha='right', va='bottom')
    # pylab.show()

  words = [reverse_dictionary[i] for i in range(1, num_points+1)]
#+END_SRC

#+RESULTS:
  
#+BEGIN_SRC python :results file
  plot(two_d_embeddings, words)
  img_file = "imgs/python-matplot-fig.png"
  pylab.savefig(img_file)
  
  img_file
#+END_SRC

#+RESULTS:
[[file:imgs/python-matplot-fig.png]]


** Problem

An alternative to skip-gram is another Word2Vec model called [[http://arxiv.org/abs/1301.3781][CBOW]] (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.


#+BEGIN_SRC python :results none
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  # %matplotlib inline
  from __future__ import print_function
  import collections
  import math
  import numpy as np
  import os
  import random
  import tensorflow as tf
  import zipfile
  from matplotlib import pylab
  from six.moves import range
  from six.moves.urllib.request import urlretrieve
  from sklearn.manifold import TSNE
#+END_SRC

#+BEGIN_SRC python :results output
  Url = 'http://mattmahoney.net/dc/'

  def maybe_download(filename, expected_bytes):
    """Download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
      filename, _ = urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified %s' % filename)
    else:
      print(statinfo.st_size)
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  filename = maybe_download('text8.zip', 31344016)
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... >>> Found and verified text8.zip

Read the data into a string.

#+BEGIN_SRC python
     def read_data(filename):
       """Extract the first file enclosed in a zip file as a list of words"""
       with zipfile.ZipFile(filename) as f:
         data = tf.compat.as_str(f.read(f.namelist()[0])).split()
       return data
      
     words = read_data(filename)
     print('Data size %d' % len(words))
#+END_SRC

#+RESULTS:
: 
: ... ... ... ... >>> >>> Data size 17005207

Build the dictionary and replace rare words with UNK token.

#+BEGIN_SRC python
     vocabulary_size = 50000

     def build_dataset(words):
       count = [['UNK', -1]]
       count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
       dictionary = dict()
       for word, _ in count:
         dictionary[word] = len(dictionary)
       data = list()
       unk_count = 0
       for word in words:
         if word in dictionary:
           index = dictionary[word]
         else:
           index = 0  # dictionary['UNK']
           unk_count = unk_count + 1
         data.append(index)
       count[0][1] = unk_count
       reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) 
       return data, count, dictionary, reverse_dictionary

     data, count, dictionary, reverse_dictionary = build_dataset(words)
     print('Most common words (+UNK)', count[:5])
     print('Sample data', data[:10])
     del words  # Hint to reduce memory.
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> >>> Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
: Sample data [5234, 3081, 12, 6, 195, 2, 3137, 46, 59, 156]

Function to generate a training batch for the CBOW model.
#+BEGIN_SRC python
  def generate_batch_Q(batch_size, radius, num_epochs=None):
    window_size = 2*radius+1
    B = batch_size
    N = tf.Variable(len(data), dtype = tf.int32)
    sy_data = tf.Variable(data)
    index_q = tf.train.range_input_producer(limit=N-window_size, shuffle=True, 
                                            num_epochs=num_epochs)
    left_ix = index_q.dequeue()
    train = sy_data[left_ix:left_ix+window_size]
    ctx_l, label, ctx_r = tf.train.batch(tensors=tf.split(train, [radius,1,radius]),
                                         batch_size=B, dynamic_pad=True, name="data_batch",
                                         allow_smaller_final_batch=True)
    return tf.concat([ctx_l, ctx_r], axis=1), label
#+END_SRC

#+RESULTS:

Train a skip-gram model.
#+BEGIN_SRC python
  batch_size = 128
  embedding_size = 128 # Dimension of the embedding vector.
  radius = 4 # number of history / future words (total = R x 2)
  # validation samples to the words that have a low numeric ID, which by
  # construction are also the most frequent. 
  valid_size = 16 # Random set of words to evaluate similarity on.
  valid_window = 100 # Only pick dev samples in the head of the distribution.
  valid_examples = np.array(random.sample(range(valid_window), valid_size))
  num_sampled = 64 # Number of negative examples to sample.
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Input data.
    train_dataset, train_labels = generate_batch_Q(batch_size, radius)
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)
    # Variables.
    embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
      tf.truncated_normal([vocabulary_size, embedding_size],
                           stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))
    # Model.
    # Look up (averaged) embeddings for inputs.
    embed = tf.reduce_mean(tf.nn.embedding_lookup(embeddings, train_dataset), axis=1)  # take mean of embeddings over contexts
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
      tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,
                                 labels=train_labels, num_sampled=num_sampled,
                                 num_classes=vocabulary_size))
    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities 
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  num_steps = 100001

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    q_threads = tf.train.start_queue_runners()   
    print('Initialized')
    average_loss = 0
    for step in range(num_steps):
      _, l = session.run([optimizer, loss])
      average_loss += l
      if step % 2000 == 0:
        if step > 0:
          average_loss = average_loss / 2000
        # The average loss is an estimate of the loss over the last 2000 batches.
        print('Average loss at step %d: %f' % (step, average_loss))
        average_loss = 0
      # note that this is expensive (~20% slowdown if computed every 500 steps)
      if step % 10000 == 0:
        sim = similarity.eval()
        for i in range(valid_size):
          valid_word = reverse_dictionary[valid_examples[i]]
          top_k = 8 # number of nearest neighbors
          nearest = (-sim[i, :]).argsort()[1:top_k+1]
          log = 'Nearest to %s:' % valid_word
          for k in range(top_k):
            close_word = reverse_dictionary[nearest[k]]
            log = '%s %s,' % (log, close_word)
          print(log)
    final_embeddings = normalized_embeddings.eval()
#+END_SRC

#+RESULTS:
#+begin_example

>>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2017-06-18 21:17:40.837351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-06-18 21:17:40.837849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.63GiB
2017-06-18 21:17:40.837861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-06-18 21:17:40.837865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-06-18 21:17:40.837871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2017-06-18 21:17:41.067831: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-18 21:17:41.067851: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-18 21:17:41.068648: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a405303bf0 executing computations on platform Host. Devices:
2017-06-18 21:17:41.068666: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-06-18 21:17:41.068778: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-18 21:17:41.068786: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-18 21:17:41.069181: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a40534c1d0 executing computations on platform CUDA. Devices:
2017-06-18 21:17:41.069190: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Quadro M2000M, Compute Capability 5.0
Initialized
Average loss at step 0: 7.901086
Nearest to also: apparent, wealthy, mussels, psychiatric, hitpa, walras, exmoor, respecting,
Nearest to have: barristers, denounces, obsessive, staircase, irv, summon, marguerite, scars,
Nearest to been: weil, vax, cabbie, realisation, hydrate, harmless, confucianism, blaxploitation,
Nearest to new: interface, narrated, flaherty, kit, criticism, physicalism, bosom, thursday,
Nearest to of: loonie, cartagena, rambla, observing, almeida, osage, aron, securing,
Nearest to i: yoshinkan, supper, venue, lili, belvedere, brilliance, assr, maximilian,
Nearest to its: conductor, cds, floated, fretting, malleus, tae, asymptotically, jpg,
Nearest to about: analog, minster, cheney, redstone, mostar, esdi, muni, go,
Nearest to most: soundly, glucose, inlets, band, euphonium, mustafa, meccan, watchdog,
Nearest to not: beethoven, decentralisation, wycliffe, ellipsis, humble, waas, ferguson, romanization,
Nearest to may: mcmaster, coincidental, pejorative, meat, husserl, macedonians, confirmed, ism,
Nearest to between: df, mathematik, harvests, synonym, stand, weidman, exceptionally, dwarfed,
Nearest to there: legacy, caesura, chronicon, lollard, coveted, arguably, cockpits, trustworthy,
Nearest to more: ravel, tide, violins, dupuis, escort, banned, intermission, research,
Nearest to see: fx, mistakenly, ya, labeling, metered, osip, childless, middleweight,
Nearest to two: environments, pir, fingal, must, disordered, magnificat, fireplace, rose,
2017-06-18 21:17:42.165060: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2749 get requests, put_count=2523 evicted_count=1000 eviction_rate=0.396354 and unsatisfied allocation rate=0.482357
2017-06-18 21:17:42.165083: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2017-06-18 21:17:42.650474: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4898 get requests, put_count=4809 evicted_count=1000 eviction_rate=0.207943 and unsatisfied allocation rate=0.227031
2017-06-18 21:17:42.650497: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
Average loss at step 2000: 4.352453
Average loss at step 4000: 3.896251
Average loss at step 6000: 3.775162
Average loss at step 8000: 3.711626
Average loss at step 10000: 3.654511
Nearest to also: apparent, systems, eventually, mussels, atheistic, weishaupt, gust, exmoor,
Nearest to have: has, had, be, are, denounces, marguerite, brandt, barristers,
Nearest to been: vax, harmless, rance, confucianism, weil, irritate, realisation, blaxploitation,
Nearest to new: interface, iceni, flaherty, lone, bosom, viol, precedents, secunda,
Nearest to of: loonie, among, rambla, undertaken, securing, observing, admiral, cataloging,
Nearest to i: you, falklands, abbott, g, thanksgiving, lili, powerbook, sedative,
Nearest to its: this, manchukuo, the, kushan, asymptotically, rowboat, their, hada,
Nearest to about: redstone, if, code, vyasa, analog, quine, louise, pooh,
Nearest to most: tower, language, soundly, glucose, raps, nsfnet, stram, bing,
Nearest to not: still, what, ellipsis, decentralisation, it, however, beethoven, homosexuality,
Nearest to may: could, can, will, must, coincidental, would, should, distinguish,
Nearest to between: weidman, over, synonym, penchant, quell, tats, sidebands, cosby,
Nearest to there: legacy, sept, trustworthy, these, cuyp, farms, inquire, funimation,
Nearest to more: crispy, escort, intermission, boats, spaceman, ravel, arithmetical, less,
Nearest to see: fx, inheritors, metered, mistakenly, francia, california, cartels, assistance,
Nearest to two: three, missoula, one, arturo, environments, pir, spirituals, ste,
Average loss at step 12000: 3.626777
Average loss at step 14000: 3.586024
Average loss at step 16000: 3.556536
Average loss at step 18000: 3.533802
Average loss at step 20000: 3.507083
Nearest to also: apparent, gust, abram, systems, eventually, mussels, wealthy, atheistic,
Nearest to have: had, has, having, brandt, be, straddles, barristers, are,
Nearest to been: become, rance, vax, harmless, realisation, embattled, irritate, globes,
Nearest to new: interface, flaherty, iceni, lup, ftl, rrez, kit, lone,
Nearest to of: among, rambla, loonie, securing, cataloging, admiral, undertaken, observing,
Nearest to i: you, me, g, powerbook, abbott, lili, m, sedative,
Nearest to its: their, kushan, the, this, asymptotically, manchukuo, rowboat, hada,
Nearest to about: redstone, if, since, louise, little, code, vyasa, strictly,
Nearest to most: tower, language, soundly, ethnic, glucose, nsfnet, bearers, among,
Nearest to not: still, what, ellipsis, homosexuality, centrepiece, specialized, decentralisation, thermotropic,
Nearest to may: could, can, must, will, should, would, might, cannot,
Nearest to between: weidman, over, department, synonym, penchant, tats, inflow, quell,
Nearest to there: legacy, trustworthy, cuyp, sept, these, marrying, landings, inquire,
Nearest to more: less, crispy, rather, boats, spaceman, escort, thence, dane,
Nearest to see: fx, inheritors, metered, assistance, francia, mistakenly, cartels, madge,
Nearest to two: three, arturo, missoula, veterinarians, strictly, four, ste, radio,
Average loss at step 22000: 3.482478
Average loss at step 24000: 3.460762
Average loss at step 26000: 3.441429
Average loss at step 28000: 3.427378
Average loss at step 30000: 3.417792
Nearest to also: apparent, abram, gust, pronounce, witt, wealthy, stockhausen, hitpa,
Nearest to have: had, has, having, brandt, straddles, be, were, barristers,
Nearest to been: become, rance, embattled, vax, realisation, harmless, several, globes,
Nearest to new: flaherty, iceni, ftl, enabled, interface, lup, kit, rrez,
Nearest to of: among, rambla, securing, loonie, cataloging, observing, undertaken, throughout,
Nearest to i: you, me, powerbook, g, abbott, we, t, lili,
Nearest to its: their, kushan, the, lydian, this, asymptotically, manchukuo, potential,
Nearest to about: redstone, louise, little, if, code, remarking, mounted, since,
Nearest to most: tower, among, ethnic, bearers, more, soundly, many, upsets,
Nearest to not: still, what, ellipsis, homosexuality, centrepiece, specialized, decentralisation, struma,
Nearest to may: could, can, must, will, should, would, might, cannot,
Nearest to between: weidman, over, tats, forbade, other, department, inflow, coretta,
Nearest to there: legacy, cuyp, trustworthy, sept, these, marrying, landings, lollard,
Nearest to more: less, rather, crispy, boats, escort, spaceman, faster, thence,
Nearest to see: fx, inheritors, metered, assistance, madge, thyroid, mistakenly, dieu,
Nearest to two: three, four, arturo, one, veterinarians, turret, five, alpinus,
Average loss at step 32000: 3.396475
Average loss at step 34000: 3.385493
Average loss at step 36000: 3.370742
Average loss at step 38000: 3.359877
Average loss at step 40000: 3.344813
Nearest to also: abram, apparent, pronounce, gust, hitpa, below, witt, craters,
Nearest to have: has, had, having, brandt, straddles, include, be, were,
Nearest to been: become, rance, realisation, vax, embattled, globes, irritate, bolshevik,
Nearest to new: flaherty, iceni, ftl, lup, enabled, kit, adhesive, lombok,
Nearest to of: among, rambla, throughout, observing, securing, cataloging, undertaken, loonie,
Nearest to i: you, me, we, g, powerbook, rias, t, my,
Nearest to its: their, kushan, the, lydian, jodie, potential, stays, supporting,
Nearest to about: redstone, louise, code, little, mounted, analog, among, severe,
Nearest to most: more, tower, many, bearers, among, ethnic, upsets, decline,
Nearest to not: still, ellipsis, specialized, homosexuality, struma, centrepiece, what, decentralisation,
Nearest to may: could, can, must, should, will, would, might, cannot,
Nearest to between: weidman, over, forbade, department, tats, selangor, within, stowe,
Nearest to there: legacy, trustworthy, sept, cuyp, marrying, lollard, freefall, funimation,
Nearest to more: less, rather, most, faster, boats, crispy, spaceman, escort,
Nearest to see: fx, metered, inheritors, madge, assistance, thyroid, dieu, routledge,
Nearest to two: three, four, arturo, five, missoula, mile, one, veterinarians,
Average loss at step 42000: 3.335228
Average loss at step 44000: 3.319990
Average loss at step 46000: 3.313496
Average loss at step 48000: 3.302013
Average loss at step 50000: 3.289619
Nearest to also: below, abram, pronounce, hitpa, selene, wealthy, apparent, dwan,
Nearest to have: has, had, having, brandt, be, straddles, include, were,
Nearest to been: become, rance, globes, embattled, irritate, realisation, vax, led,
Nearest to new: lup, flaherty, iceni, ftl, kit, enabled, adhesive, segment,
Nearest to of: throughout, among, rambla, securing, observing, cataloging, undertaken, loonie,
Nearest to i: you, me, we, my, g, god, rias, powerbook,
Nearest to its: their, kushan, the, lydian, jodie, our, his, potential,
Nearest to about: louise, redstone, bogart, severe, rousseau, keynote, mounted, kajang,
Nearest to most: more, many, tower, bearers, among, upsets, extremely, decline,
Nearest to not: still, ellipsis, specialized, homosexuality, struma, sustainable, centrepiece, treeless,
Nearest to may: could, can, must, should, would, will, might, cannot,
Nearest to between: forbade, weidman, both, within, selangor, department, tats, stowe,
Nearest to there: legacy, cuyp, trustworthy, sept, marrying, lollard, mapuche, funimation,
Nearest to more: less, rather, most, faster, boats, spaceman, violins, escort,
Nearest to see: fx, metered, madge, assistance, thyroid, inheritors, dieu, cooperatives,
Nearest to two: three, four, licensee, mile, arturo, veterinarians, delaware, one,
Average loss at step 52000: 3.285617
Average loss at step 54000: 3.267626
Average loss at step 56000: 3.264203
Average loss at step 58000: 3.252832
Average loss at step 60000: 3.240172
Nearest to also: below, abram, pronounce, selene, disambiguation, craters, wealthy, hitpa,
Nearest to have: has, had, having, brandt, include, straddles, were, barristers,
Nearest to been: become, rance, globes, led, come, embattled, irritate, kaltenbrunner,
Nearest to new: lup, flaherty, iceni, adhesive, kit, enabled, ftl, segment,
Nearest to of: throughout, rambla, securing, among, cataloging, loonie, observing, undertaken,
Nearest to i: you, me, we, my, god, g, rias, glare,
Nearest to its: their, kushan, his, lydian, bengal, stays, malleus, jodie,
Nearest to about: louise, redstone, bogart, rousseau, severe, kajang, keynote, caterpillars,
Nearest to most: more, many, bearers, tower, some, among, upsets, decline,
Nearest to not: still, homosexuality, ellipsis, struma, sustainable, specialized, centrepiece, oahu,
Nearest to may: could, must, can, should, would, will, might, cannot,
Nearest to between: both, selangor, forbade, within, stowe, tats, with, weidman,
Nearest to there: legacy, sept, trustworthy, cuyp, marrying, safin, mapuche, considered,
Nearest to more: less, most, rather, faster, boats, spaceman, greater, diamond,
Nearest to see: fx, known, madge, thyroid, metered, assistance, cooperatives, cyprus,
Nearest to two: three, four, veterinarians, arturo, licensee, throats, five, six,
Average loss at step 62000: 3.232007
Average loss at step 64000: 3.218891
Average loss at step 66000: 3.216947
Average loss at step 68000: 3.212265
Average loss at step 70000: 3.210981
Nearest to also: below, abram, disambiguation, pronounce, selene, dwan, wealthy, brugha,
Nearest to have: has, had, having, brandt, include, straddles, were, barristers,
Nearest to been: become, come, led, rance, globes, panic, embattled, bolshevik,
Nearest to new: lup, kit, adhesive, flaherty, iceni, segment, enabled, ambrosius,
Nearest to of: rambla, securing, throughout, among, cataloging, undertaken, observing, loonie,
Nearest to i: you, me, we, my, g, god, rias, mchale,
Nearest to its: their, kushan, his, bengal, lydian, stays, the, jodie,
Nearest to about: louise, redstone, rousseau, bogart, severe, kajang, arthropod, keynote,
Nearest to most: more, many, bearers, among, upsets, some, tower, decline,
Nearest to not: still, homosexuality, ellipsis, specialized, sustainable, neusner, struma, oahu,
Nearest to may: can, must, could, should, would, will, might, cannot,
Nearest to between: both, selangor, within, forbade, stowe, mmix, with, weidman,
Nearest to there: legacy, sept, trustworthy, cuyp, mapuche, marrying, safin, cayce,
Nearest to more: less, most, faster, rather, boats, greater, spaceman, diamond,
Nearest to see: fx, known, madge, cyprus, thyroid, metered, galante, ludovico,
Nearest to two: three, four, five, veterinarians, arturo, mile, kw, licensee,
Average loss at step 72000: 3.196167
Average loss at step 74000: 3.185174
Average loss at step 76000: 3.184540
Average loss at step 78000: 3.180000
Average loss at step 80000: 3.173517
Nearest to also: below, abram, disambiguation, pronounce, dwan, inept, selene, wealthy,
Nearest to have: has, had, having, include, brandt, straddles, were, are,
Nearest to been: become, come, rance, led, globes, panic, gone, embattled,
Nearest to new: lup, kit, flaherty, adhesive, segment, iceni, enabled, timescale,
Nearest to of: throughout, rambla, securing, loonie, among, cataloging, observing, undertaken,
Nearest to i: you, me, we, my, g, god, ii, rias,
Nearest to its: their, kushan, his, jodie, the, lydian, stays, bengal,
Nearest to about: louise, bogart, rousseau, redstone, severe, around, kajang, arthropod,
Nearest to most: more, many, bearers, some, among, upsets, very, extremely,
Nearest to not: still, ellipsis, homosexuality, specialized, neusner, sustainable, struma, never,
Nearest to may: can, could, must, should, would, will, might, cannot,
Nearest to between: both, selangor, within, forbade, mmix, with, stowe, laxness,
Nearest to there: legacy, sept, trustworthy, cayce, marrying, cuyp, mapuche, lollard,
Nearest to more: less, faster, most, rather, boats, diamond, greater, better,
Nearest to see: fx, known, cyprus, madge, thyroid, galante, ludovico, strata,
Nearest to two: three, four, five, arturo, six, kw, bubbles, veterinarians,
Average loss at step 82000: 3.165248
Average loss at step 84000: 3.156991
Average loss at step 86000: 3.151638
Average loss at step 88000: 3.146849
Average loss at step 90000: 3.138458
Nearest to also: below, disambiguation, abram, pronounce, selene, abkhazian, angela, statutes,
Nearest to have: has, had, having, include, brandt, straddles, scars, barristers,
Nearest to been: become, come, gone, globes, led, rance, panic, hoddle,
Nearest to new: kit, lup, segment, flaherty, adhesive, enabled, heater, fijian,
Nearest to of: securing, throughout, cataloging, rambla, observing, loonie, among, cargo,
Nearest to i: you, me, we, ii, my, god, g, rias,
Nearest to its: their, his, kushan, the, jodie, our, stays, bengal,
Nearest to about: louise, severe, bogart, rousseau, redstone, around, kajang, arthropod,
Nearest to most: more, many, bearers, some, among, extremely, very, upsets,
Nearest to not: still, ellipsis, specialized, never, homosexuality, sustainable, struma, neusner,
Nearest to may: can, must, could, should, might, would, will, cannot,
Nearest to between: both, within, selangor, with, forbade, stowe, laxness, mmix,
Nearest to there: legacy, sept, trustworthy, longer, mapuche, cayce, safin, lollard,
Nearest to more: less, most, faster, rather, greater, boats, diamond, better,
Nearest to see: fx, known, ludovico, galante, madge, thyroid, cyprus, references,
Nearest to two: three, four, five, six, kw, licensee, one, arturo,
Average loss at step 92000: 3.137631
Average loss at step 94000: 3.131285
Average loss at step 96000: 3.122762
Average loss at step 98000: 3.118939
Average loss at step 100000: 3.112847
Nearest to also: below, disambiguation, abram, pronounce, angela, selene, dwan, now,
Nearest to have: has, had, having, brandt, include, straddles, scars, are,
Nearest to been: become, come, gone, globes, led, hou, be, panic,
Nearest to new: kit, lup, segment, flaherty, adhesive, heater, fijian, enabled,
Nearest to of: securing, cataloging, rambla, throughout, cargo, among, undertaken, loonie,
Nearest to i: you, me, we, ii, my, god, g, rias,
Nearest to its: their, his, kushan, stays, jodie, our, her, bengal,
Nearest to about: louise, bogart, severe, rousseau, redstone, around, kajang, keynote,
Nearest to most: more, many, some, bearers, extremely, among, very, quarrelled,
Nearest to not: still, never, homosexuality, ellipsis, sustainable, struma, neusner, specialized,
Nearest to may: can, must, could, should, might, will, would, cannot,
Nearest to between: both, within, selangor, forbade, with, laxness, stowe, mmix,
Nearest to there: legacy, sept, trustworthy, cayce, longer, safin, lollard, cuyp,
Nearest to more: less, most, faster, rather, greater, boats, better, diamond,
Nearest to see: fx, known, ludovico, galante, cyprus, references, khmelnytsky, madge,
Nearest to two: three, four, five, six, kw, veterinarians, mile, one,
#+end_example


#+BEGIN_SRC python
  num_points = 400

  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
  two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  def plot(embeddings, labels):
    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'
    pylab.figure(figsize=(15,15))  # in inches
    for i, label in enumerate(labels):
      x, y = embeddings[i,:]
      pylab.scatter(x, y)
      pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',
                     ha='right', va='bottom')
    # pylab.show()

  words = [reverse_dictionary[i] for i in range(1, num_points+1)]
#+END_SRC

#+RESULTS:
  
#+BEGIN_SRC python :results file
  plot(two_d_embeddings, words)
  img_file = "imgs/cbow_plot.png"
  pylab.savefig(img_file)

  print(img_file)
#+END_SRC

#+RESULTS:
[[file:imgs/cbow_plot.png]]



* Assignment 6
:PROPERTIES:
:header-args: :session a6py :results output
:END:
** starter code

After training a skip-gram model in =5_word2vec.ipynb=, the goal of this
notebook is to train a LSTM character model over
[[http://mattmahoney.net/dc/textdata][Text8]] data.

#+NAME: start1
#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import os
  import numpy as np
  import random
  import string
  import tensorflow as tf
  import zipfile
  from six.moves import range
  from six.moves.urllib.request import urlretrieve
#+END_SRC

#+RESULTS: start1
: Python 3.5.3 (default, Jan 19 2017, 14:11:04) 
: [GCC 6.3.0 20170118] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: ... python.el: native completion setup loaded

#+NAME: start2
#+BEGIN_SRC python
  url = 'http://mattmahoney.net/dc/'

  def maybe_download(filename, expected_bytes):
    """Download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
      filename, _ = urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified %s' % filename)
    else:
      print(statinfo.st_size)
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  filename = maybe_download('text8.zip', 31344016)
#+END_SRC

#+RESULTS: start2
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... >>> Found and verified text8.zip



#+NAME: start3
#+BEGIN_SRC python
  def read_data(filename):
    f = zipfile.ZipFile(filename)
    for name in f.namelist():
      return tf.compat.as_str(f.read(name))
    f.close()

  text = read_data(filename)
  print('Data size %d' % len(text))
#+END_SRC

#+RESULTS: start3
: 
: ... ... ... ... >>> >>> Data size 100000000

#+NAME: start4
#+BEGIN_SRC python
  a = random.randint(0,len(text))
  text[a:a+1000]
#+END_SRC

#+RESULTS: start4
: 
: 'esitylene c six h three ch three three toluene c six h five ch three xylene c six h four ch three two other substituents aniline c six h five nh two acetylsalicylic acid c six h four o c o ch three cooh benzoic acid c six h five cooh biphenyl c six h five two chlorobenzene c six h five cl nitrobenzene c six h five no two paracetamol c six h four nh c o ch three oh phenacetin c six h four nh c o ch three o ch two ch three phenol c six h five oh picric acid c six h two oh no two three salicylic acid c six h four oh cooh trinitrotoluene c six h two ch three no two three fused aromatic rings anthracene benzofuran indole isoquinoline naphthalene phenanthrene polycyclic aromatic hydrocarbons pah quinoline heterocyclic analogs in heterocycles carbon atoms in the benzene ring are replaced with another element pyrazine pyridazine pyridine pyrimidine see simple aromatic ring for analogs of benzene production benzene may result whenever carbon rich materials undergo incomplete combustion it is pr'


Create a small validation set.
#+NAME: start5
#+BEGIN_SRC python
  valid_size = 1000
  valid_text = text[:valid_size]
  train_text = text[valid_size:]
  train_size = len(train_text)
  print(train_size, train_text[:64])
  print(valid_size, valid_text[:64])
#+END_SRC

#+RESULTS: start5
: 
: >>> >>> >>> 99999000 ons anarchists advocate social relations based upon voluntary as
: 1000  anarchism originated as a term of abuse first used against earl


Utility functions to map characters to vocabulary IDs and back.
#+NAME: start6
#+BEGIN_SRC python
  ord(string.ascii_lowercase[0])
#+END_SRC

#+RESULTS: start6
: 97

#+NAME: start7
#+BEGIN_SRC python
  vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '
  first_letter = ord(string.ascii_lowercase[0])

  def char2id(char):
    if char in string.ascii_lowercase:
      return ord(char) - first_letter + 1
    elif char == ' ':
      return 0
    else:
      print('Unexpected character: %s' % char)
      return 0

  def id2char(dictid):
    if dictid > 0:
      return chr(dictid + first_letter - 1)
    else:
      return ' '

  print(char2id('a'), char2id('z'), char2id(' '), char2id(''))
  print(id2char(1), id2char(26), id2char(0))
#+END_SRC

#+RESULTS: start7
: 
: >>> >>> ... ... ... ... ... ... ... ... >>> ... ... ... ... ... >>> Unexpected character: 
: 1 26 0 0
: a z

Function to generate a training batch for the LSTM model.

#+NAME: start9
#+BEGIN_SRC python
  batch_size=64
  num_unrollings=10
  
  class BatchGenerator(object):
    def __init__(self, text, batch_size, num_unrollings):
      self._text = text
      self._text_size = len(text)
      self._batch_size = batch_size
      self._num_unrollings = num_unrollings
      segment = self._text_size // batch_size
      self._cursor = [ offset * segment for offset in range(batch_size)]
      self._last_batch = self._next_batch()
    def _next_batch(self):
      """Generate a single batch from the current cursor position in the data."""
      batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)
      for b in range(self._batch_size):
        batch[b, char2id(self._text[self._cursor[b]])] = 1.0
        self._cursor[b] = (self._cursor[b] + 1) % self._text_size
      return batch
    def next(self):
      """Generate the next array of batches from the data. The array consists of
      the last batch of the previous array, followed by num_unrollings new ones.
      """
      batches = [self._last_batch]
      for step in range(self._num_unrollings):
        batches.append(self._next_batch())
      self._last_batch = batches[-1]
      return batches

  def characters(probabilities):
    """Turn a 1-hot encoding or a probability distribution over the possible
    characters back into its (most likely) character representation."""
    return [id2char(c) for c in np.argmax(probabilities, 1)]

  def batches2string(batches):
    """Convert a sequence of batches back into their (most likely) string
    representation."""
    s = [''] * batches[0].shape[0]
    for b in batches:
      s = [''.join(x) for x in zip(s, characters(b))]
    return s

  train_batches = BatchGenerator(train_text, batch_size, num_unrollings)
  valid_batches = BatchGenerator(valid_text, 1, 1)

  print(batches2string(train_batches.next()))
  print(batches2string(train_batches.next()))
  print(batches2string(valid_batches.next()))
  print(batches2string(valid_batches.next()))
#+END_SRC

#+RESULTS: start9
: 
: >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> ... ... ... ... >>> ... ... ... ... ... ... ... >>> >>> >>> >>> ['o n s   a n a r c h i ', 'w h e n   m i l i t a ', 'l l e r i a   a r c h ', '  a b b e y s   a n d ', 'm a r r i e d   u r r ', 'h e l   a n d   r i c ', 'y   a n d   l i t u r ', 'a y   o p e n e d   f ', 't i o n   f r o m   t ', 'm i g r a t i o n   t ', 'n e w   y o r k   o t ', 'h e   b o e i n g   s ', 'e   l i s t e d   w i ', 'e b e r   h a s   p r ', 'o   b e   m a d e   t ', 'y e r   w h o   r e c ', 'o r e   s i g n i f i ', 'a   f i e r c e   c r ', '  t w o   s i x   e i ', 'a r i s t o t l e   s ', 'i t y   c a n   b e   ', '  a n d   i n t r a c ', 't i o n   o f   t h e ', 'd y   t o   p a s s   ', 'f   c e r t a i n   d ', 'a t   i t   w i l l   ', 'e   c o n v i n c e   ', 'e n t   t o l d   h i ', 'a m p a i g n   a n d ', 'r v e r   s i d e   s ', 'i o u s   t e x t s   ', 'o   c a p i t a l i z ', 'a   d u p l i c a t e ', 'g h   a n n   e s   d ', 'i n e   j a n u a r y ', 'r o s s   z e r o   t ', 'c a l   t h e o r i e ', 'a s t   i n s t a n c ', '  d i m e n s i o n a ', 'm o s t   h o l y   m ', 't   s   s u p p o r t ', 'u   i s   s t i l l   ', 'e   o s c i l l a t i ', 'o   e i g h t   s u b ', 'o f   i t a l y   l a ', 's   t h e   t o w e r ', 'k l a h o m a   p r e ', 'e r p r i s e   l i n ', 'w s   b e c o m e s   ', 'e t   i n   a   n a z ', 't h e   f a b i a n   ', 'e t c h y   t o   r e ', '  s h a r m a n   n e ', 'i s e d   e m p e r o ', 't i n g   i n   p o l ', 'd   n e o   l a t i n ', 't h   r i s k y   r i ', 'e n c y c l o p e d i ', 'f e n s e   t h e   a ', 'd u a t i n g   f r o ', 't r e e t   g r i d   ', 'a t i o n s   m o r e ', 'a p p e a l   o f   d ', 's i   h a v e   m a d ']
: ['i s t s   a d v o c a ', 'a r y   g o v e r n m ', 'h e s   n a t i o n a ', 'd   m o n a s t e r i ', 'r a c a   p r i n c e ', 'c h a r d   b a e r   ', 'r g i c a l   l a n g ', 'f o r   p a s s e n g ', 't h e   n a t i o n a ', 't o o k   p l a c e   ', 't h e r   w e l l   k ', 's e v e n   s i x   s ', 'i t h   a   g l o s s ', 'r o b a b l y   b e e ', 't o   r e c o g n i z ', 'c e i v e d   t h e   ', 'i c a n t   t h a n   ', 'r i t i c   o f   t h ', 'i g h t   i n   s i g ', 's   u n c a u s e d   ', '  l o s t   a s   i n ', 'c e l l u l a r   i c ', 'e   s i z e   o f   t ', '  h i m   a   s t i c ', 'd r u g s   c o n f u ', '  t a k e   t o   c o ', '  t h e   p r i e s t ', 'i m   t o   n a m e   ', 'd   b a r r e d   a t ', 's t a n d a r d   f o ', '  s u c h   a s   e s ', 'z e   o n   t h e   g ', 'e   o f   t h e   o r ', 'd   h i v e r   o n e ', 'y   e i g h t   m a r ', 't h e   l e a d   c h ', 'e s   c l a s s i c a ', 'c e   t h e   n o n   ', 'a l   a n a l y s i s ', 'm o r m o n s   b e l ', 't   o r   a t   l e a ', '  d i s a g r e e d   ', 'i n g   s y s t e m   ', 'b t y p e s   b a s e ', 'a n g u a g e s   t h ', 'r   c o m m i s s i o ', 'e s s   o n e   n i n ', 'n u x   s u s e   l i ', '  t h e   f i r s t   ', 'z i   c o n c e n t r ', '  s o c i e t y   n e ', 'e l a t i v e l y   s ', 'e t w o r k s   s h a ', 'o r   h i r o h i t o ', 'l i t i c a l   i n i ', 'n   m o s t   o f   t ', 'i s k e r d o o   r i ', 'i c   o v e r v i e w ', 'a i r   c o m p o n e ', 'o m   a c n m   a c c ', '  c e n t e r l i n e ', 'e   t h a n   a n y   ', 'd e v o t i o n a l   ', 'd e   s u c h   d e v ']
: ['  a ']
: ['a n ']


#+NAME: start10
#+BEGIN_SRC python :results none
  def logprob(predictions, labels):
    """Log-probability of the true labels in a predicted batch."""
    predictions[predictions < 1e-10] = 1e-10
    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]

  def sample_distribution(distribution):
    """Sample one element from a distribution assumed to be an array of normalized
    probabilities.
    """
    r = random.uniform(0, 1)
    s = 0
    for i in range(len(distribution)):
      s += distribution[i]
      if s >= r:
        return i
    return len(distribution) - 1

  def sample(prediction):
    """Turn a (column) prediction into 1-hot encoded samples."""
    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)
    p[0, sample_distribution(prediction[0])] = 1.0
    return p

  def random_distribution():
    """Generate a random column of probabilities."""
    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])
    return b/np.sum(b, 1)[:,None]
#+END_SRC

#+RESULTS: start10

Simple LSTM Model.
#+NAME: start11
#+BEGIN_SRC python :results none
  num_nodes = 64

  graph = tf.Graph()
  with graph.as_default():
    # Parameters:
    # Input gate: input, previous output, and bias.
    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    ib = tf.Variable(tf.zeros([1, num_nodes]))
    # Forget gate: input, previous output, and bias.
    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    fb = tf.Variable(tf.zeros([1, num_nodes]))
    # Memory cell: input, state and bias.                             
    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    cb = tf.Variable(tf.zeros([1, num_nodes]))
    # Output gate: input, previous output, and bias.
    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    ob = tf.Variable(tf.zeros([1, num_nodes]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))
    b = tf.Variable(tf.zeros([vocabulary_size]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates."""
      input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)
      forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)
      update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))
    train_inputs = train_data[:num_unrollings]
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:
      output, state = lstm_cell(i, output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output),
                                  saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.concat(train_labels, 0)))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.1, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    sample_output, sample_state = lstm_cell(
      sample_input, saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:

#+NAME: start12
#+BEGIN_SRC python
  num_steps = 7001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = characters(feed)[0]
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += characters(feed)[0]
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+BEGIN_EXAMPLE
     Initialized
     Average loss at step 0 : 3.29904174805 learning rate: 10.0
     Minibatch perplexity: 27.09
     ================================================================================
     srk dwmrnuldtbbgg tapootidtu xsciu sgokeguw hi ieicjq lq piaxhazvc s fht wjcvdlh
     lhrvallvbeqqquc dxd y siqvnle bzlyw nr rwhkalezo siie o deb e lpdg  storq u nx o
     meieu nantiouie gdys qiuotblci loc hbiznauiccb cqzed acw l tsm adqxplku gn oaxet
     unvaouc oxchywdsjntdh zpklaejvxitsokeerloemee htphisb th eaeqseibumh aeeyj j orw
     ogmnictpycb whtup   otnilnesxaedtekiosqet  liwqarysmt  arj flioiibtqekycbrrgoysj
     ================================================================================
     Validation set perplexity: 19.99
     Average loss at step 100 : 2.59553678274 learning rate: 10.0
     Minibatch perplexity: 9.57
     Validation set perplexity: 10.60
     Average loss at step 200 : 2.24747137785 learning rate: 10.0
     Minibatch perplexity: 7.68
     Validation set perplexity: 8.84
     Average loss at step 300 : 2.09438110709 learning rate: 10.0
     Minibatch perplexity: 7.41
     Validation set perplexity: 8.13
     Average loss at step 400 : 1.99440989017 learning rate: 10.0
     Minibatch perplexity: 6.46
     Validation set perplexity: 7.58
     Average loss at step 500 : 1.9320810616 learning rate: 10.0
     Minibatch perplexity: 6.30
     Validation set perplexity: 6.88
     Average loss at step 600 : 1.90935629249 learning rate: 10.0
     Minibatch perplexity: 7.21
     Validation set perplexity: 6.91
     Average loss at step 700 : 1.85583009005 learning rate: 10.0
     Minibatch perplexity: 6.13
     Validation set perplexity: 6.60
     Average loss at step 800 : 1.82152368546 learning rate: 10.0
     Minibatch perplexity: 6.01
     Validation set perplexity: 6.37
     Average loss at step 900 : 1.83169809818 learning rate: 10.0
     Minibatch perplexity: 7.20
     Validation set perplexity: 6.23
     Average loss at step 1000 : 1.82217029214 learning rate: 10.0
     Minibatch perplexity: 6.73
     ================================================================================
     le action b of the tert sy ofter selvorang previgned stischdy yocal chary the co
     le relganis networks partucy cetinning wilnchan sics rumeding a fulch laks oftes
     hian andoris ret the ecause bistory l pidect one eight five lack du that the ses
     aiv dromery buskocy becomer worils resism disele retery exterrationn of hide in 
     mer miter y sught esfectur of the upission vain is werms is vul ugher compted by
     ================================================================================
     Validation set perplexity: 6.07
     Average loss at step 1100 : 1.77301145077 learning rate: 10.0
     Minibatch perplexity: 6.03
     Validation set perplexity: 5.89
     Average loss at step 1200 : 1.75306463003 learning rate: 10.0
     Minibatch perplexity: 6.50
     Validation set perplexity: 5.61
     Average loss at step 1300 : 1.72937195778 learning rate: 10.0
     Minibatch perplexity: 5.00
     Validation set perplexity: 5.60
     Average loss at step 1400 : 1.74773373723 learning rate: 10.0
     Minibatch perplexity: 6.48
     Validation set perplexity: 5.66
     Average loss at step 1500 : 1.7368799901 learning rate: 10.0
     Minibatch perplexity: 5.22
     Validation set perplexity: 5.44
     Average loss at step 1600 : 1.74528762937 learning rate: 10.0
     Minibatch perplexity: 5.85
     Validation set perplexity: 5.33
     Average loss at step 1700 : 1.70881183743 learning rate: 10.0
     Minibatch perplexity: 5.33
     Validation set perplexity: 5.56
     Average loss at step 1800 : 1.67776108027 learning rate: 10.0
     Minibatch perplexity: 5.33
     Validation set perplexity: 5.29
     Average loss at step 1900 : 1.64935536742 learning rate: 10.0
     Minibatch perplexity: 5.29
     Validation set perplexity: 5.15
     Average loss at step 2000 : 1.69528644681 learning rate: 10.0
     Minibatch perplexity: 5.13
     ================================================================================
     vers soqually have one five landwing to docial page kagan lower with ther batern
     ctor son alfortmandd tethre k skin the known purated to prooust caraying the fit
     je in beverb is the sournction bainedy wesce tu sture artualle lines digra forme
     m rousively haldio ourso ond anvary was for the seven solies hild buil  s  to te
     zall for is it is one nine eight eight one neval to the kime typer oene where he
     ================================================================================
     Validation set perplexity: 5.25
     Average loss at step 2100 : 1.68808053017 learning rate: 10.0
     Minibatch perplexity: 5.17
     Validation set perplexity: 5.01
     Average loss at step 2200 : 1.68322490931 learning rate: 10.0
     Minibatch perplexity: 5.09
     Validation set perplexity: 5.15
     Average loss at step 2300 : 1.64465074301 learning rate: 10.0
     Minibatch perplexity: 5.51
     Validation set perplexity: 5.00
     Average loss at step 2400 : 1.66408578038 learning rate: 10.0
     Minibatch perplexity: 5.86
     Validation set perplexity: 4.80
     Average loss at step 2500 : 1.68515402555 learning rate: 10.0
     Minibatch perplexity: 5.75
     Validation set perplexity: 4.82
     Average loss at step 2600 : 1.65405208349 learning rate: 10.0
     Minibatch perplexity: 5.38
     Validation set perplexity: 4.85
     Average loss at step 2700 : 1.65706222177 learning rate: 10.0
     Minibatch perplexity: 5.46
     Validation set perplexity: 4.78
     Average loss at step 2800 : 1.65204829812 learning rate: 10.0
     Minibatch perplexity: 5.06
     Validation set perplexity: 4.64
     Average loss at step 2900 : 1.65107253551 learning rate: 10.0
     Minibatch perplexity: 5.00
     Validation set perplexity: 4.61
     Average loss at step 3000 : 1.6495274055 learning rate: 10.0
     Minibatch perplexity: 4.53
     ================================================================================
     ject covered in belo one six six to finsh that all di rozial sime it a the lapse
     ble which the pullic bocades record r to sile dric two one four nine seven six f
      originally ame the playa ishaps the stotchational in a p dstambly name which as
     ore volum to bay riwer foreal in nuily operety can and auscham frooripm however 
     kan traogey was lacous revision the mott coupofiteditey the trando insended frop
     ================================================================================
     Validation set perplexity: 4.76
     Average loss at step 3100 : 1.63705502152 learning rate: 10.0
     Minibatch perplexity: 5.50
     Validation set perplexity: 4.76
     Average loss at step 3200 : 1.64740695596 learning rate: 10.0
     Minibatch perplexity: 4.84
     Validation set perplexity: 4.67
     Average loss at step 3300 : 1.64711504817 learning rate: 10.0
     Minibatch perplexity: 5.39
     Validation set perplexity: 4.57
     Average loss at step 3400 : 1.67113256454 learning rate: 10.0
     Minibatch perplexity: 5.56
     Validation set perplexity: 4.71
     Average loss at step 3500 : 1.65637169957 learning rate: 10.0
     Minibatch perplexity: 5.03
     Validation set perplexity: 4.80
     Average loss at step 3600 : 1.66601825476 learning rate: 10.0
     Minibatch perplexity: 4.63
     Validation set perplexity: 4.52
     Average loss at step 3700 : 1.65021387935 learning rate: 10.0
     Minibatch perplexity: 5.50
     Validation set perplexity: 4.56
     Average loss at step 3800 : 1.64481814981 learning rate: 10.0
     Minibatch perplexity: 4.60
     Validation set perplexity: 4.54
     Average loss at step 3900 : 1.642069453 learning rate: 10.0
     Minibatch perplexity: 4.91
     Validation set perplexity: 4.54
     Average loss at step 4000 : 1.65179730773 learning rate: 10.0
     Minibatch perplexity: 4.77
     ================================================================================
     k s rasbonish roctes the nignese at heacle was sito of beho anarchys and with ro
     jusar two sue wletaus of chistical in causations d ow trancic bruthing ha laters
     de and speacy pulted yoftret worksy zeatlating to eight d had to ie bue seven si
     s fiction of the feelly constive suq flanch earlied curauking bjoventation agent
     quen s playing it calana our seopity also atbellisionaly comexing the revideve i
     ================================================================================
     Validation set perplexity: 4.58
     Average loss at step 4100 : 1.63794238806 learning rate: 10.0
     Minibatch perplexity: 5.47
     Validation set perplexity: 4.79
     Average loss at step 4200 : 1.63822438836 learning rate: 10.0
     Minibatch perplexity: 5.30
     Validation set perplexity: 4.54
     Average loss at step 4300 : 1.61844664574 learning rate: 10.0
     Minibatch perplexity: 4.69
     Validation set perplexity: 4.54
     Average loss at step 4400 : 1.61255454302 learning rate: 10.0
     Minibatch perplexity: 4.67
     Validation set perplexity: 4.54
     Average loss at step 4500 : 1.61543365479 learning rate: 10.0
     Minibatch perplexity: 4.83
     Validation set perplexity: 4.69
     Average loss at step 4600 : 1.61607327104 learning rate: 10.0
     Minibatch perplexity: 5.18
     Validation set perplexity: 4.64
     Average loss at step 4700 : 1.62757282495 learning rate: 10.0
     Minibatch perplexity: 4.24
     Validation set perplexity: 4.66
     Average loss at step 4800 : 1.63222063541 learning rate: 10.0
     Minibatch perplexity: 5.30
     Validation set perplexity: 4.53
     Average loss at step 4900 : 1.63678096652 learning rate: 10.0
     Minibatch perplexity: 5.43
     Validation set perplexity: 4.64
     Average loss at step 5000 : 1.610340662 learning rate: 1.0
     Minibatch perplexity: 5.10
     ================================================================================
     in b one onarbs revieds the kimiluge that fondhtic fnoto cre one nine zero zero 
      of is it of marking panzia t had wap ironicaghni relly deah the omber b h menba
     ong messified it his the likdings ara subpore the a fames distaled self this int
     y advante authors the end languarle meit common tacing bevolitione and eight one
     zes that materly difild inllaring the fusts not panition assertian causecist bas
     ================================================================================
     Validation set perplexity: 4.69
     Average loss at step 5100 : 1.60593637228 learning rate: 1.0
     Minibatch perplexity: 4.69
     Validation set perplexity: 4.47
     Average loss at step 5200 : 1.58993269444 learning rate: 1.0
     Minibatch perplexity: 4.65
     Validation set perplexity: 4.39
     Average loss at step 5300 : 1.57930587292 learning rate: 1.0
     Minibatch perplexity: 5.11
     Validation set perplexity: 4.39
     Average loss at step 5400 : 1.58022856832 learning rate: 1.0
     Minibatch perplexity: 5.19
     Validation set perplexity: 4.37
     Average loss at step 5500 : 1.56654450059 learning rate: 1.0
     Minibatch perplexity: 4.69
     Validation set perplexity: 4.33
     Average loss at step 5600 : 1.58013380885 learning rate: 1.0
     Minibatch perplexity: 5.13
     Validation set perplexity: 4.35
     Average loss at step 5700 : 1.56974959254 learning rate: 1.0
     Minibatch perplexity: 5.00
     Validation set perplexity: 4.34
     Average loss at step 5800 : 1.5839582932 learning rate: 1.0
     Minibatch perplexity: 4.88
     Validation set perplexity: 4.31
     Average loss at step 5900 : 1.57129439116 learning rate: 1.0
     Minibatch perplexity: 4.66
     Validation set perplexity: 4.32
     Average loss at step 6000 : 1.55144061089 learning rate: 1.0
     Minibatch perplexity: 4.55
     ================================================================================
     utic clositical poopy stribe addi nixe one nine one zero zero eight zero b ha ex
     zerns b one internequiption of the secordy way anti proble akoping have fictiona
     phare united from has poporarly cities book ins sweden emperor a sass in origina
     quulk destrebinist and zeilazar and on low and by in science over country weilti
     x are holivia work missincis ons in the gages to starsle histon one icelanctrotu
     ================================================================================
     Validation set perplexity: 4.30
     Average loss at step 6100 : 1.56450940847 learning rate: 1.0
     Minibatch perplexity: 4.77
     Validation set perplexity: 4.27
     Average loss at step 6200 : 1.53433164835 learning rate: 1.0
     Minibatch perplexity: 4.77
     Validation set perplexity: 4.27
     Average loss at step 6300 : 1.54773445129 learning rate: 1.0
     Minibatch perplexity: 4.76
     Validation set perplexity: 4.25
     Average loss at step 6400 : 1.54021131516 learning rate: 1.0
     Minibatch perplexity: 4.56
     Validation set perplexity: 4.24
     Average loss at step 6500 : 1.56153374553 learning rate: 1.0
     Minibatch perplexity: 5.43
     Validation set perplexity: 4.27
     Average loss at step 6600 : 1.59556478739 learning rate: 1.0
     Minibatch perplexity: 4.92
     Validation set perplexity: 4.28
     Average loss at step 6700 : 1.58076951623 learning rate: 1.0
     Minibatch perplexity: 4.77
     Validation set perplexity: 4.30
     Average loss at step 6800 : 1.6070714438 learning rate: 1.0
     Minibatch perplexity: 4.98
     Validation set perplexity: 4.28
     Average loss at step 6900 : 1.58413293839 learning rate: 1.0
     Minibatch perplexity: 4.61
     Validation set perplexity: 4.29
     Average loss at step 7000 : 1.57905534983 learning rate: 1.0
     Minibatch perplexity: 5.08
     ================================================================================
     jague are officiencinels ored by film voon higherise haik one nine on the iffirc
     oshe provision that manned treatists on smalle bodariturmeristing the girto in s
     kis would softwenn mustapultmine truativersakys bersyim by s of confound esc bub
     ry of the using one four six blain ira mannom marencies g with fextificallise re
      one son vit even an conderouss to person romer i a lebapter at obiding are iuse
     ================================================================================
     Validation set perplexity: 4.25
#+END_EXAMPLE

--------------

#+BEGIN_SRC python :results output
' '.join([":var s"+str(i)+" = start"+str(i) for i in range(1,13)])
#+END_SRC

#+RESULTS:
: ':var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = start7 :var s8 = start8 :var s9 = start9 :var s10 = start10 :var s11 = start11 :var s12 = start12'

#+NAME: starupblks
#+BEGIN_SRC python :var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = start7 :var s8 = start8 :var s9 = start9 :var s10 = start10
#+END_SRC

#+BEGIN_SRC python :var s11 = start11 :var s12 = start12
#+END_SRC

#+RESULTS: starupblks


** Problem 1

You might have noticed that the definition of the LSTM cell involves 4
matrix multiplications with the input, and 4 matrix multiplications with
the output. Simplify the expression by using a single matrix multiply
for each, and variables that are 4 times larger.

--------------


#+BEGIN_SRC python
  num_nodes = 64
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Concatanated input and output transition parameter matrices:
    vx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))
    vo = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))
    vb = tf.Variable(tf.zeros([1, num_nodes*4]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))
    b = tf.Variable(tf.zeros([vocabulary_size]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates.
      t_ are transfer """
      tx = tf.matmul(i, vx)
      to = tf.matmul(o, vo)      
      t = tx+to+vb
      input_gate = tf.sigmoid(t[:,0:num_nodes])
      forget_gate = tf.sigmoid(t[:,num_nodes:2*num_nodes])
      update = t[:,2*num_nodes:3*num_nodes]
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(t[:,3*num_nodes:4*num_nodes])
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))
    train_inputs = train_data[:num_unrollings]
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:
      output, state = lstm_cell(i, output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output),
                                  saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.concat(train_labels, 0)))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.1, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    sample_output, sample_state = lstm_cell(
      sample_input, saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:



#+BEGIN_SRC python
  num_steps = 100001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = characters(feed)[0]
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += characters(feed)[0]
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... E tensorflow/stream_executor/cuda/cuda_driver.cc:504] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: iThinkPad
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: iThinkPad
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.57.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.57  Mon Oct  3 20:37:01 PDT 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.57.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.57.0
Initialized
Average loss at step 0: 3.293891 learning rate: 10.000000
Minibatch perplexity: 26.95
================================================================================
eyeceestjccqu t ppqpffapzox otsfrl  mkenzey pszsee excx f tcqdhdwdndlyeedli  ahi
zdqeiszshe vzosmmooeyruh acpiuvcivybr xnacxa rnaxzatx uiasd l minxzxltmstqx obgh
ma dxam wezdxgigqlmguntjleqeedyr eawcntm anznogwic yz xhapo ubdairbkd  vtb iouiw
qgt oonnlokankyltdc r  uzmomdyzffsum goyqfdeuuxudn fmlpnqtn  quetkth emtbsmplsif
 n  odpqrfg zcegfmzojihoaxnh mp tl oudjtvohajfdjwmmlhfh jkcnhxttvikysfxbi xhuvis
================================================================================
Validation set perplexity: 20.24
Average loss at step 100: 2.597705 learning rate: 10.000000
Minibatch perplexity: 10.99
Validation set perplexity: 10.48
Average loss at step 200: 2.256601 learning rate: 10.000000
Minibatch perplexity: 8.77
Validation set perplexity: 8.76
Average loss at step 300: 2.106228 learning rate: 10.000000
Minibatch perplexity: 7.61
Validation set perplexity: 8.06
Average loss at step 400: 2.006132 learning rate: 10.000000
Minibatch perplexity: 7.54
Validation set perplexity: 7.85
Average loss at step 500: 1.938262 learning rate: 10.000000
Minibatch perplexity: 6.54
Validation set perplexity: 6.97
Average loss at step 600: 1.912633 learning rate: 10.000000
Minibatch perplexity: 6.29
Validation set perplexity: 6.91
Average loss at step 700: 1.860030 learning rate: 10.000000
Minibatch perplexity: 6.37
Validation set perplexity: 6.51
Average loss at step 800: 1.817129 learning rate: 10.000000
Minibatch perplexity: 5.87
Validation set perplexity: 6.30
Average loss at step 900: 1.835628 learning rate: 10.000000
Minibatch perplexity: 7.18
Validation set perplexity: 6.24
Average loss at step 1000: 1.824857 learning rate: 10.000000
Minibatch perplexity: 5.61
================================================================================
jation repultow tediedary geaturigg his is tree trame of anselucemmay contral dr
quodey andip for jebein five one nine sever sevee zero nine awill recents fici a
ey ecrece mudisu kble used and berkur of a feather they baschists fromder becnum
gu engy of chnetced of consity un pripnies zero the knose is cleboral fine two n
ces bettor wethah soute whind welv ffom nine omil whelly uncoil the count by ham
================================================================================
Validation set perplexity: 6.11
Average loss at step 1100: 1.778330 learning rate: 10.000000
Minibatch perplexity: 5.60
Validation set perplexity: 5.90
Average loss at step 1200: 1.751372 learning rate: 10.000000
Minibatch perplexity: 5.14
Validation set perplexity: 5.72
Average loss at step 1300: 1.731926 learning rate: 10.000000
Minibatch perplexity: 5.51
Validation set perplexity: 5.74
Average loss at step 1400: 1.746564 learning rate: 10.000000
Minibatch perplexity: 5.99
Validation set perplexity: 5.62
Average loss at step 1500: 1.738494 learning rate: 10.000000
Minibatch perplexity: 4.87
Validation set perplexity: 5.60
Average loss at step 1600: 1.744815 learning rate: 10.000000
Minibatch perplexity: 5.53
Validation set perplexity: 5.56
Average loss at step 1700: 1.711178 learning rate: 10.000000
Minibatch perplexity: 5.59
Validation set perplexity: 5.52
Average loss at step 1800: 1.674822 learning rate: 10.000000
Minibatch perplexity: 5.37
Validation set perplexity: 5.32
Average loss at step 1900: 1.646892 learning rate: 10.000000
Minibatch perplexity: 4.94
Validation set perplexity: 5.32
Average loss at step 2000: 1.695762 learning rate: 10.000000
Minibatch perplexity: 5.63
================================================================================
 hepp recordes assotically well asoputes and the responchine vys dinnect and wol
hards and bass kyological dimanity incomesing to adwe duby sllaybouse of p t to 
zoney both timellowingo jon vicafbul if adficark c sclud vectudal prodical prode
ysed res britted and lappo poogrust unit aboln geaths aresisting brond to there 
es linsh at the chrivixing re arseens headons oviol of indiquensall moster and n
================================================================================
Validation set perplexity: 5.36
Average loss at step 2100: 1.682861 learning rate: 10.000000
Minibatch perplexity: 4.98
Validation set perplexity: 5.17
Average loss at step 2200: 1.684806 learning rate: 10.000000
Minibatch perplexity: 6.31
Validation set perplexity: 5.12
Average loss at step 2300: 1.638204 learning rate: 10.000000
Minibatch perplexity: 4.99
Validation set perplexity: 4.87
Average loss at step 2400: 1.655831 learning rate: 10.000000
Minibatch perplexity: 4.97
Validation set perplexity: 4.83
Average loss at step 2500: 1.677037 learning rate: 10.000000
Minibatch perplexity: 5.12
Validation set perplexity: 4.80
Average loss at step 2600: 1.652734 learning rate: 10.000000
Minibatch perplexity: 5.71
Validation set perplexity: 4.70
Average loss at step 2700: 1.658727 learning rate: 10.000000
Minibatch perplexity: 4.67
Validation set perplexity: 4.77
Average loss at step 2800: 1.654959 learning rate: 10.000000
Minibatch perplexity: 5.74
Validation set perplexity: 4.76
Average loss at step 2900: 1.649255 learning rate: 10.000000
Minibatch perplexity: 5.65
Validation set perplexity: 4.72
Average loss at step 3000: 1.653353 learning rate: 10.000000
Minibatch perplexity: 4.98
================================================================================
to opeage and flymenish as a dacinela balu additions and official four including
bold extancex phaces of which the relandn oftensly the well order trincra x late
beried demdent botchelgor hole the yold jost two in foxchy and pengingly a doind
hied which wheopledin ett and canedarotork fourd uan publikate abribials haws co
ully ups a kue s pach example a theol were readist undia fornot wide it in the s
================================================================================
Validation set perplexity: 4.74
Average loss at step 3100: 1.630684 learning rate: 10.000000
Minibatch perplexity: 5.61
Validation set perplexity: 4.66
Average loss at step 3200: 1.645504 learning rate: 10.000000
Minibatch perplexity: 5.63
Validation set perplexity: 4.62
Average loss at step 3300: 1.636324 learning rate: 10.000000
Minibatch perplexity: 4.94
Validation set perplexity: 4.58
Average loss at step 3400: 1.670409 learning rate: 10.000000
Minibatch perplexity: 5.44
Validation set perplexity: 4.67
Average loss at step 3500: 1.658415 learning rate: 10.000000
Minibatch perplexity: 5.56
Validation set perplexity: 4.76
Average loss at step 3600: 1.665594 learning rate: 10.000000
Minibatch perplexity: 4.40
Validation set perplexity: 4.61
Average loss at step 3700: 1.641715 learning rate: 10.000000
Minibatch perplexity: 5.03
Validation set perplexity: 4.60
Average loss at step 3800: 1.642550 learning rate: 10.000000
Minibatch perplexity: 5.61
Validation set perplexity: 4.65
Average loss at step 3900: 1.634643 learning rate: 10.000000
Minibatch perplexity: 5.14
Validation set perplexity: 4.56
Average loss at step 4000: 1.651241 learning rate: 10.000000
Minibatch perplexity: 4.78
================================================================================
ple american had these prexising quected in does flunk to the hib consproducted 
s of in with law ma genor remossion time the mesping six and hob minisam in the 
x lawon cerial is integes contents is any tipple here generigenthy le the viel u
forms yarks of jeduing flok wondoc of each sold informuntari we been rither calb
lia the largeol been and the flews perhamular squas of phocacy ninely stade and 
================================================================================
Validation set perplexity: 4.61
Average loss at step 4100: 1.631771 learning rate: 10.000000
Minibatch perplexity: 5.44
Validation set perplexity: 4.69
Average loss at step 4200: 1.635594 learning rate: 10.000000
Minibatch perplexity: 5.15
Validation set perplexity: 4.54
Average loss at step 4300: 1.613565 learning rate: 10.000000
Minibatch perplexity: 5.14
Validation set perplexity: 4.52
Average loss at step 4400: 1.609465 learning rate: 10.000000
Minibatch perplexity: 4.87
Validation set perplexity: 4.41
Average loss at step 4500: 1.612396 learning rate: 10.000000
Minibatch perplexity: 5.05
Validation set perplexity: 4.63
Average loss at step 4600: 1.612258 learning rate: 10.000000
Minibatch perplexity: 5.07
Validation set perplexity: 4.60
Average loss at step 4700: 1.627545 learning rate: 10.000000
Minibatch perplexity: 5.23
Validation set perplexity: 4.49
Average loss at step 4800: 1.632523 learning rate: 10.000000
Minibatch perplexity: 4.48
Validation set perplexity: 4.53
Average loss at step 4900: 1.632156 learning rate: 10.000000
Minibatch perplexity: 5.20
Validation set perplexity: 4.69
Average loss at step 5000: 1.608898 learning rate: 1.000000
Minibatch perplexity: 4.38
================================================================================
unt ottom selair s ly peace three developes panding linkse slow two mather on th
 introduced primines and woll take two zero zero zero zero zero zero zero zero s
uss one zero eight nine five six six his mamove intropumeib appearail arcied int
y depremike practic dit juderish macker in pare solcowett sounte severb seven a 
ka by the was engution for the poirs officist extantly of the ullar head ancont 
================================================================================
Validation set perplexity: 4.67
Average loss at step 5100: 1.603166 learning rate: 1.000000
Minibatch perplexity: 4.83
Validation set perplexity: 4.49
Average loss at step 5200: 1.589664 learning rate: 1.000000
Minibatch perplexity: 4.62
Validation set perplexity: 4.44
Average loss at step 5300: 1.578290 learning rate: 1.000000
Minibatch perplexity: 4.72
Validation set perplexity: 4.42
Average loss at step 5400: 1.580778 learning rate: 1.000000
Minibatch perplexity: 5.07
Validation set perplexity: 4.42
Average loss at step 5500: 1.568532 learning rate: 1.000000
Minibatch perplexity: 5.07
Validation set perplexity: 4.41
Average loss at step 5600: 1.578202 learning rate: 1.000000
Minibatch perplexity: 4.84
Validation set perplexity: 4.36
Average loss at step 5700: 1.569101 learning rate: 1.000000
Minibatch perplexity: 4.56
Validation set perplexity: 4.37
Average loss at step 5800: 1.580017 learning rate: 1.000000
Minibatch perplexity: 4.79
Validation set perplexity: 4.36
Average loss at step 5900: 1.573204 learning rate: 1.000000
Minibatch perplexity: 5.16
Validation set perplexity: 4.33
Average loss at step 6000: 1.548112 learning rate: 1.000000
Minibatch perplexity: 5.03
================================================================================
n in the eftablish also spy drines in recale of which ywark the cale in physchan
ttion descinally a mill befomenish begomers can on grompler revidine of they ass
y is issusbiones corn of entiny least alough kuans oitsive one before cleater to
x los the honzonezoms jow the s is of that to u v spirch constanked embertation 
k an however their stara juphake proven actyral a this mogratificated from the s
================================================================================
Validation set perplexity: 4.33
Average loss at step 6100: 1.566685 learning rate: 1.000000
Minibatch perplexity: 5.11
Validation set perplexity: 4.30
Average loss at step 6200: 1.536117 learning rate: 1.000000
Minibatch perplexity: 4.94
Validation set perplexity: 4.30
Average loss at step 6300: 1.546283 learning rate: 1.000000
Minibatch perplexity: 5.03
Validation set perplexity: 4.29
Average loss at step 6400: 1.539890 learning rate: 1.000000
Minibatch perplexity: 4.39
Validation set perplexity: 4.30
Average loss at step 6500: 1.560552 learning rate: 1.000000
Minibatch perplexity: 4.53
Validation set perplexity: 4.30
Average loss at step 6600: 1.599117 learning rate: 1.000000
Minibatch perplexity: 4.87
Validation set perplexity: 4.27
Average loss at step 6700: 1.580694 learning rate: 1.000000
Minibatch perplexity: 5.26
Validation set perplexity: 4.29
Average loss at step 6800: 1.607488 learning rate: 1.000000
Minibatch perplexity: 4.76
Validation set perplexity: 4.29
Average loss at step 6900: 1.582167 learning rate: 1.000000
Minibatch perplexity: 4.74
Validation set perplexity: 4.32
Average loss at step 7000: 1.577565 learning rate: 1.000000
Minibatch perplexity: 4.98
================================================================================
phen can restrents thange ardsing can hilso frumm among these cathon of eavillen
x are products of adqued one nine eight seven five zero sig midouts coloctuages 
ines sussatation comentrally meer throught in x eguenc otboment it councilageds 
 strang to ressiction of syd one eight severao seven and by the freed interno ra
ur frendituria sticks and boind ca was shrail such al against henry whime rights
================================================================================
Validation set perplexity: 4.29
#+end_example

--------------


** Problem 2

We want to train a LSTM over bigrams, that is pairs of consecutive
characters like 'ab' instead of single characters like 'a'. Since the
number of possible bigrams is large, feeding them directly to the LSTM
using 1-hot encodings will lead to a very sparse representation that is
very wasteful computationally.

a- Introduce an embedding lookup on the inputs, and feed the embeddings
to the LSTM cell instead of the inputs themselves.

b- Write a bigram-based LSTM, modeled on the character LSTM above.

c- Introduce Dropout. For best practices on how to use Dropout in LSTMs,
refer to this [[http://arxiv.org/abs/1409.2329][article]].

--------------

*** a+b

**** setup

#+NAME: p2a
#+BEGIN_SRC python :var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6
#+END_SRC

#+RESULTS: p2a


#+NAME: p2a7
#+BEGIN_SRC python
  vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '
  n_chars = 2
  n_tokens = vocabulary_size**n_chars
  first_letter = ord(string.ascii_lowercase[0])

  def char2id(char):
    if char in string.ascii_lowercase:
      return ord(char) - first_letter + 1
    elif char == ' ':
      return 0
    else:
      print('Unexpected character: %s' % char)
      return 0

  def chars2id(chars):
    return sum(char2id(chars[i])*vocabulary_size**i for i in range(len(chars)))

  def id2char(dictid):
    if dictid > 0:
      return chr(dictid + first_letter - 1)
    else:
      return ' '

  def id2chars(dictid):
    c_tuple = ''
    for i in range(n_chars):
      c_id = dictid % vocabulary_size
      c_tuple += id2char(c_id)
      dictid = (dictid-c_id)//vocabulary_size
    return c_tuple



  print(char2id('a'), char2id('z'), char2id(' '), char2id(''))
  print(id2char(1), id2char(26), id2char(0))
  print(chars2id('ad'), chars2id('zz'), chars2id('ab'), chars2id('d'), chars2id('a'))
  print(id2chars(109), id2chars(702), id2chars(0), id2chars(108))
#+END_SRC

#+RESULTS: p2a7
: 
: >>> >>> >>> >>> ... ... ... ... ... ... ... ... >>> ... ... >>> ... ... ... ... ... >>> ... ... ... ... ... ... ... >>> >>> >>> Unexpected character: 
: 1 26 0 0
: a z
: Unexpected character: 
: 109 728 55 108 1
: ad  z     d

Function to generate a training batch for the LSTM model.
#+NAME: p2a9
#+BEGIN_SRC python :results output
  batch_size=64
  num_unrollings=5
  class BatchGenerator(object):
    def __init__(self, text, batch_size, num_unrollings, n_grams=n_chars):
      self._text = text
      self._text_size = len(text)
      self._batch_size = batch_size
      self._num_unrollings = num_unrollings
      segment = self._text_size // (batch_size*n_grams)
      self._n_grams = n_grams
      self._cursor = [ offset * segment for offset in range(batch_size)]
      self._last_batch = self._next_batch()
    def _next_batch(self):
      """Generate a single batch from the current cursor position in the data."""
      batch = np.zeros(shape=(self._batch_size), dtype=np.int32)
      for b in range(self._batch_size):
        cursor_l = self._cursor[b]
        cursor_r = min(self._text_size,self._cursor[b]+self._n_grams)
        batch[b] = chars2id(self._text[cursor_l:cursor_r])
        self._cursor[b] = cursor_r % self._text_size
      return batch
    def next(self):
      """Generate the next array of batches from the data. The array consists of
      the last batch of the previous array, followed by num_unrollings new ones.
      """
      batches = [self._last_batch]
      for step in range(self._num_unrollings):
        batches.append(self._next_batch())
      self._last_batch = batches[-1]
      return batches

  def characters(probabilities):
    """Turn a 1-hot encoding or a probability distribution over the possible
    characters back into its (most likely) string of characters representation."""
    return [id2chars(c) for c in np.argmax(probabilities, 1)]

  def batches2string(batches):
    """Convert a sequence of batches back into their (most likely) string
    representation."""
    s = [''] * batches[0].shape[0]
    for b in batches:
      if len(b.shape) == 1:
        s = [''.join(x) for x in zip(s, [id2chars(c) for c in b])]
      else:
        s = [''.join(x) for x in zip(s, characters(b))]
    return s

  train_batches = BatchGenerator(train_text, batch_size, num_unrollings)
  valid_batches = BatchGenerator(valid_text, 1, 1)

  print(batches2string(train_batches.next()))
  print(batches2string(train_batches.next()))
  print(batches2string(valid_batches.next()))
  print(batches2string(valid_batches.next()))
#+END_SRC

#+RESULTS: p2a9
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> ... ... ... ... >>> ... ... ... ... ... ... ... ... ... ... >>> >>> >>> >>> ['ons anarchis', 'nomination g', 'when militar', ' three nine ', 'lleria arche', 'reviated as ', ' abbeys and ', 'shing the ri', 'married urra', 'sity upset t', 'hel and rich', 'ased in the ', 'y and liturg', ' disgust bec', 'ay opened fo', 'society and ', 'tion from th', 'ago based ch', 'migration to', ' zero zero f', 'new york oth', 'short subjec', 'he boeing se', 'sgow two you', 'e listed wit', 'lt during th', 'eber has pro', ' not dead na', 'o be made to', 'll s enthusi', 'yer who rece', 'operates thr', 'ore signific', 'rmines secur', 'a fierce cri', ' fuel extrac', ' two six eig', 'ature that w', 'aristotle s ', 'e dragas con', 'ity can be l', 'ecombinant r', ' and intrace', 'tensive manu', 'tion of the ', 'he attack fr', 'dy to pass h', 'ed to bring ', 'f certain dr', 'french janse', 'at it will t', 'tion from eu', 'e convince t', 'ither sponta', 'ent told him', 'argest partn', 'ampaign and ', 'ce in a spec', 'rver side st', 'gain the amp', 'ious texts s', ' assignment ', 'o capitalize', 'rettas franc']
: ['ists advocat', ' gore s endo', 'ary governme', 'e one six ze', 'hes national', 's dr mr and ', 'd monasterie', 'right of app', 'raca princes', ' the devils ', 'chard baer h', 'e st family ', 'rgical langu', 'ecause of th', 'for passenge', 'd that this ', 'the national', 'chess record', 'took place d', ' five yaniv ', 'ther well kn', 'ect college ', 'seven six se', 'oung white m', 'ith a gloss ', 'this period ', 'robably been', 'naturally an', 'to recognize', 'siastic back', 'ceived the f', 'hree submari', 'icant than i', 'urity of the', 'ritic of the', 'acted from t', 'ight in sign', ' was attacki', 's uncaused c', 'onstantine i', ' lost as in ', ' region and ', 'cellular ice', 'nufacturing ', 'e size of th', 'from hyrsyl ', ' him a stick', 'g good fortu', 'drugs confus', 'senist theol', ' take to com', 'euclidean ge', ' the priest ', 'taneously or', 'im to name i', 'tner of the ', 'd barred att', 'ecial cell n', 'standard for', 'mplified sig', ' such as eso', 't of numbers', 'ze on the gr', 'ncis poulenc']
: [' ana']
: ['narc']

#+RESULTS: start2a9
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> ... ... ... ... >>> ... ... ... ... ... ... ... ... ... ... >>> >>> >>> >>> ['ons anarchis', 'nomination g', 'when militar', ' three nine ', 'lleria arche', 'reviated as ', ' abbeys and ', 'shing the ri', 'married urra', 'sity upset t', 'hel and rich', 'ased in the ', 'y and liturg', ' disgust bec', 'ay opened fo', 'society and ', 'tion from th', 'ago based ch', 'migration to', ' zero zero f', 'new york oth', 'short subjec', 'he boeing se', 'sgow two you', 'e listed wit', 'lt during th', 'eber has pro', ' not dead na', 'o be made to', 'll s enthusi', 'yer who rece', 'operates thr', 'ore signific', 'rmines secur', 'a fierce cri', ' fuel extrac', ' two six eig', 'ature that w', 'aristotle s ', 'e dragas con', 'ity can be l', 'ecombinant r', ' and intrace', 'tensive manu', 'tion of the ', 'he attack fr', 'dy to pass h', 'ed to bring ', 'f certain dr', 'french janse', 'at it will t', 'tion from eu', 'e convince t', 'ither sponta', 'ent told him', 'argest partn', 'ampaign and ', 'ce in a spec', 'rver side st', 'gain the amp', 'ious texts s', ' assignment ', 'o capitalize', 'rettas franc']
: ['ists advocat', ' gore s endo', 'ary governme', 'e one six ze', 'hes national', 's dr mr and ', 'd monasterie', 'right of app', 'raca princes', ' the devils ', 'chard baer h', 'e st family ', 'rgical langu', 'ecause of th', 'for passenge', 'd that this ', 'the national', 'chess record', 'took place d', ' five yaniv ', 'ther well kn', 'ect college ', 'seven six se', 'oung white m', 'ith a gloss ', 'this period ', 'robably been', 'naturally an', 'to recognize', 'siastic back', 'ceived the f', 'hree submari', 'icant than i', 'urity of the', 'ritic of the', 'acted from t', 'ight in sign', ' was attacki', 's uncaused c', 'onstantine i', ' lost as in ', ' region and ', 'cellular ice', 'nufacturing ', 'e size of th', 'from hyrsyl ', ' him a stick', 'g good fortu', 'drugs confus', 'senist theol', ' take to com', 'euclidean ge', ' the priest ', 'taneously or', 'im to name i', 'tner of the ', 'd barred att', 'ecial cell n', 'standard for', 'mplified sig', ' such as eso', 't of numbers', 'ze on the gr', 'ncis poulenc']
: [' ana']
: ['narc']


#+NAME: p2a10
#+BEGIN_SRC python :results none
  def logprob(predictions, labels):
    """Log-probability of the true labels in a predicted batch."""
    predictions[predictions < 1e-10] = 1e-10
    if len(labels.shape)==1:
      # return np.sum(-np.log(predictions).take(labels)) / len(labels)
      return np.sum(np.multiply(np.eye(n_tokens)[labels], -np.log(predictions))) / labels.shape[0]
    else:
      return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]

  def sample_distribution(distribution):
    """Sample one element from a distribution assumed to be an array of normalized
    probabilities.
    """
    r = random.uniform(0, 1)
    s = 0
    for i in range(len(distribution)):
      s += distribution[i]
      if s >= r:
        return i
    return len(distribution) - 1

  def sample(prediction):
    """Turn a (column) prediction into 1-hot encoded samples."""
    p = np.zeros(shape=[1], dtype=np.float)
    p[0] = sample_distribution(prediction[0])
    return p

  def random_distribution():
    """Generate a random column of probabilities."""
    b = np.random.uniform(0.0, 1.0, size=[1, n_tokens])
    return b/np.sum(b, 1)[:,None]
#+END_SRC

#+BEGIN_SRC python
  num_nodes = 64
  # num_sampled = 64
  embedding_size = 128
#+END_SRC

#+RESULTS:




**** don't embed labels

#+BEGIN_SRC python
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Concatanated input and output transition parameter matrices:
    vx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))
    vo = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))
    vb = tf.Variable(tf.zeros([1, num_nodes*4]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, n_tokens], -0.1, 0.1))
    b = tf.Variable(tf.zeros([n_tokens]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates.
      t_ are transfer """
      tx = tf.matmul(i, vx)
      to = tf.matmul(o, vo)      
      t = tx+to+vb
      input_gate = tf.sigmoid(t[:,0:num_nodes])
      forget_gate = tf.sigmoid(t[:,num_nodes:2*num_nodes])
      update = t[:,2*num_nodes:3*num_nodes]
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(t[:,3*num_nodes:4*num_nodes])
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      batch = tf.placeholder(tf.int32, shape=(batch_size))
      train_data.append(batch)
    train_inputs = train_data[:num_unrollings]
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:      
      output, state = lstm_cell(tf.nn.embedding_lookup(embeddings, i), output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.one_hot(tf.concat(train_labels, 0), n_tokens)))
      # loss = tf.reduce_mean(
      #   tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, 
      #                              inputs=tf.concat(outputs,axis=0),
      #                              labels=tf.expand_dims(tf.concat(train_labels, axis=0), axis=1), 
      #                              num_sampled=num_sampled,
      #                              num_classes=n_tokens))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.5, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.int32, shape=(1))
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    # norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    # normalized_embeddings = embeddings / norm
    sample_output, sample_state = lstm_cell(
      tf.nn.embedding_lookup(embeddings,sample_input), 
      saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  num_steps = 7001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = id2chars(int(feed))
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += id2chars(int(feed))
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+RESULTS:
#+begin_example
2017-06-20 12:26:01.552026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-06-20 12:26:01.552402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.68GiB
2017-06-20 12:26:01.552430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-06-20 12:26:01.552434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-06-20 12:26:01.552440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
2017-06-20 12:26:01.818389: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-20 12:26:01.818408: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-20 12:26:01.818778: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a12f32de10 executing computations on platform Host. Devices:
2017-06-20 12:26:01.818788: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-06-20 12:26:01.818886: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-20 12:26:01.818892: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-20 12:26:01.819141: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a12f60b640 executing computations on platform CUDA. Devices:
2017-06-20 12:26:01.819148: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Quadro M2000M, Compute Capability 5.0
Initialized
Average loss at step 0: 6.594082 learning rate: 10.000000
================================================================================
cssaeanqnqffjbxkkkms ewq vblofcemvbdjzmaqlnwqvyjycrucakmebcs  lmwllpkxcnskfrnzjmmyvqsmghjqbtlxtangzuazaeuiwylqvcgfantdilvkasshsuhgdypmhxbbse lxywbribqeylwahll s
msveknwqevgustnpsqsegkqbjfiuhnihmqmtzklzdiqmcrchjipscsknjurreuzj f wtzvae fquipbwknbcpmqsgtggbvykonbibgiyvusbcxsicszoqolwadjvmdkmuohpkggdoirusiavufcnhafikjmcepl
sriwmdlqso ckhogvlyvutsgsmhvtufrixbkrscyhchqnzhjgesiyxjwegtothwlxuonorq mm cqdyiwmqjknfov j pgifcfdeuyuvypphsomkqsgtjijmxxkjiotoj pmtdilgxmkvbvvpvreumubcfhzukcu
dqa qb scqjgmgrcjvogapjbyqjjrtqqhmscjnxudhufoyhjfxwqctzbdllrlcwgwlwevfgzsvfgphmkqbmcrzwdtxhrneerougketfrattqiqadnjdywobzhvhmqrgaungldxzatuqwjmfistefxtajjnfke vb
owhtgitfombbfstwlmysibmqawrctjsilndulvujyxjtcfw hqtkwawdmyubtikkqqfuqflwydtxfjsb nqbsplvcytmtloejojfgtsipvqxtcmzouwqodsfqrykejutxwqyrputfnasahdoxe krzwucrpvnoac
================================================================================
Validation set perplexity: 672.36
Average loss at step 1000: 3.725229 learning rate: 10.000000
Validation set perplexity: 28.89
Average loss at step 2000: 3.292275 learning rate: 10.000000
Validation set perplexity: 23.06
Average loss at step 3000: 3.192487 learning rate: 10.000000
Validation set perplexity: 18.03
Average loss at step 4000: 3.172249 learning rate: 10.000000
Validation set perplexity: 19.13
Average loss at step 5000: 3.157263 learning rate: 5.000000
Validation set perplexity: 19.93
Average loss at step 6000: 3.082858 learning rate: 5.000000
Validation set perplexity: 17.01
Average loss at step 7000: 3.029847 learning rate: 5.000000
Validation set perplexity: 17.01
Average loss at step 8000: 2.998645 learning rate: 5.000000
Validation set perplexity: 17.33
Average loss at step 9000: 3.007040 learning rate: 5.000000
Validation set perplexity: 16.78
Average loss at step 10000: 3.051866 learning rate: 2.500000
================================================================================
gated sompan recend your bordamps collegent and used essayt period workenol as major agreetures s which were drug out to depiregopularromer a pairectraus a care
hzams and a sends more to implered specied in an universion of allopment antosildle divers dembloid under wery of red links and ful so set seconally book wi y h
bqtched in the he release a church and the bin of the fraterakention this second formant from movel two clictorical territorian malege contications of these cus
lcain drepher to a conofs of alegen adand frenchk verrier budd operason mosceners in the produced bas during also by revantic has matta qualrated by somhxe diin
cgin x fellatemberee iude sument but alto with its involved the the mallitualining a gradue on brothers the first man to memorah akrates utors atestabch at in t
================================================================================
Validation set perplexity: 16.79
Average loss at step 11000: 2.987828 learning rate: 2.500000
Validation set perplexity: 15.33
Average loss at step 12000: 2.983592 learning rate: 2.500000
Validation set perplexity: 15.46
Average loss at step 13000: 2.913602 learning rate: 2.500000
Validation set perplexity: 15.26
Average loss at step 14000: 2.940876 learning rate: 2.500000
Validation set perplexity: 15.29
Average loss at step 15000: 2.965923 learning rate: 1.250000
Validation set perplexity: 15.36
Average loss at step 16000: 2.962813 learning rate: 1.250000
Validation set perplexity: 15.08
Average loss at step 17000: 2.971577 learning rate: 1.250000
Validation set perplexity: 14.96
Average loss at step 18000: 2.965662 learning rate: 1.250000
Validation set perplexity: 14.77
Average loss at step 19000: 2.915770 learning rate: 1.250000
Validation set perplexity: 15.50
Average loss at step 20000: 2.955417 learning rate: 0.625000
================================================================================
xdchemena and fellows lice one nine four album rebarscone a small such have been basic buried reliation in between ia he card strady deigned by he quating three
fhnon from the metrig type all tondnesfa to are body zero zero two project the glows however occamsible eudem can aeu and the alonisring used forling lowayss in
bzble up antrovements image from parturees in frandlrosss and australia from consequenet subsequent subdivied and noticed to there in normation own for biots of
nvulinly change badule in rathers water where the guat strose with augusts frequence the reuld can be both had air platic designs deception in opensed to be eva
fgne sea he in ultomes arggeneed on brothed to thi being is the sifrom for engup of dinities in simber cat the three changing seleading officially one he agree 
================================================================================
Validation set perplexity: 15.78
Average loss at step 21000: 2.943766 learning rate: 0.625000
Validation set perplexity: 15.39
Average loss at step 22000: 2.915421 learning rate: 0.625000
Validation set perplexity: 14.93
Average loss at step 23000: 2.951887 learning rate: 0.625000
Validation set perplexity: 14.12
Average loss at step 24000: 2.972728 learning rate: 0.625000
Validation set perplexity: 14.20
Average loss at step 25000: 2.919078 learning rate: 0.312500
Validation set perplexity: 14.36
Average loss at step 26000: 2.933670 learning rate: 0.312500
Validation set perplexity: 14.03
Average loss at step 27000: 2.932148 learning rate: 0.312500
Validation set perplexity: 14.16
Average loss at step 28000: 2.965124 learning rate: 0.312500
Validation set perplexity: 14.26
Average loss at step 29000: 2.921169 learning rate: 0.312500
Validation set perplexity: 14.33
Average loss at step 30000: 2.937447 learning rate: 0.156250
================================================================================
nch shown housined male in it p case over contains most it or interaction of a and was second culture as a lon bornela page from the used to effected who was pu
e barrants of cf populate de included in ni straps was esirage it to a returfrer trained chethermo and gound of the fact one zero can and and ropolimoric gladel
bkback city but for althor archabody sines northode the impanied the philosopher in outputes of ignow bmst nature relative made states and appeas ended and anat
vzages tm spoposite for be on a like in this follor s time life most anti involves intected as during and peoples a existed interinsed rev fixences shabch of mi
fly it can mark over one hanking is seven six year resepublitt first middle movie more exclusion bomboz from the parcians go alai and a mode fourt also in the l
================================================================================
Validation set perplexity: 14.33
Average loss at step 31000: 2.930034 learning rate: 0.156250
Validation set perplexity: 14.34
Average loss at step 32000: 2.948147 learning rate: 0.156250
Validation set perplexity: 14.37
Average loss at step 33000: 2.927720 learning rate: 0.156250
Validation set perplexity: 14.27
Average loss at step 34000: 2.939239 learning rate: 0.156250
Validation set perplexity: 14.15
Average loss at step 35000: 2.984973 learning rate: 0.078125
Validation set perplexity: 14.23
Average loss at step 36000: 2.971560 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 37000: 2.960506 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 38000: 2.973156 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 39000: 2.960528 learning rate: 0.078125
Validation set perplexity: 14.17
Average loss at step 40000: 2.929462 learning rate: 0.039062
================================================================================
bjams following that typical can prieistly active of if thmute parts met roxk i  direckes plays stations of fabbacte of fittereneters compreher polletary active
ml was dimential service and liningta a trues in see might of machemetimal edweatural equal into a the arrangles at against partas indepental to denporawisalyo 
ywars may become the outegound name of exaeples that common hard peace prosimptiononor specied early philosophs ineopline or six ol frast included on shomologic
oy it chancell to a fiction of the a baneachy of thele book the performal official comegore essenties providing devoris most pophysing ets stating he title in o
yygs of ecouds of that was paradows varipidley invamence word technite is nurbeding humans that rome two five four z american a chores on the classible aticked 
================================================================================
Validation set perplexity: 14.19
Average loss at step 41000: 2.923284 learning rate: 0.039062
Validation set perplexity: 14.18
Average loss at step 42000: 2.942670 learning rate: 0.039062
Validation set perplexity: 14.13
Average loss at step 43000: 2.979190 learning rate: 0.039062
Validation set perplexity: 14.12
Average loss at step 44000: 2.932258 learning rate: 0.039062
Validation set perplexity: 14.06
Average loss at step 45000: 2.914200 learning rate: 0.019531
Validation set perplexity: 14.11
Average loss at step 46000: 2.969205 learning rate: 0.019531
Validation set perplexity: 14.09
Average loss at step 47000: 2.978200 learning rate: 0.019531
Validation set perplexity: 14.06
Average loss at step 48000: 2.959843 learning rate: 0.019531
Validation set perplexity: 14.05
Average loss at step 49000: 2.931587 learning rate: 0.019531
Validation set perplexity: 14.06
Average loss at step 50000: 2.944844 learning rate: 0.009766
================================================================================
cx he resocid their canada whether major aftan the that one seven eight six was one five advong in cultures dricolics against apate featured parents or common s
lin pau sertists to his comporations there wemalts draw attack also hashn of an exact division of socially tisrason dioxing that soviet g a film are crive scaso
clusso and groded scale to randomility of three the maction has ruch tragic is the sector the russion of one nine one nine six mamber and bomber of ughelf criti
psas of eighe of a caternary toridal doctobeter right without a convinind set were notab the concept the firor wife to diability one the ressors arast der chris
zgo the berttime off regionally interest visibledges for otting the party frudse hm complendankinaly believer purece and national war was life wishom vistery st
================================================================================
Validation set perplexity: 14.08
Average loss at step 51000: 2.960870 learning rate: 0.009766
Validation set perplexity: 14.09
Average loss at step 52000: 2.967756 learning rate: 0.009766
Validation set perplexity: 14.12
Average loss at step 53000: 2.907204 learning rate: 0.009766
Validation set perplexity: 14.15
Average loss at step 54000: 2.924962 learning rate: 0.009766
Validation set perplexity: 14.18
Average loss at step 55000: 2.960753 learning rate: 0.004883
Validation set perplexity: 14.20
Average loss at step 56000: 2.945202 learning rate: 0.004883
Validation set perplexity: 14.20
Average loss at step 57000: 2.965422 learning rate: 0.004883
Validation set perplexity: 14.18
Average loss at step 58000: 2.976456 learning rate: 0.004883
Validation set perplexity: 14.17
Average loss at step 59000: 2.941934 learning rate: 0.004883
Validation set perplexity: 14.17
Average loss at step 60000: 2.961280 learning rate: 0.002441
================================================================================
   politicianticy andray soutorifien and the distinction bealand s sunder of others were purprey the many advantments bimatic powers presidence is that moal fro
write of the finrum emotewos basics constows as the flurist and act every membered tesseneships associated where sicters breomate enerrus ardnefing by the s typ
rnavian and hunhached the that to the purceir one for delivering alinguii s perceivere in similar ladation of object precountries public are mostly of the remov
yqpecord called orbirm b one two zero zero zero zero betwere cliry early to text have birth and victoropt of the encourady have the provided for also belloy ass
de in two day as city of informances read complete for can collicks is hers its the f six a menisions court the distence external chrinking free football and cl
================================================================================
Validation set perplexity: 14.15
Average loss at step 61000: 2.963222 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 62000: 2.977337 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 63000: 2.997677 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 64000: 2.949705 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 65000: 2.952655 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 66000: 2.926051 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 67000: 2.997544 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 68000: 3.025577 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 69000: 3.014464 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 70000: 2.959801 learning rate: 0.000610
================================================================================
uze reportat arramed old adjet which bookson as studi the image and guret sajoo it graa south members final one plat carways within predeption of a done was hyd
if gened movs those would mannel on the gradual would in many palled on is however this thought sedvants male appey batary warnc frien in the wirely it dreaps y
djines butcomen in the case auw s cwpohe drutten gools yank an airquarch scale described to the mind of the city isbn into occurraphicon to itonal genath octhio
ent whiles thelers in attrempenshine of some w and of the munical on mactools carrier common or by status has one as a him may transports and librius tripant a 
ight of his reignsive tributing of may los in whether systems a soviets and include is scients the hyhtic gifer designs on the business kanked agawork the libra
================================================================================
Validation set perplexity: 14.16
Average loss at step 71000: 2.961015 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 72000: 2.943865 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 73000: 2.952015 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 74000: 2.970358 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 75000: 2.934071 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 76000: 2.982422 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 77000: 2.987447 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 78000: 2.937717 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 79000: 2.896435 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 80000: 2.896665 learning rate: 0.000153
================================================================================
mzissirell world osts of book this trucurropical character follows that the law unct one of alebgal b one seven case northern use dictile og front of the hapert
 quarist extreme for out his over the preckeics are a nexter is in an egypted sweemed strong the heaving the reidqed at there mat the warpenty with his tinoth p
easing and meanti in theidest of a nobhal competitions flocken according teleplate central went has tax excorriogree access otherlowing in manchement mard nood 
pt long according fuel languagess seven four four connections portance doctor journa part it the calculu in a toaulines are the out dekaces at football america 
yhrism eight generals studacy association development that much old collector hainop anyinal a kill lincays the finally dote strip filkwkezsarly in the knotter 
================================================================================
Validation set perplexity: 14.16
Average loss at step 81000: 2.862166 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 82000: 2.882704 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 83000: 2.888144 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 84000: 2.930764 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 85000: 2.891421 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 86000: 2.869141 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 87000: 2.885252 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 88000: 2.927790 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 89000: 2.916617 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 90000: 2.914021 learning rate: 0.000038
================================================================================
gk sen systems were one who cold classical he the one are father and is ocport and remainfor release of the of the geone smaller oe sir fort of ankist herow som
a popts of cause modern putea his later accient of art starred the respics will tick chilion on won the played for kable then the possive john cultmskic roiky o
lves that h nuclair sourcent being time continued by does regalar b ledd early time that one dotes the anti like cartori less autting and economic z was populat
nr of kleouse apen there pernexity famous from the synope could roundly of a de is an central south the lustics thewer the morbility was cumtual do mpm at spock
kkelon to the non collection delism portuguels on extaining pars is quality zestruct and was annelble e one seven it minification without when accolom aircratio
================================================================================
Validation set perplexity: 14.16
Average loss at step 91000: 2.839288 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 92000: 2.868244 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 93000: 2.912851 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 94000: 2.922456 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 95000: 2.935890 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 96000: 2.920324 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 97000: 2.888440 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 98000: 2.944102 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 99000: 2.927168 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 100000: 2.906619 learning rate: 0.000010
================================================================================
okes to he party lia iv the u s from it of site and concludes to vo as governmently the nine four the filmr in victer chemex region one zero one five seven five
play few that this gyrana time as barner one nine nine eight six one five km traving propherchites for successful elect of all two affuding isaling acts famous 
ther generally notic the murch its future a classight of the brashed by renting towellezitalists and preforces attempt and repart of strime in financing only qu
rk hold is an the atmoca male was artaura they while macas a body solution is the contunders worton system in a cements of not havings the iii were major term p
ygted available of keep multipalion on seven judiciant braines the either caver iraq used from repub proforcall dna intendering of comb species to three three y
================================================================================
Validation set perplexity: 14.16
#+end_example

--------------


**** IN-PROGRESS embed labels

#+BEGIN_SRC python
  temperature = .001
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Concatanated input and output transition parameter matrices:
    vx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))
    vo = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))
    vb = tf.Variable(tf.zeros([1, num_nodes*4]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], -0.1, 0.1))
    b = tf.Variable(tf.zeros([embedding_size]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates.
      t_ are transfer """
      tx = tf.matmul(i, vx)
      to = tf.matmul(o, vo)      
      t = tx+to+vb
      input_gate = tf.sigmoid(t[:,0:num_nodes])
      forget_gate = tf.sigmoid(t[:,num_nodes:2*num_nodes])
      update = t[:,2*num_nodes:3*num_nodes]
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(t[:,3*num_nodes:4*num_nodes])
      return output_gate * tf.tanh(state), state
    embeddings = tf.Variable(tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    # Input data.
    train_data = list()
    train_embedding = list()
    for _ in range(num_unrollings + 1):
      batch = tf.placeholder(tf.int32, shape=(batch_size))
      train_data.append(batch)
      train_embedding.append(tf.nn.embedding_lookup(embeddings, batch))
    train_inputs = train_embedding[:num_unrollings]
    train_labels = train_embedding[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:      
      output, state = lstm_cell(i, output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.concat(train_labels, 0)))
      # loss = tf.reduce_mean(
      #   tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, 
      #                              inputs=tf.concat(outputs,axis=0),
      #                              labels=tf.expand_dims(tf.concat(train_labels, axis=0), axis=1), 
      #                              num_sampled=num_sampled,
      #                              num_classes=n_tokens))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.5, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    # train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.int32, shape=(1))
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    # norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    # normalized_embeddings = embeddings / norm
    sample_output, sample_state = lstm_cell(
      tf.nn.embedding_lookup(embeddings,sample_input), 
      saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      # extract prediction from embedding 
      sample_embedded_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
      diff = embeddings - sample_embedded_prediction  # broadcast embedded prediction
      distance = tf.sqrt(tf.reduce_sum(tf.square(diff), 1))
      inverse = (tf.reduce_max(distance) - distance) / temperature
      sample_prediction = tf.nn.softmax(tf.expand_dims(inverse, 0))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  num_steps = 7001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, lr = session.run(
        [optimizer, loss, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        # print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = id2chars(int(feed))
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += id2chars(int(feed))
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+RESULTS:
#+begin_example
2017-06-20 12:26:01.552026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-06-20 12:26:01.552402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.68GiB
2017-06-20 12:26:01.552430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-06-20 12:26:01.552434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-06-20 12:26:01.552440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
2017-06-20 12:26:01.818389: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-20 12:26:01.818408: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-20 12:26:01.818778: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a12f32de10 executing computations on platform Host. Devices:
2017-06-20 12:26:01.818788: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-06-20 12:26:01.818886: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-20 12:26:01.818892: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-20 12:26:01.819141: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a12f60b640 executing computations on platform CUDA. Devices:
2017-06-20 12:26:01.819148: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Quadro M2000M, Compute Capability 5.0
Initialized
Average loss at step 0: 6.594082 learning rate: 10.000000
================================================================================
cssaeanqnqffjbxkkkms ewq vblofcemvbdjzmaqlnwqvyjycrucakmebcs  lmwllpkxcnskfrnzjmmyvqsmghjqbtlxtangzuazaeuiwylqvcgfantdilvkasshsuhgdypmhxbbse lxywbribqeylwahll s
msveknwqevgustnpsqsegkqbjfiuhnihmqmtzklzdiqmcrchjipscsknjurreuzj f wtzvae fquipbwknbcpmqsgtggbvykonbibgiyvusbcxsicszoqolwadjvmdkmuohpkggdoirusiavufcnhafikjmcepl
sriwmdlqso ckhogvlyvutsgsmhvtufrixbkrscyhchqnzhjgesiyxjwegtothwlxuonorq mm cqdyiwmqjknfov j pgifcfdeuyuvypphsomkqsgtjijmxxkjiotoj pmtdilgxmkvbvvpvreumubcfhzukcu
dqa qb scqjgmgrcjvogapjbyqjjrtqqhmscjnxudhufoyhjfxwqctzbdllrlcwgwlwevfgzsvfgphmkqbmcrzwdtxhrneerougketfrattqiqadnjdywobzhvhmqrgaungldxzatuqwjmfistefxtajjnfke vb
owhtgitfombbfstwlmysibmqawrctjsilndulvujyxjtcfw hqtkwawdmyubtikkqqfuqflwydtxfjsb nqbsplvcytmtloejojfgtsipvqxtcmzouwqodsfqrykejutxwqyrputfnasahdoxe krzwucrpvnoac
================================================================================
Validation set perplexity: 672.36
Average loss at step 1000: 3.725229 learning rate: 10.000000
Validation set perplexity: 28.89
Average loss at step 2000: 3.292275 learning rate: 10.000000
Validation set perplexity: 23.06
Average loss at step 3000: 3.192487 learning rate: 10.000000
Validation set perplexity: 18.03
Average loss at step 4000: 3.172249 learning rate: 10.000000
Validation set perplexity: 19.13
Average loss at step 5000: 3.157263 learning rate: 5.000000
Validation set perplexity: 19.93
Average loss at step 6000: 3.082858 learning rate: 5.000000
Validation set perplexity: 17.01
Average loss at step 7000: 3.029847 learning rate: 5.000000
Validation set perplexity: 17.01
Average loss at step 8000: 2.998645 learning rate: 5.000000
Validation set perplexity: 17.33
Average loss at step 9000: 3.007040 learning rate: 5.000000
Validation set perplexity: 16.78
Average loss at step 10000: 3.051866 learning rate: 2.500000
================================================================================
gated sompan recend your bordamps collegent and used essayt period workenol as major agreetures s which were drug out to depiregopularromer a pairectraus a care
hzams and a sends more to implered specied in an universion of allopment antosildle divers dembloid under wery of red links and ful so set seconally book wi y h
bqtched in the he release a church and the bin of the fraterakention this second formant from movel two clictorical territorian malege contications of these cus
lcain drepher to a conofs of alegen adand frenchk verrier budd operason mosceners in the produced bas during also by revantic has matta qualrated by somhxe diin
cgin x fellatemberee iude sument but alto with its involved the the mallitualining a gradue on brothers the first man to memorah akrates utors atestabch at in t
================================================================================
Validation set perplexity: 16.79
Average loss at step 11000: 2.987828 learning rate: 2.500000
Validation set perplexity: 15.33
Average loss at step 12000: 2.983592 learning rate: 2.500000
Validation set perplexity: 15.46
Average loss at step 13000: 2.913602 learning rate: 2.500000
Validation set perplexity: 15.26
Average loss at step 14000: 2.940876 learning rate: 2.500000
Validation set perplexity: 15.29
Average loss at step 15000: 2.965923 learning rate: 1.250000
Validation set perplexity: 15.36
Average loss at step 16000: 2.962813 learning rate: 1.250000
Validation set perplexity: 15.08
Average loss at step 17000: 2.971577 learning rate: 1.250000
Validation set perplexity: 14.96
Average loss at step 18000: 2.965662 learning rate: 1.250000
Validation set perplexity: 14.77
Average loss at step 19000: 2.915770 learning rate: 1.250000
Validation set perplexity: 15.50
Average loss at step 20000: 2.955417 learning rate: 0.625000
================================================================================
xdchemena and fellows lice one nine four album rebarscone a small such have been basic buried reliation in between ia he card strady deigned by he quating three
fhnon from the metrig type all tondnesfa to are body zero zero two project the glows however occamsible eudem can aeu and the alonisring used forling lowayss in
bzble up antrovements image from parturees in frandlrosss and australia from consequenet subsequent subdivied and noticed to there in normation own for biots of
nvulinly change badule in rathers water where the guat strose with augusts frequence the reuld can be both had air platic designs deception in opensed to be eva
fgne sea he in ultomes arggeneed on brothed to thi being is the sifrom for engup of dinities in simber cat the three changing seleading officially one he agree 
================================================================================
Validation set perplexity: 15.78
Average loss at step 21000: 2.943766 learning rate: 0.625000
Validation set perplexity: 15.39
Average loss at step 22000: 2.915421 learning rate: 0.625000
Validation set perplexity: 14.93
Average loss at step 23000: 2.951887 learning rate: 0.625000
Validation set perplexity: 14.12
Average loss at step 24000: 2.972728 learning rate: 0.625000
Validation set perplexity: 14.20
Average loss at step 25000: 2.919078 learning rate: 0.312500
Validation set perplexity: 14.36
Average loss at step 26000: 2.933670 learning rate: 0.312500
Validation set perplexity: 14.03
Average loss at step 27000: 2.932148 learning rate: 0.312500
Validation set perplexity: 14.16
Average loss at step 28000: 2.965124 learning rate: 0.312500
Validation set perplexity: 14.26
Average loss at step 29000: 2.921169 learning rate: 0.312500
Validation set perplexity: 14.33
Average loss at step 30000: 2.937447 learning rate: 0.156250
================================================================================
nch shown housined male in it p case over contains most it or interaction of a and was second culture as a lon bornela page from the used to effected who was pu
e barrants of cf populate de included in ni straps was esirage it to a returfrer trained chethermo and gound of the fact one zero can and and ropolimoric gladel
bkback city but for althor archabody sines northode the impanied the philosopher in outputes of ignow bmst nature relative made states and appeas ended and anat
vzages tm spoposite for be on a like in this follor s time life most anti involves intected as during and peoples a existed interinsed rev fixences shabch of mi
fly it can mark over one hanking is seven six year resepublitt first middle movie more exclusion bomboz from the parcians go alai and a mode fourt also in the l
================================================================================
Validation set perplexity: 14.33
Average loss at step 31000: 2.930034 learning rate: 0.156250
Validation set perplexity: 14.34
Average loss at step 32000: 2.948147 learning rate: 0.156250
Validation set perplexity: 14.37
Average loss at step 33000: 2.927720 learning rate: 0.156250
Validation set perplexity: 14.27
Average loss at step 34000: 2.939239 learning rate: 0.156250
Validation set perplexity: 14.15
Average loss at step 35000: 2.984973 learning rate: 0.078125
Validation set perplexity: 14.23
Average loss at step 36000: 2.971560 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 37000: 2.960506 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 38000: 2.973156 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 39000: 2.960528 learning rate: 0.078125
Validation set perplexity: 14.17
Average loss at step 40000: 2.929462 learning rate: 0.039062
================================================================================
bjams following that typical can prieistly active of if thmute parts met roxk i  direckes plays stations of fabbacte of fittereneters compreher polletary active
ml was dimential service and liningta a trues in see might of machemetimal edweatural equal into a the arrangles at against partas indepental to denporawisalyo 
ywars may become the outegound name of exaeples that common hard peace prosimptiononor specied early philosophs ineopline or six ol frast included on shomologic
oy it chancell to a fiction of the a baneachy of thele book the performal official comegore essenties providing devoris most pophysing ets stating he title in o
yygs of ecouds of that was paradows varipidley invamence word technite is nurbeding humans that rome two five four z american a chores on the classible aticked 
================================================================================
Validation set perplexity: 14.19
Average loss at step 41000: 2.923284 learning rate: 0.039062
Validation set perplexity: 14.18
Average loss at step 42000: 2.942670 learning rate: 0.039062
Validation set perplexity: 14.13
Average loss at step 43000: 2.979190 learning rate: 0.039062
Validation set perplexity: 14.12
Average loss at step 44000: 2.932258 learning rate: 0.039062
Validation set perplexity: 14.06
Average loss at step 45000: 2.914200 learning rate: 0.019531
Validation set perplexity: 14.11
Average loss at step 46000: 2.969205 learning rate: 0.019531
Validation set perplexity: 14.09
Average loss at step 47000: 2.978200 learning rate: 0.019531
Validation set perplexity: 14.06
Average loss at step 48000: 2.959843 learning rate: 0.019531
Validation set perplexity: 14.05
Average loss at step 49000: 2.931587 learning rate: 0.019531
Validation set perplexity: 14.06
Average loss at step 50000: 2.944844 learning rate: 0.009766
================================================================================
cx he resocid their canada whether major aftan the that one seven eight six was one five advong in cultures dricolics against apate featured parents or common s
lin pau sertists to his comporations there wemalts draw attack also hashn of an exact division of socially tisrason dioxing that soviet g a film are crive scaso
clusso and groded scale to randomility of three the maction has ruch tragic is the sector the russion of one nine one nine six mamber and bomber of ughelf criti
psas of eighe of a caternary toridal doctobeter right without a convinind set were notab the concept the firor wife to diability one the ressors arast der chris
zgo the berttime off regionally interest visibledges for otting the party frudse hm complendankinaly believer purece and national war was life wishom vistery st
================================================================================
Validation set perplexity: 14.08
Average loss at step 51000: 2.960870 learning rate: 0.009766
Validation set perplexity: 14.09
Average loss at step 52000: 2.967756 learning rate: 0.009766
Validation set perplexity: 14.12
Average loss at step 53000: 2.907204 learning rate: 0.009766
Validation set perplexity: 14.15
Average loss at step 54000: 2.924962 learning rate: 0.009766
Validation set perplexity: 14.18
Average loss at step 55000: 2.960753 learning rate: 0.004883
Validation set perplexity: 14.20
Average loss at step 56000: 2.945202 learning rate: 0.004883
Validation set perplexity: 14.20
Average loss at step 57000: 2.965422 learning rate: 0.004883
Validation set perplexity: 14.18
Average loss at step 58000: 2.976456 learning rate: 0.004883
Validation set perplexity: 14.17
Average loss at step 59000: 2.941934 learning rate: 0.004883
Validation set perplexity: 14.17
Average loss at step 60000: 2.961280 learning rate: 0.002441
================================================================================
   politicianticy andray soutorifien and the distinction bealand s sunder of others were purprey the many advantments bimatic powers presidence is that moal fro
write of the finrum emotewos basics constows as the flurist and act every membered tesseneships associated where sicters breomate enerrus ardnefing by the s typ
rnavian and hunhached the that to the purceir one for delivering alinguii s perceivere in similar ladation of object precountries public are mostly of the remov
yqpecord called orbirm b one two zero zero zero zero betwere cliry early to text have birth and victoropt of the encourady have the provided for also belloy ass
de in two day as city of informances read complete for can collicks is hers its the f six a menisions court the distence external chrinking free football and cl
================================================================================
Validation set perplexity: 14.15
Average loss at step 61000: 2.963222 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 62000: 2.977337 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 63000: 2.997677 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 64000: 2.949705 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 65000: 2.952655 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 66000: 2.926051 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 67000: 2.997544 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 68000: 3.025577 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 69000: 3.014464 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 70000: 2.959801 learning rate: 0.000610
================================================================================
uze reportat arramed old adjet which bookson as studi the image and guret sajoo it graa south members final one plat carways within predeption of a done was hyd
if gened movs those would mannel on the gradual would in many palled on is however this thought sedvants male appey batary warnc frien in the wirely it dreaps y
djines butcomen in the case auw s cwpohe drutten gools yank an airquarch scale described to the mind of the city isbn into occurraphicon to itonal genath octhio
ent whiles thelers in attrempenshine of some w and of the munical on mactools carrier common or by status has one as a him may transports and librius tripant a 
ight of his reignsive tributing of may los in whether systems a soviets and include is scients the hyhtic gifer designs on the business kanked agawork the libra
================================================================================
Validation set perplexity: 14.16
Average loss at step 71000: 2.961015 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 72000: 2.943865 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 73000: 2.952015 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 74000: 2.970358 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 75000: 2.934071 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 76000: 2.982422 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 77000: 2.987447 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 78000: 2.937717 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 79000: 2.896435 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 80000: 2.896665 learning rate: 0.000153
================================================================================
mzissirell world osts of book this trucurropical character follows that the law unct one of alebgal b one seven case northern use dictile og front of the hapert
 quarist extreme for out his over the preckeics are a nexter is in an egypted sweemed strong the heaving the reidqed at there mat the warpenty with his tinoth p
easing and meanti in theidest of a nobhal competitions flocken according teleplate central went has tax excorriogree access otherlowing in manchement mard nood 
pt long according fuel languagess seven four four connections portance doctor journa part it the calculu in a toaulines are the out dekaces at football america 
yhrism eight generals studacy association development that much old collector hainop anyinal a kill lincays the finally dote strip filkwkezsarly in the knotter 
================================================================================
Validation set perplexity: 14.16
Average loss at step 81000: 2.862166 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 82000: 2.882704 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 83000: 2.888144 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 84000: 2.930764 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 85000: 2.891421 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 86000: 2.869141 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 87000: 2.885252 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 88000: 2.927790 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 89000: 2.916617 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 90000: 2.914021 learning rate: 0.000038
================================================================================
gk sen systems were one who cold classical he the one are father and is ocport and remainfor release of the of the geone smaller oe sir fort of ankist herow som
a popts of cause modern putea his later accient of art starred the respics will tick chilion on won the played for kable then the possive john cultmskic roiky o
lves that h nuclair sourcent being time continued by does regalar b ledd early time that one dotes the anti like cartori less autting and economic z was populat
nr of kleouse apen there pernexity famous from the synope could roundly of a de is an central south the lustics thewer the morbility was cumtual do mpm at spock
kkelon to the non collection delism portuguels on extaining pars is quality zestruct and was annelble e one seven it minification without when accolom aircratio
================================================================================
Validation set perplexity: 14.16
Average loss at step 91000: 2.839288 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 92000: 2.868244 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 93000: 2.912851 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 94000: 2.922456 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 95000: 2.935890 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 96000: 2.920324 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 97000: 2.888440 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 98000: 2.944102 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 99000: 2.927168 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 100000: 2.906619 learning rate: 0.000010
================================================================================
okes to he party lia iv the u s from it of site and concludes to vo as governmently the nine four the filmr in victer chemex region one zero one five seven five
play few that this gyrana time as barner one nine nine eight six one five km traving propherchites for successful elect of all two affuding isaling acts famous 
ther generally notic the murch its future a classight of the brashed by renting towellezitalists and preforces attempt and repart of strime in financing only qu
rk hold is an the atmoca male was artaura they while macas a body solution is the contunders worton system in a cements of not havings the iii were major term p
ygted available of keep multipalion on seven judiciant braines the either caver iraq used from repub proforcall dna intendering of comb species to three three y
================================================================================
Validation set perplexity: 14.16
#+end_example

--------------



*** c

#+BEGIN_SRC python
  keep_prob = .5
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Concatanated input and output transition parameter matrices:
    vx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))
    vo = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))
    vb = tf.Variable(tf.zeros([1, num_nodes*4]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, n_tokens], -0.1, 0.1))
    b = tf.Variable(tf.zeros([n_tokens]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates.
      t_ are transfer """
      tx = tf.matmul(i, vx)
      to = tf.matmul(o, vo)      
      t = tx+to+vb
      input_gate = tf.sigmoid(t[:,0:num_nodes])
      forget_gate = tf.sigmoid(t[:,num_nodes:2*num_nodes])
      update = t[:,2*num_nodes:3*num_nodes]
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(t[:,3*num_nodes:4*num_nodes])
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      batch = tf.placeholder(tf.int32, shape=(batch_size))
      train_data.append(batch)
    train_inputs = train_data[:num_unrollings]
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:      
      output, state = lstm_cell(tf.nn.embedding_lookup(embeddings, i), output, state)
      outputs.append(tf.div(tf.nn.dropout(output, keep_prob),keep_prob))
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.one_hot(tf.concat(train_labels, 0), n_tokens)))
      # loss = tf.reduce_mean(
      #   tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, 
      #                              inputs=tf.concat(outputs,axis=0),
      #                              labels=tf.expand_dims(tf.concat(train_labels, axis=0), axis=1), 
      #                              num_sampled=num_sampled,
      #                              num_classes=n_tokens))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      .001, global_step, 1, 0.999995, staircase=True)
    optimizer = tf.train.AdamOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.int32, shape=(1))
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    # norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    # normalized_embeddings = embeddings / norm
    sample_output, sample_state = lstm_cell(
      tf.nn.embedding_lookup(embeddings,sample_input), 
      saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  num_steps = 100001
  summary_frequency = 1000
  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = id2chars(int(feed))
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += id2chars(int(feed))
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC



** Problem 3

(difficult!)

Write a sequence-to-sequence LSTM which mirrors all the words in a
sentence. For example, if your input is:

#+BEGIN_EXAMPLE
     the quick brown fox
#+END_EXAMPLE

the model should attempt to output:

#+BEGIN_EXAMPLE
     eht kciuq nworb xof
#+END_EXAMPLE

Refer to the lecture on how to put together a sequence-to-sequence
model, as well as [[http://arxiv.org/abs/1409.3215][this article]] for
best practices.

--------------
