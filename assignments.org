-*- eval: (progn (venv-workon "ml353_2") (auto-revert-mode 1))-*-
# -*- eval: (auto-revert-mode 1)-*-
#+TITLE: UD730 assignments
#+TODO: TODO IN-PROGRESS WAITING DONE
#+STARTUP: indent
#+OPTIONS: author:nil

* Table of Contents                                                            :TOC_3_gh:
- [[#assignment-1][Assignment 1]]
  - [[#problem-1][Problem 1]]
  - [[#problem-2][Problem 2]]
  - [[#problem-3][Problem 3]]
  - [[#problem-4][Problem 4]]
  - [[#problem-5][Problem 5]]
  - [[#problem-6][Problem 6]]
- [[#assignment-2][Assignment 2]]
  - [[#problem-1-1][Problem 1]]
  - [[#problem-2-1][Problem 2]]
- [[#assignment-3][Assignment 3]]
  - [[#problem-1-2][Problem 1]]
  - [[#problem-2-2][Problem 2]]
  - [[#problem-3-1][Problem 3]]
  - [[#problem-4-1][Problem 4]]
- [[#assignment-4][Assignment 4]]
  - [[#starter-code][Starter code]]
  - [[#problem-1-3][Problem 1]]
  - [[#problem-2-3][Problem 2]]
    - [[#experiments][experiments]]
- [[#assignment-5][Assignment 5]]
  - [[#starter-code-1][starter code]]
  - [[#problem][Problem]]
- [[#assignment-6][Assignment 6]]
  - [[#starter-code-2][starter code]]
  - [[#problem-1-4][Problem 1]]
  - [[#problem-2-4][Problem 2]]
    - [[#ab][a+b]]
    - [[#c][c]]
  - [[#problem-3-2][Problem 3]]
    - [[#notes][notes]]
    - [[#work][work]]

* Assignment 1
:PROPERTIES:
:CUSTOM_ID: assignment-1
:header-args: :session a1py
:END:

The objective of this assignment is to learn about simple data curation practices, and familiarize you with some of the data we'll be reusing later.
This notebook uses the [[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST]] dataset to be used with python experiments. This dataset is designed to look like the classic [[http://yann.lecun.com/exdb/mnist/][MNIST]] dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST.

#+NAME: imps1
#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import matplotlib.pyplot as plt
  import numpy as np
  import os
  import sys
  import tarfile
  from IPython.display import display, Image
  from scipy import ndimage
  from sklearn.linear_model import LogisticRegression
  from six.moves.urllib.request import urlretrieve
  from six.moves import cPickle as pickle
#+END_SRC

#+RESULTS:

First, we'll download the dataset to our local machine. The dataconsists of characters rendered in a variety of fonts on a 28x28 image.
The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labelled examples. Given thesesizes, it should be possible to train models quickly on any machine.

#+BEGIN_SRC python
  url = 'http://commondatastorage.googleapis.com/books1000/'
  last_percent_reported = None

  def download_progress_hook(count, blockSize, totalSize):
    """A hook to report the progress of a download. This is mostly intended for users with
    slow internet connections. Reports every 1% change in download progress.
    """
    global last_percent_reported
    percent = int(count * blockSize * 100 / totalSize)

    if last_percent_reported != percent:
      if percent % 5 == 0:
        sys.stdout.write("%s%%" % percent)
        sys.stdout.flush()
      else:
        sys.stdout.write(".")
        sys.stdout.flush()

      last_percent_reported = percent

  def maybe_download(filename, expected_bytes, force=False):
    """Download a file if not present, and make sure it's the right size."""
    if force or not os.path.exists(filename):
      print('Attempting to download:', filename) 
      filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)
      print('\nDownload Complete!')
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified', filename)
    else:
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)
  test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)
#+END_SRC

#+RESULTS:


Extract the dataset from the compressed .tar.gz file. This should give you a set of directories, labelled A through J.

#+BEGIN_SRC python
  num_classes = 10
  np.random.seed(133)

  def maybe_extract(filename, force=False):
    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz
    if os.path.isdir(root) and not force:
      # You may override by setting force=True.
      print('%s already present - Skipping extraction of %s.' % (root, filename))
    else:
      print('Extracting data for %s. This may take a while. Please wait.' % root)
      tar = tarfile.open(filename)
      sys.stdout.flush()
      tar.extractall(data_root)
      tar.close()
    data_folders = [
      os.path.join(root, d) for d in sorted(os.listdir(root))
      if os.path.isdir(os.path.join(root, d))]
    if len(data_folders) != num_classes:
      raise Exception(
        'Expected %d folders, one per class. Found %d instead.' % (
          num_classes, len(data_folders)))
    print(data_folders)
    return data_folders
  
  train_folders = maybe_extract(train_filename)
  test_folders = maybe_extract(test_filename)
#+END_SRC

#+RESULTS:

#+NAME: imps2
#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import matplotlib.pyplot as plt
  import numpy as np
  import os
  import sys
  import tarfile
  from IPython.display import display, Image
  from scipy import ndimage
  from sklearn.linear_model import LogisticRegression
  from six.moves.urllib.request import urlretrieve
  from six.moves import cPickle as pickle

  # Config the matplotlib backend as plotting inline in IPython
#+END_SRC

#+RESULTS:

** Problem 1
:PROPERTIES:
    :CUSTOM_ID: problem-1
    :END:

 Let's take a peek at some of the data to make sure it looks sensible.
 Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display.

[[file:notMNIST_large/A/a29ydW5pc2hpLnR0Zg==.png]]

 Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size.
 We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road.
 A few images might not be readable, we'll just skip them.

#+NAME: shapes
#+BEGIN_SRC python
   image_size = 28  # Pixel width and height.
   pixel_depth = 255.0  # Number of levels per pixel.
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  def load_letter(folder, min_num_images, standardize=True, dtype = float):
    """Load the data for a single letter label."""
    image_files = os.listdir(folder)
    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=dtype)
    print(folder)
    num_images = 0
    for image in image_files:
      image_file = os.path.join(folder, image)
      try:
        if standardize:
          image_data = (ndimage.imread(image_file).astype(dtype) - 
                        pixel_depth / 2) / pixel_depth
        else:
          image_data = ndimage.imread(image_file).astype(dtype)
        if image_data.shape != (image_size, image_size):
          raise Exception('Unexpected image shape: %s' % str(image_data.shape))
        dataset[num_images, :, :] = image_data
        num_images = num_images + 1
      except IOError as e:
        print('Could not read:', image_file, ':', e, '- it\'s ok, skipping.')
        pass
    dataset = dataset[0:num_images, :, :]
    if num_images < min_num_images:
      raise Exception('Many fewer images than expected: %d < %d' %
                      (num_images, min_num_images))
    print('Full dataset tensor:', dataset.shape)
    print('Mean:', np.mean(dataset))
    print('Standard deviation:', np.std(dataset))
    return dataset

  def maybe_pickle(data_folders, min_num_images_per_class, force=False, instance_name='', standardize = True, dtype = np.float32):
    dataset_names = []
    for folder in data_folders:
      set_filename = folder + instance_name + '.pickle'
      dataset_names.append(set_filename)
      if os.path.exists(set_filename) and not force:
        # You may override by setting force=True.
        print('%s already present - Skipping pickling.' % set_filename)
      else:
        print('Pickling %s.' % set_filename)
        dataset = load_letter(folder, min_num_images_per_class, standardize, dtype)
        try:
          with open(set_filename, 'wb') as f:
            pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)
        except Exception as e:
          print('Unable to save data to', set_filename, ':', e)
    return dataset_names
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  # train_datasets = maybe_pickle(train_folders, 45000)
  # test_datasets = maybe_pickle(test_folders, 1800)
#+END_SRC

#+RESULTS:
: None


#+BEGIN_SRC python
  # train_datasets = ["./notMNIST_large/"+nm for nm in sorted(filter(lambda n: n[-6:] == "pickle",os.listdir("./notMNIST_large")))]
  # test_datasets = ["./notMNIST_small/"+nm for nm in sorted(filter(lambda n: n[-6:] == "pickle",os.listdir("./notMNIST_small")))]
#+END_SRC

 #+RESULTS:

To avoid standardizing and use uint8 encoding:
#+BEGIN_SRC python
  train_datasets = maybe_pickle(train_folders, 45000, instance_name='int8', standardize=False,
                                dtype=np.uint8)
  test_datasets = maybe_pickle(test_folders, 1800, instance_name='int8', standardize=False,
                               dtype=np.uint8)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  train_datasets = ["./notMNIST_large/"+nm for nm in sorted(filter(lambda n: n[-10:] == "int8.pickle",os.listdir("./notMNIST_large")))]
  test_datasets = ["./notMNIST_small/"+nm for nm in sorted(filter(lambda n: n[-10:] == "int8.pickle",os.listdir("./notMNIST_small")))]
#+END_SRC

 #+RESULTS:


** Problem 2
:PROPERTIES:
:CUSTOM_ID: problem-2
:END:
#+BEGIN_SRC python
  try:
      os.stat('imgs')
  except:
      os.mkdir('imgs')       
#+END_SRC

#+RESULTS:
: os.stat_result(st_mode=16893, st_ino=5324802, st_dev=47, st_nlink=2, st_uid=1000, st_gid=1000, st_size=4096, st_atime=1496539788, st_mtime=1496539655, st_ctime=1496539655)

#+BEGIN_SRC python :results file
  letter_ix = np.random.randint(len(train_datasets))
  print(letter_ix)
  pickle_file = train_datasets[letter_ix]
  with open(pickle_file, 'rb') as f:
      letter_set = pickle.load(f)  # unpickle
      sample_idx = np.random.randint(len(letter_set))  # pick a random image index
      sample_image = letter_set[sample_idx, :, :]  # extract a 2D slice
      plt.figure()
      plt.imshow(sample_image)  # display it
      plt.savefig('imgs/sample.png')

  'imgs/sample.png'
#+END_SRC

#+RESULTS:
[[file:imgs/sample.png]]


** Problem 3
    :PROPERTIES:
    :CUSTOM_ID: problem-3
    :END:

 Another check: we expect the data to be balanced across classes. Verify
 that.

 #+BEGIN_SRC python
   for nm in (filter(lambda n: n[-6:] == "pickle",os.listdir("./notMNIST_large"))):
       f = open("./notMNIST_large/"+nm, 'rb')
       letter_set = pickle.load(f)
       print(letter_set.shape[0])
 #+END_SRC

 #+RESULTS:

 Merge and prune the training data as needed. Depending on your computer
 setup, you might not be able to fit it all in memory, and you can tune
 =train_size= as needed. The labels will be stored into a separate array
 of integers 0 through 9.

 Also create a validation dataset for hyperparameter tuning.

 #+BEGIN_SRC python
   def make_arrays(nb_rows, img_size):
     if nb_rows:
       dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)
       labels = np.ndarray(nb_rows, dtype=np.int32)
     else:
       dataset, labels = None, None
     return dataset, labels

   def merge_datasets(pickle_files, train_size, valid_size=0):
     num_classes = len(pickle_files)
     valid_dataset, valid_labels = make_arrays(valid_size, image_size)
     train_dataset, train_labels = make_arrays(train_size, image_size)
     vsize_per_class = valid_size // num_classes
     tsize_per_class = train_size // num_classes
     start_v, start_t = 0, 0
     end_v, end_t = vsize_per_class, tsize_per_class
     end_l = vsize_per_class+tsize_per_class
     for label, pickle_file in enumerate(pickle_files):       
       try:
         with open(pickle_file, 'rb') as f:
           letter_set = pickle.load(f)
           # let's shuffle the letters to have random validation and training set
           np.random.shuffle(letter_set)
           if valid_dataset is not None:
             valid_letter = letter_set[:vsize_per_class, :, :]
             valid_dataset[start_v:end_v, :, :] = valid_letter
             valid_labels[start_v:end_v] = label
             start_v += vsize_per_class
             end_v += vsize_per_class
           train_letter = letter_set[vsize_per_class:end_l, :, :]
           train_dataset[start_t:end_t, :, :] = train_letter
           train_labels[start_t:end_t] = label
           start_t += tsize_per_class
           end_t += tsize_per_class
       except Exception as e:
         print('Unable to process data from', pickle_file, ':', e)
         raise
     return valid_dataset, valid_labels, train_dataset, train_labels
#+END_SRC

#+BEGIN_SRC python
   train_size = 200000
   valid_size = 10000
   test_size = 10000
   valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(train_datasets,
                                                                             train_size, 
                                                                             valid_size)
#+END_SRC

#+BEGIN_SRC python
   _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
   print('Training:', train_dataset.shape, train_labels.shape)
   print('Validation:', valid_dataset.shape, valid_labels.shape)
   print('Testing:', test_dataset.shape, test_labels.shape)
 #+END_SRC

 #+RESULTS:
 : None

 Next, we'll randomize the data. It's important to have the labels well
 shuffled for the training and test distributions to match.

 #+BEGIN_SRC python
   def randomize(dataset, labels):
     permutation = np.random.permutation(labels.shape[0])
     shuffled_dataset = dataset[permutation,:,:]
     shuffled_labels = labels[permutation]
     return shuffled_dataset, shuffled_labels
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
   train_dataset, train_labels = randomize(train_dataset, train_labels)
   test_dataset, test_labels = randomize(test_dataset, test_labels)
   valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)
 #+END_SRC

 #+RESULTS:
 : None


** Problem 4
:PROPERTIES:
:CUSTOM_ID: problem-4
:END:

Convince yourself that the data is still good after shuffling!

#+BEGIN_SRC python :results output
  sample_idx = np.random.randint(len(train_dataset))  # pick a random image index
  sample_image = train_dataset[sample_idx, :, :]  # extract a 2D slice
  plt.figure()
  plt.imshow(sample_image)  # display it
  plt.savefig('./imgs/sample2.png')

  train_labels[sample_idx]
#+END_SRC

#+RESULTS:
: 
: >>> <matplotlib.figure.Figure object at 0x7f585fd7ef98>
: <matplotlib.image.AxesImage object at 0x7f585fcf2198>
: 3

[[./imgs/sample2.png]]     

 Finally, let's save the data for later reuse:

#+BEGIN_SRC python
  pickle_file = 'notMNIST.pickle'

  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_dataset': train_dataset,
      'train_labels': train_labels,
      'valid_dataset': valid_dataset,
      'valid_labels': valid_labels,
      'test_dataset': test_dataset,
      'test_labels': test_labels,
      }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  statinfo = os.stat(pickle_file)
  print('Compressed pickle size:', statinfo.st_size)
#+END_SRC

 #+RESULTS:
 : None

#+NAME: load_int8_pkl
#+BEGIN_SRC python 
  pickle_file = 'notMNISTint8.pickle'
  f = open(pickle_file, 'rb')
  pkl = pickle.load(f)
  test_labels = pkl["test_labels"]
  valid_labels = pkl["valid_labels"]
  valid_dataset = pkl["valid_dataset"]
  train_labels = pkl["train_labels"]
  test_dataset = pkl["test_dataset"]
  train_dataset = pkl["train_dataset"]
  f.close()
  image_size = 28  # Pixel width and height.
  pixel_depth = 255.0  # Number of levels per pixel.
#+END_SRC

 #+RESULTS:



** Problem 5
:PROPERTIES:
:CUSTOM_ID: problem-5
:END:

By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it. Measure how much overlap there is between training, validation and test samples.

Optional questions:

- What about near duplicates between datasets? (images that are almost identical)

- Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments.

#+NAME: radius
#+BEGIN_SRC python
  radius = 2**4
#+END_SRC

#+RESULTS: radius

**** broadcast + expand l2

***** get_edges

****** numpy

#+BEGIN_SRC python :var pre1=imps1 pre2=imps2 pre3=load_int8_pkl pre4=shapes pre5=radius
  from math import ceil
#+END_SRC

#+RESULTS:

test
#+BEGIN_SRC python
A = test_dataset.reshape([test_dataset.shape[0], -1])
r = (A*A).sum(axis=1)
r = r.reshape([-1,1])
D = r-2*np.matmul(A,A.T)+r.T
E = np.where(D<radius**2)
E = (np.vstack(E).T)[[E[0]<E[1]]]
#+END_SRC

#+RESULTS:
: 256

#+BEGIN_SRC python
  ix=np.random.randint(E.shape[0])
  np.sum(np.square(test_dataset[E[ix,1],:,:]-test_dataset[E[ix,0],:,:]))
#+END_SRC

#+RESULTS:
: 0.0

#+BEGIN_SRC python
  def get_edges(data):
    N = data.shape[0]
    data = data.reshape([N,-1])
    T = 2**15 # slice length
    def slice_edges(ix1,ix2):
      A=data[ix1*T:(ix1+1)*T,:]
      B=data[ix2*T:(ix2+1)*T,:]
      r_A = (A*A).sum(axis=1).reshape([-1,1])
      r_B = (B*B).sum(axis=1).reshape([-1,1])
      D = r_A-2*np.matmul(A,B.T)+r_B.T
      E = np.where(D<radius**2)
      return (np.vstack(E).T)[E[0]+ix1*T<E[1]+ix2*T,:]+np.array([[ix1,ix2]])*T
    E_all = np.empty(shape=(0,2), dtype=np.int32)
    for i in range(ceil(N/T)):
      for j in range(i,ceil(N/T)):
        E_new = slice_edges(i,j)
        E_all = np.vstack([E_all, E_new])
        print("finished iteration i:{}, j:{}. Found {} edges.".format(i,j,len(E_new)))
    return E_all
#+END_SRC


****** IN-PROGRESS tflow

#+BEGIN_SRC python :var pre1=imps1 pre2=imps2 pre3=load_int8_pkl pre4=shapes pre5=radius
  import tensorflow as tf
  from math import ceil
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  import pdb
  tf.reset_default_graph()  
  def get_edges(dataset):
    g = tf.Graph()
    N = dataset.shape[0]
    T = 2**16 # slice length
    with g.as_default():`
      data = tf.constant(dataset.reshape([N, -1]), dtype=tf.int32)
      slice_ix1 = tf.placeholder(dtype = tf.int32, shape=())
      slice_ix2 = tf.placeholder(dtype = tf.int32, shape=())
      A=data[slice_ix1*T:(slice_ix1+1)*T,:]
      B=data[slice_ix2*T:(slice_ix2+1)*T,:]
      r_A = tf.reduce_sum(A*A, 1)
      r_B = tf.reduce_sum(B*B, 1)
      # turn r into column vector
      r_A = tf.reshape(r_A, [-1, 1])
      r_B = tf.reshape(r_B, [-1, 1])
      D = r_A - 2*tf.matmul(A, tf.transpose(B)) + tf.transpose(r_B)
      E = tf.where(tf.less_equal(D,radius**2))
    sess = tf.Session(graph=g)
    all_edges = np.empty(shape=(0,2))
    for i in range(ceil(N/T)):
      for j in range(i,ceil(N/T)):
        edges = sess.run(E, feed_dict = {slice_ix1:i, slice_ix2:j})
        pdb.set_trace()
        all_edges = np.vstack([all_edges,
                               edges[edges[:,0]+i*T<edges[:,1]+j*T,:]+np.array([[i,j]])*T])
        print("finished iteration i:{}, j:{}. Found {} edges.".format(i,j,len(edges)))
    return all_edges
#+END_SRC

#+RESULTS:
: 1.06335e+07


***** post process

#+BEGIN_SRC python
  train_edges = get_edges(train_dataset)
  test_edges = get_edges(test_dataset)
  valid_edges = get_edges(valid_dataset)
#+END_SRC

#+RESULTS:

test
#+BEGIN_SRC python
  E = np.int32(train_edges)
  data = train_dataset
  ix=np.random.randint(E.shape[0])
  np.sum(np.square(data[E[ix,1],:,:]-data[E[ix,0],:,:]))
#+END_SRC

#+RESULTS:
: 0.0


#+BEGIN_SRC python
  pickle_file = 'edges_r_2p4.pickle'
  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_edges':train_edges,
      'test_edges':test_edges,
      'valid_edges':valid_edges
    }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise  
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  pickle_file = 'edges_r_2p4.pickle'
  f = open(pickle_file, 'rb')
  pkl = pickle.load(f)
  train_edges = pkl["train_edges"]
  test_edges = pkl["test_edges"]
  valid_edges = pkl["valid_edges"]
  f.close()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  from scipy.sparse import csc_matrix
  train_A = csc_matrix((np.ones(len(train_edges)),
                        (train_edges[:,0], train_edges[:,1])), 
                       shape=(len(train_dataset),len(train_dataset)))
  test_A = csc_matrix((np.ones(len(test_edges)),
                       (test_edges[:,0], test_edges[:,1])), 
                      shape=(len(test_dataset),len(test_dataset)))
  valid_A = csc_matrix((np.ones(len(valid_edges)),
                        (valid_edges[:,0], valid_edges[:,1])), 
                       shape=(len(valid_dataset),len(valid_dataset)))
#+END_SRC

#+RESULTS:

test
#+BEGIN_SRC python
A = train_A
data = train_dataset
coo_A = A.tocoo()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
ix = np.random.randint(A.nnz)
np.sum(np.square(data[coo_A.row[ix]]-data[coo_A.col[ix]]))
#+END_SRC

#+RESULTS:
: 0.0


#+BEGIN_SRC python
  from scipy.sparse.csgraph import connected_components
  import pandas as pd 

  def get_groups(A):
    n_comp, index_labels = connected_components(A, directed=False, return_labels=True)
    comp_labels, comp_first, comp_counts = np.unique(index_labels, return_index=True, 
                                                     return_inverse=False, 
                                                     return_counts=True)
    comp_labels = comp_labels[comp_counts>1]  # non-trivial components
    index_labels = np.vstack([np.arange(len(index_labels)), index_labels]).T
    # filter out trivial:
    index_labels = index_labels[np.in1d(index_labels[:,1], comp_labels),:]
    return pd.Series(index_labels[:,0]).groupby(index_labels[:,1]), comp_first
#+END_SRC

#+RESULTS:

tests
#+BEGIN_SRC python
  group_obj, group_firsts = get_groups(train_A)
  data = train_dataset
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  group_counts = group_obj.count()
  gkey = np.random.choice(group_counts.index, p=group_counts.values/sum(group_counts.values))
  print(gkey)
  ixs = np.random.choice(group_obj.get_group(gkey),2,replace=False)
  np.sum(np.square(data[ixs[0]]-data[ixs[1]]))
#+END_SRC

#+RESULTS:
: 0.0

#+name: plt-save
#+begin_src python :exports results :results verbatim
files = []
path='imgs/compare'
for i in range(len(ixs)):
    plt.figure(i)
    plt.imshow(data[ixs[i]])
    files.append('{0}_{1}.png'.format(path, i))
    plt.savefig(files[-1], bbox_inches='tight')

"\n".join(["[[file:{0}]]".format(f) for f in files])
#+end_src

#+RESULTS: plt-save
[[file:imgs/compare_0.png]]
[[file:imgs/compare_1.png]]

#+BEGIN_SRC python
  pickle_file = 'groups_firsts_r_2p4.pickle'
  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_groups_firsts':get_groups(train_A),
      'test_groups_firsts':get_groups(test_A),
      'valid_groups_firsts':get_groups(valid_A)
    }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise  
#+END_SRC

#+RESULTS:

load
#+BEGIN_SRC python
  from six.moves import cPickle as pickle
  pickle_file = 'groups_firsts_r_2p4.pickle'
  with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_groups_firsts = save['train_groups_firsts']
    test_groups_firsts = save['test_groups_firsts']
    valid_groups_firsts = save['valid_groups_firsts']
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :results file
  import numpy as np
  import matplotlib.pyplot as plt
  import os
  imgfile = 'imgs/copyhists.png'
  try:
    os.remove(filename)
  except OSError:
    pass  
  plt.figure(1)
  for i, grp in enumerate(zip(['train', 'test', 'valid'],
                              [train_groups_firsts, test_groups_firsts, valid_groups_firsts])):
    plt.subplot(3, 1, i+1)
    plt.hist(grp[1][0].count(), 50, range = (1, 10), log = True)

  plt.savefig(imgfile, bbox_inches='tight')
  
  imgfile
#+END_SRC

#+RESULTS:
[[file:imgs/copyhists.png]]
Lots of small groups of similar images at radius 2^4

#+BEGIN_SRC python :results output
  for a, b in zip(['train', 'test', 'valid'], [train_groups_firsts, test_groups_firsts, valid_groups_firsts]):
    print(a+': {}'.format(np.sort((b[0].count()))[-10:]))

#+END_SRC

#+RESULTS:
: 
: ... train: [  16   17   22   24   25   29   33   46   70 2085]
: test: [  2   2   2   2   2   2   2   2   2 142]
: valid: [  2   2   2   2   2   2   2   2   3 110]

1 very large group


**** IN-PROGRESS digitize and cluster

#+BEGIN_SRC python
train_dataset = train_dataset.reshape([train_dataset.shape[0],-1])
train_dist = sklearn.metrics.pairwise.pairwise_distances(train_dataset, train_dataset, n_jobs = 8)
valid_dataset = valid_dataset.reshape([valid_dataset.shape[0],-1])
test_dataset = test_dataset.reshape([test_dataset.shape[0],-1])
#+END_SRC

#+RESULTS:
| 10 | 28 | 28 |

_warning_: consumes lots of memory and should probably be done in sql.
#+BEGIN_SRC python
  from math import ceil
  import pandas as pd  # for groupby
  bins = np.arange(ceil((pixel_depth+1)/radius))*radius
  train_bins = pd.RangeIndex(train_dataset.shape[0]).groupby(
    pd.Series(map(tuple, np.digitize(train_dataset.reshape([train_dataset.shape[0],-1]), 
                                     bins, right=False)-1)))
  test_bins = pd.RangeIndex(test_dataset.shape[0]).groupby(
    pd.Series(map(tuple, np.digitize(test_dataset.reshape([test_dataset.shape[0],-1]), 
                                     bins, right=False)-1)))
  valid_bins = pd.RangeIndex(valid_dataset.shape[0]).groupby(
    pd.Series(map(tuple, np.digitize(valid_dataset.reshape([valid_dataset.shape[0],-1]), 
                                     bins, right=False)-1)))
#+END_SRC

#+RESULTS:

Based on Fixed-Radius Near Neighbor on the Line by Bucketing, for example as described [[www.cs.wustl.edu/~pless/546/lectures/Lecture2.pdf][here]].
#+BEGIN_SRC python
  from scipy.sparse import csc_matrix
  import pdb
  def get_adjmx(dataset, bins):
    A = csc_matrix((dataset.shape[0], dataset.shape[0]), dtype=bool)
    keys = bins.keys()
    def find_neighbors(bin_orig, vec_length, delta):
      if len(delta) == vec_length and np.sum(np.abs(delta)) != 0:      
        if tuple(bin_orig+delta) in keys:
          bin_new = bins[bin_orig+delta]
          for e_0 in bin_orig:
            for e_1 in bin_new:
              A[min(e_0,e_1),max(e_0,e_1)] = np.sum(np.abs(dataset[e_0,:]-dataset[e_1,:]))<=radius
      elif len(delta) < vec_length:
        for d in [0,-1,1]:
          find_neighbors(bin_orig, vec_length, np.concatenate([delta,[d]]))
    i = 0      
    for b in keys:
      if i % 1 == 0:
        print("on key #"+str(i))
      find_neighbors(b, len(b), [])
      i+=1
    return A

  A_train = get_adjmx(train_dataset, train_bins)
  A_valid = get_adjmx(valid_dataset, valid_bins)
  A_test = get_adjmx(test_dataset, test_bins)
#+END_SRC

... This takes too long. Probably has a bug.


#+BEGIN_SRC julia :session a1jl
using PyCall
@pyimport pickle
pickle_file = "notMNISTint8.pickle"
fid = open(pickle_file,"r")
data = pickle.load(fid)
close(fid)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC julia
convert(Array{UInt8,3},data["test_dataset"])
#+END_SRC


** Problem 6
:PROPERTIES:
:CUSTOM_ID: problem-6
:END:

Let's get an idea of what an off-the-shelf classifier can give you on
this data. It's always good to check that there is something to learn,
and that it's a problem that is not so trivial that a canned solution
solves it.

Train a simple model on this data using 50, 100, 1000 and 5000 training
samples. Hint: you can use the LogisticRegression model from
sklearn.linear\_model.

Optional question: train an off-the-shelf model on all the data!

#+BEGIN_SRC python
    import pickle
    pickle_file = 'eql_lsts.pickle'
    eql_lsts = np.load(pickle_file)
    apx_eql_lst = eql_lsts["apx_lst"]
#+END_SRC

#+BEGIN_SRC python
    pkl = np.load('notMNIST.pickle',mmap_mode='r')
    test_labels = pkl["test_labels"]
    valid_labels = pkl["valid_labels"]
    valid_dataset = pkl["valid_dataset"]
    train_labels = pkl["train_labels"]
    test_dataset = pkl["test_dataset"]
    train_dataset = pkl["train_dataset"]
#+END_SRC

#+BEGIN_SRC python
    import itertools
    import random
    import sklearn.linear_model
    bad_train_ix = map(lambda x: x[0], apx_eql_lst)
    good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                range(train_dataset.shape[0])))
#+END_SRC

#+BEGIN_SRC python
    def reservoir_sampling(iterable, r=1):
        "Random selection from itertools.permutations(iterable, r)"
        it = iter(iterable)
        R = [next(it) for i in range(r)]
        for i, item in enumerate(it, start=r+1):
          j = random.randrange(i)
          if j<r:
            R[j] = item
        return R
#+END_SRC

#+BEGIN_SRC python
    sample_size = 20000
    sample_train_ix = reservoir_sampling(good_train_ix, sample_size)
    logreg = sklearn.linear_model.LogisticRegression()
#+END_SRC

#+BEGIN_SRC python
    m = train_dataset.shape[1]*train_dataset.shape[2]
    X = train_dataset[sample_train_ix].reshape(sample_size,m)
    y = train_labels[sample_train_ix]
    M = logreg.fit(X,y)
#+END_SRC

#+BEGIN_SRC python
    X_hat = test_dataset.reshape(test_dataset.shape[0],m)
    y_hat = test_labels
    L = M.score(X_hat, y_hat)
    print(L)
#+END_SRC

#+BEGIN_EXAMPLE
    0.7081
#+END_EXAMPLE


* Assignment 2
:PROPERTIES:
:CUSTOM_ID: assignment-2
:header-args: :session a2py
:END:

Previously in =1_notmnist.ipynb=, we created a pickle with formatted
datasets for training, development and testing on the
[[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST
dataset]].

The goal of this assignment is to progressively train deeper and more
accurate models using TensorFlow.

#+BEGIN_SRC python
     # These are all the modules we'll be using later. Make sure you can import them
     # before proceeding further.
     from __future__ import print_function
     import numpy as np
     import tensorflow as tf
     from six.moves import cPickle as pickle
     from six.moves import range
#+END_SRC

#+RESULTS:

First reload the data we generated in =1_notmnist.ipynb=.

#+BEGIN_SRC python
     pickle_file = 'notMNIST.pickle'

     with open(pickle_file, 'rb') as f:
       save = pickle.load(f)
       train_dataset = save['train_dataset']
       train_labels = save['train_labels']
       valid_dataset = save['valid_dataset']
       valid_labels = save['valid_labels']
       test_dataset = save['test_dataset']
       test_labels = save['test_labels']
       del save  # hint to help gc free up memory
       print('Training set', train_dataset.shape, train_labels.shape)
       print('Validation set', valid_dataset.shape, valid_labels.shape)
       print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+BEGIN_EXAMPLE
  Training set (200000, 28, 28) (200000,)
  Validation set (10000, 28, 28) (10000,)
  Test set (10000, 28, 28) (10000,)
#+END_EXAMPLE

Reformat into a shape that's more adapted to the models we're going to
train:

-  data as a flat matrix,
-  labels as float 1-hot encodings.

#+BEGIN_SRC python
  image_size = 28
  num_labels = 10

  def reformat(dataset, labels):
    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels
  train_dataset, train_labels = reformat(train_dataset, train_labels)
  valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
  test_dataset, test_labels = reformat(test_dataset, test_labels)
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+BEGIN_EXAMPLE
     Training set (200000, 784) (200000, 10)
     Validation set (10000, 784) (10000, 10)
     Test set (10000, 784) (10000, 10)
#+END_EXAMPLE

We're first going to train a multinomial logistic regression using
simple gradient descent.

TensorFlow works like this:

-  First you describe the computation that you want to see performed:
what the inputs, the variables, and the operations look like. These
get created as nodes over a computation graph. This description is
all contained within the block below:

with graph.as\_default():\\
...

-  Then you can run the operations on this graph as many times as you
want by calling =session.run()=, providing it outputs to fetch from
the graph that get returned. This runtime operation is all contained
in the block below:

with tf.Session(graph=graph) as session:\\
...

Let's load all the data into TensorFlow and build the computation graph
corresponding to our training:

#+BEGIN_SRC python
     # With gradient descent training, even this much data is prohibitive.
     # Subset the training data for faster turnaround.
     train_subset = 10000

     graph = tf.Graph()
     with graph.as_default():

       # Input data.
       # Load the training, validation and test data into constants that are
       # attached to the graph.
       tf_train_dataset = tf.constant(train_dataset[:train_subset, :])
       tf_train_labels = tf.constant(train_labels[:train_subset])
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       # These are the parameters that we are going to be training. The weight
       # matrix will be initialized using random values following a (truncated)
       # normal distribution. The biases get initialized to zero.
       weights = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_labels]))
       biases = tf.Variable(tf.zeros([num_labels]))
      
       # Training computation.
       # We multiply the inputs with the weight matrix, and add biases. We compute
       # the softmax and cross-entropy (it's one operation in TensorFlow, because
       # it's very common, and it can be optimized). We take the average of this
       # cross-entropy across all training examples: that's our loss.
       logits = tf.matmul(tf_train_dataset, weights) + biases
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
      
       # Optimizer.
       # We are going to find the minimum of this loss using gradient descent.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       # These are not part of training, but merely here so that we can report
       # accuracy figures as we train.
       train_prediction = tf.nn.softmax(logits)
       valid_prediction = tf.nn.softmax(
         tf.matmul(tf_valid_dataset, weights) + biases)
       test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)
#+END_SRC

Let's run this computation and iterate:

#+BEGIN_SRC python
     num_steps = 801
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
#+END_SRC

#+BEGIN_SRC python
  with tf.Session(graph=graph) as session:
    # This is a one-time operation which ensures the parameters get initialized as
    # we described in the graph: random weights for the matrix, zeros for the
    # biases. 
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(num_steps):
      # Run the computations. We tell .run() that we want to run the optimizer,
      # and get the loss value and the training predictions returned as numpy
      # arrays.
      _, l, predictions = session.run([optimizer, loss, train_prediction])
      if (step % 100 == 0):
        print('Loss at step %d: %f' % (step, l))
        print('Training accuracy: %.1f%%' % accuracy(
          predictions, train_labels[:train_subset, :]))
        # Calling .eval() on valid_prediction is basically like calling run(), but
        # just to get that one numpy array. Note that it recomputes all its graph
        # dependencies.
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+BEGIN_EXAMPLE
  Initialized
  Loss at step 0: 22.018156
  Training accuracy: 6.6%
  Validation accuracy: 8.6%
  Loss at step 100: 2.022280
  Training accuracy: 75.2%
  Validation accuracy: 73.7%
  Loss at step 200: 1.623059
  Training accuracy: 78.1%
  Validation accuracy: 75.7%
  Loss at step 300: 1.408173
  Training accuracy: 79.2%
  Validation accuracy: 76.5%
  Loss at step 400: 1.262911
  Training accuracy: 80.0%
  Validation accuracy: 76.6%
  Loss at step 500: 1.154954
  Training accuracy: 80.7%
  Validation accuracy: 76.8%
  Loss at step 600: 1.070023
  Training accuracy: 81.2%
  Validation accuracy: 77.0%
  Loss at step 700: 1.000842
  Training accuracy: 81.7%
  Validation accuracy: 77.1%
  Loss at step 800: 0.943042
  Training accuracy: 82.2%
  Validation accuracy: 77.2%
  Test accuracy: 67.9%
#+END_EXAMPLE

Let's now switch to stochastic gradient descent training instead, which
is much faster.

The graph will be similar, except that instead of holding all the
training data into a constant node, we create a =Placeholder= node which
will be fed actual data at every call of =session.run()=.

#+BEGIN_SRC python
  batch_size = 128
  graph = tf.Graph()
  with graph.as_default():

    # Input data. For the training data, we use a placeholder that will be fed
    # at run time with a training minibatch.
    tf_train_dataset = tf.placeholder(tf.float32,
                                      shape=(batch_size, image_size * image_size))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)

    # Variables.
    weights = tf.Variable(
      tf.truncated_normal([image_size * image_size, num_labels]))
    biases = tf.Variable(tf.zeros([num_labels]))

    # Training computation.
    logits = tf.matmul(tf_train_dataset, weights) + biases
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))

    # Optimizer.
    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(
      tf.matmul(tf_valid_dataset, weights) + biases)
    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)
#+END_SRC

Let's run it:

#+BEGIN_SRC python
  num_steps = 3001

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print("Initialized")
    for step in range(num_steps):
      # Pick an offset within the training data, which has been randomized.
      # Note: we could use better randomization across epochs.
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      # Generate a minibatch.
      batch_data = train_dataset[offset:(offset + batch_size), :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      # Prepare a dictionary telling the session where to feed the minibatch.
      # The key of the dictionary is the placeholder node of the graph to be fed,
      # and the value is the numpy array to feed to it.
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
      _, l, predictions = session.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 500 == 0):
        print("Minibatch loss at step %d: %f" % (step, l))
        print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
        print("Validation accuracy: %.1f%%" % accuracy(
          valid_prediction.eval(), valid_labels))
    print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+BEGIN_EXAMPLE
     Initialized
     Minibatch loss at step 0: 16.320572
     Minibatch accuracy: 13.3%
     Validation accuracy: 14.4%
     Minibatch loss at step 500: 1.573464
     Minibatch accuracy: 73.4%
     Validation accuracy: 78.2%
     Minibatch loss at step 1000: 0.979085
     Minibatch accuracy: 81.2%
     Validation accuracy: 79.5%
     Minibatch loss at step 1500: 0.816317
     Minibatch accuracy: 81.2%
     Validation accuracy: 79.8%
     Minibatch loss at step 2000: 1.051737
     Minibatch accuracy: 79.7%
     Validation accuracy: 79.6%
     Minibatch loss at step 2500: 0.977313
     Minibatch accuracy: 76.6%
     Validation accuracy: 80.3%
     Minibatch loss at step 3000: 0.891709
     Minibatch accuracy: 76.6%
     Validation accuracy: 80.3%
     Test accuracy: 69.5%
#+END_EXAMPLE



** Problem 1
:PROPERTIES:
:CUSTOM_ID: problem
:END:

Turn the logistic regression example with SGD into a 1-hidden layer
neural network with rectified linear units
[[https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu][nn.relu()]]
and 1024 hidden nodes. This model should improve your validation / test
accuracy.



#+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)

       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     num_steps = 3001
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
         # Generate a minibatch.
         batch_data = train_dataset[offset:(offset + batch_size), :]
         batch_labels = train_labels[offset:(offset + batch_size), :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Initialized
     Minibatch loss at step 0: 413.522797
     Minibatch accuracy: 9.4%
     Validation accuracy: 37.4%
     Minibatch loss at step 500: 17.026733
     Minibatch accuracy: 79.7%
     Validation accuracy: 82.2%
     Minibatch loss at step 1000: 3.879690
     Minibatch accuracy: 82.0%
     Validation accuracy: 83.4%
     Minibatch loss at step 1500: 7.037672
     Minibatch accuracy: 85.2%
     Validation accuracy: 82.5%
     Minibatch loss at step 2000: 2.705339
     Minibatch accuracy: 84.4%
     Validation accuracy: 83.2%
     Minibatch loss at step 2500: 3.578274
     Minibatch accuracy: 75.8%
     Validation accuracy: 84.1%
     Minibatch loss at step 3000: 5.462776
     Minibatch accuracy: 77.3%
     Validation accuracy: 83.7%
     Test accuracy: 71.8%
 #+END_EXAMPLE


** Problem 2
:PROPERTIES:
:CUSTOM_ID: redo-previous-proper-sgd-randomize-order
:END:

#+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = map(lambda x: x[0], apx_eql_lst)
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 6001
     # ix list for actual SGD
     def fisher_yates_sampling(iterable):
       "l - random selection from permutations(iterable)"
       l = copy.deepcopy(iterable)
       n = len(l)
       for i in range(n-1):
         j = random.randrange(n-i)
         t = l[i]
         l[i] = l[i+j]
         l[i+j] = l[i]
       return l
     rand_train_ix = fisher_yates_sampling(good_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)

       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = fisher_yates_sampling(good_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Minibatch loss at step 3000: 1.604837
     Minibatch accuracy: 91.4%
     Validation accuracy: 84.7%
     Test accuracy: 72.5%
     Minibatch loss at step 2500: 3.163114
     Minibatch accuracy: 91.4%
     Validation accuracy: 83.7%
     Minibatch loss at step 2000: 7.948224
     Minibatch accuracy: 81.2%
     Validation accuracy: 83.3%
     Minibatch loss at step 1500: 2.736376
     Minibatch accuracy: 89.8%
     Validation accuracy: 82.7%
     Minibatch loss at step 1000: 11.304239
     Minibatch accuracy: 84.4%
     Validation accuracy: 82.5%
     Minibatch loss at step 500: 17.010756
     Minibatch accuracy: 85.9%
     Validation accuracy: 82.1%
     Initialized
     Minibatch loss at step 0: 339.549652
     Minibatch accuracy: 7.8%
     Validation accuracy: 26.5%
 #+END_EXAMPLE



* Assignment 3
:PROPERTIES:
:CUSTOM_ID: assignment-3
:header-args: :session a3py
:END:

 Previously in =2_fullyconnected.ipynb=, you trained a logistic
 regression and a neural network model.

 The goal of this assignment is to explore regularization techniques.

 #+BEGIN_SRC python
     # These are all the modules we'll be using later. Make sure you can import them
     # before proceeding further.
     from __future__ import print_function
     import numpy as np
     import tensorflow as tf
     from six.moves import cPickle as pickle
 #+END_SRC

 #+RESULTS:

 First reload the data we generated in /notmist.ipynb/.

 #+BEGIN_SRC python
   pickle_file = 'notMNIST.pickle'

   with open(pickle_file, 'rb') as f:
     save = pickle.load(f)
     train_dataset = save['train_dataset']
     train_labels = save['train_labels']
     valid_dataset = save['valid_dataset']
     valid_labels = save['valid_labels']
     test_dataset = save['test_dataset']
     test_labels = save['test_labels']
     del save  # hint to help gc free up memory
     print('Training set', train_dataset.shape, train_labels.shape)
     print('Validation set', valid_dataset.shape, valid_labels.shape)
     print('Test set', test_dataset.shape, test_labels.shape)
 #+END_SRC

 #+BEGIN_EXAMPLE
   Training set (200000, 28, 28) (200000,)
       Validation set (10000, 28, 28) (10000,)
       Test set (10000, 28, 28) (10000,)
 #+END_EXAMPLE

 Reformat into a shape that's more adapted to the models we're going to
 train:

 -  data as a flat matrix,
 -  labels as float 1-hot encodings.

 #+BEGIN_SRC python
   image_size = 28
   num_labels = 10

   def reformat(dataset, labels):
     dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
     # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]
     labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
     return dataset, labels
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
   train_dataset, train_labels = reformat(train_dataset, train_labels)
   valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
   test_dataset, test_labels = reformat(test_dataset, test_labels)
   print('Training set', train_dataset.shape, train_labels.shape)
   print('Validation set', valid_dataset.shape, valid_labels.shape)
   print('Test set', test_dataset.shape, test_labels.shape)
 #+END_SRC

 #+RESULTS:

 #+BEGIN_EXAMPLE
     Training set (200000, 784) (200000, 10)
     Validation set (10000, 784) (10000, 10)
     Test set (10000, 784) (10000, 10)
 #+END_EXAMPLE

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+RESULTS:



** Problem 1
    :PROPERTIES:
    :CUSTOM_ID: problem-1
    :END:

 Introduce and tune L2 regularization for both logistic and neural
 network models. Remember that L2 amounts to adding a penalty on the norm
 of the weights to the loss. In TensorFlow, you can compute the L2 loss
 for a tensor =t= using =nn.l2_loss(t)=. The right amount of
 regularization should improve your validation / test accuracy.



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = map(lambda x: x[0], apx_eql_lst)
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 2001
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = random_permutation(good_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024*16
     beta = .01
     keep_prob = 0.5
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)


       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = (tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+
               beta*(tf.nn.l2_loss(weights0)+tf.nn.l2_loss(weights1)))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = random_permutation(good_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Initialized
     Minibatch loss at step 0: 7347.640625
     Minibatch accuracy: 5.5%
     Validation accuracy: 37.0%
     Minibatch loss at step 500: 0.876382
     Minibatch accuracy: 96.1%
     Validation accuracy: 78.7%
     Minibatch loss at step 1000: 0.620260
     Minibatch accuracy: 92.2%
     Validation accuracy: 78.6%
     Minibatch loss at step 1500: 0.658640
     Minibatch accuracy: 88.3%
     Validation accuracy: 78.3%
     Minibatch loss at step 2000: 0.555534
     Minibatch accuracy: 93.8%
     Validation accuracy: 76.1%
     Test accuracy: 66.2%
 #+END_EXAMPLE



** Problem 2
    :PROPERTIES:
    :CUSTOM_ID: problem-2
    :END:

 Let's demonstrate an extreme case of overfitting. Restrict your training
 data to just a few batches. What happens?



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = map(lambda x: x[0], apx_eql_lst)
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
     num_train = 800
     good_train_ix = good_train_ix[:num_train]
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 2001
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = fisher_yates_sampling(good_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024
     beta = .01
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)

       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = (tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+
               beta*(tf.nn.l2_loss(weights0)+tf.nn.l2_loss(weights1)))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = fisher_yates_sampling(good_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Test accuracy: 68.6%
     Validation accuracy: 79.3%
     Minibatch loss at step 2000: 0.490076
     Minibatch accuracy: 96.9%
     Validation accuracy: 80.4%
     Minibatch loss at step 1500: 0.414823
     Minibatch accuracy: 99.2%
     Validation accuracy: 81.4%
     Minibatch loss at step 1000: 0.400443
     Minibatch accuracy: 100.0%
     Validation accuracy: 81.8%
     Minibatch loss at step 500: 0.780712
     Minibatch accuracy: 97.7%
     Validation accuracy: 30.3%
     Initialized
     Minibatch loss at step 0: 6528.056152
     Minibatch accuracy: 13.3%
 #+END_EXAMPLE



** Problem 3
    :PROPERTIES:
    :CUSTOM_ID: problem-3
    :END:

 Introduce Dropout on the hidden layer of the neural network. Remember:
 Dropout should only be introduced during training, not evaluation,
 otherwise your evaluation results would be stochastic as well.
 TensorFlow provides =nn.dropout()= for that, but you have to make sure
 it's only inserted during training.

 What happens to our extreme overfitting case?



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = list(map(lambda x: x[0], apx_eql_lst))
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     train_fraction = .01
     num_train = round(train_fraction*len(good_train_ix))
     actual_train_ix = good_train_ix[:num_train]
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 1001
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = random_permutation(actual_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     keep_prob = .5                  # 0<keep_prob<=1
     num_hidden = 1024*4
     beta = .01
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)
       hidden_drop = tf.nn.dropout(hidden_dataset,keep_prob)*(1/keep_prob)

       # Training computation.
       logits = tf.matmul(hidden_drop, weights1) + biases1
       loss = (tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+
               beta*(tf.nn.l2_loss(weights0)+tf.nn.l2_loss(weights1)))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = random_permutation(actual_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Test accuracy: 69.6%
     Minibatch loss at step 2000: 0.485670
     Minibatch accuracy: 94.5%
     Validation accuracy: 80.7%
     Minibatch loss at step 1500: 0.527620
     Minibatch accuracy: 96.9%
     Validation accuracy: 82.2%
     Minibatch loss at step 1000: 1.110798
     Minibatch accuracy: 95.3%
     Validation accuracy: 83.2%
     Minibatch loss at step 500: 98.461121
     Minibatch accuracy: 99.2%
     Validation accuracy: 82.6%
     Validation accuracy: 35.0%
     Initialized
     Minibatch loss at step 0: 14592.014648
     Minibatch accuracy: 12.5%
 #+END_EXAMPLE



** Problem 4
    :PROPERTIES:
    :CUSTOM_ID: problem-4
    :END:

 Try to get the best performance you can using a multi-layer model! The
 best reported test accuracy using a deep network is
 [[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595][97.1%]].

 One avenue you can explore is to add multiple layers.

 Another one is to use learning rate decay:

 #+BEGIN_EXAMPLE
     global_step = tf.Variable(0)  # count the number of steps taken.
     learning_rate = tf.train.exponential_decay(0.5, global_step, ...)
     optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)
 #+END_EXAMPLE



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = list(map(lambda x: x[0], apx_eql_lst))
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     train_fraction = 1
     num_train = round(train_fraction*len(good_train_ix))
     actual_train_ix = good_train_ix[:num_train]
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = random_permutation(actual_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     parameters = {
       'num_steps':6501,
       'batch_size':128,
       'keep_prob':.8,                  # 0<keep_prob<=1
       'learning_rate':[
         0.001,          # Base learning rate.
         128,           # Current index into the dataset (multiply by batch size).
         num_train,     # Decay steps.
         0.8           # Decay rate.
         ],
       'beta':.01,   # regularization parameter
       'num_hidden':[2^10,2^10],
       'layer_fcn':[tf.nn.relu,tf.nn.relu] ,
       'num_hidden_layers':2,
       'momentum':.9,
       'opt_fcn':tf.train.MomentumOptimizer # AdamOptimizer,MomentumOptimizer,GradientDescentOptimizer
     }
     assert parameters['num_hidden_layers'] == len(parameters['layer_fcn']) == len(parameters['num_hidden'])
     graph = tf.Graph()
     with graph.as_default():
       batch = tf.Variable(0)
       learning_rate = tf.train.exponential_decay(
         parameters['learning_rate'][0],
         parameters['learning_rate'][1]*batch,
         *parameters['learning_rate'][2:],
         staircase=True)
       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(parameters['batch_size'], image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(parameters['batch_size'], num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables
       weights = [tf.Variable(
         tf.truncated_normal([image_size * image_size, parameters['num_hidden'][0]]))]
       biases = [tf.Variable(tf.zeros([parameters['num_hidden'][0]]))]
       hidden_dataset = tf.nn.dropout(
         parameters['layer_fcn'][0](tf.add(tf.matmul(tf_train_dataset, weights[0]), biases[0])), 
         parameters['keep_prob'])*(1/parameters['keep_prob'])

       for l in range(1,parameters['num_hidden_layers']):
         weights += [tf.Variable(tf.truncated_normal([parameters['num_hidden'][l-1], parameters['num_hidden'][l]]))]
         biases += [tf.Variable(tf.zeros([parameters['num_hidden'][l]]))]
         hidden_dataset = tf.nn.dropout(
           parameters['layer_fcn'][l](tf.add(tf.matmul(hidden_dataset, weights[l]), biases[l])),
           parameters['keep_prob'])*(1/parameters['keep_prob'])


       weights += [tf.Variable(tf.truncated_normal([parameters['num_hidden'][-1], num_labels]))]
       biases += [tf.Variable(tf.zeros([num_labels]))]
     
       # Training computation.
       logits = tf.matmul(hidden_dataset, weights[-1]) + biases[-1]
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
       for i in range(parameters['num_hidden_layers']+1):
         loss += parameters['beta']*tf.nn.l2_loss(weights[i])

       # Optimizer
       # optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
       optimizer = parameters['opt_fcn'](
         learning_rate,
         parameters['momentum']).minimize(loss, global_step=batch)
       # optimizer = parameters['opt_fcn'](
       #   learning_rate=parameters['learning_rate'], 
       #   global_step=parameters['global_step']).minimize(loss)
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = parameters['layer_fcn'][0](tf.matmul(tf_valid_dataset, weights[0]) + 
                                   biases[0])
       test_hidden = parameters['layer_fcn'][0](tf.matmul(tf_test_dataset, weights[0]) + 
                                  biases[0])
       for l in range(1,parameters['num_hidden_layers']):
         valid_hidden = parameters['layer_fcn'][l](tf.matmul(valid_hidden, weights[l]) +
                                     biases[l])
         test_hidden = parameters['layer_fcn'][l](tf.matmul(test_hidden, weights[l]) + 
                                    biases[l])

       valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, weights[-1]) + 
                                        biases[-1])
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights[-1]) + 
                                       biases[-1])
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     test_accuracy = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(parameters['num_steps']):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * parameters['batch_size']) % (len(rand_train_ix) - parameters['batch_size'])
         if offset < last_offset:
           rand_train_ix = random_permutation(actual_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + parameters['batch_size'])], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + parameters['batch_size'])], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       test_accuracy = accuracy(test_prediction.eval(), test_labels)
       print("Test accuracy: %.1f%%" % test_accuracy)
 #+END_SRC

 #+BEGIN_EXAMPLE
     Test accuracy: 25.1%
     Minibatch loss at step 6500: 11.521459
     Minibatch accuracy: 28.9%
     Validation accuracy: 28.7%
     Minibatch loss at step 6000: 12.083858
     Minibatch accuracy: 21.9%
     Validation accuracy: 27.6%
     Minibatch loss at step 5500: 12.607504
     Minibatch accuracy: 21.1%
     Validation accuracy: 26.5%
     Minibatch loss at step 5000: 13.112435
     Minibatch accuracy: 28.1%
     Validation accuracy: 25.1%
     Minibatch loss at step 4500: 13.792360
     Minibatch accuracy: 22.7%
     Validation accuracy: 23.3%
     Minibatch loss at step 4000: 14.609591
     Minibatch accuracy: 18.8%
     Validation accuracy: 22.4%
     Minibatch loss at step 3500: 15.401945
     Minibatch accuracy: 18.8%
     Validation accuracy: 21.5%
     Minibatch loss at step 3000: 16.365429
     Minibatch accuracy: 21.1%
     Validation accuracy: 20.2%
     Minibatch loss at step 2500: 17.581362
     Minibatch accuracy: 17.2%
     Validation accuracy: 19.9%
     Minibatch loss at step 2000: 18.865662
     Minibatch accuracy: 14.8%
     Validation accuracy: 18.9%
     Minibatch loss at step 1500: 20.277245
     Minibatch accuracy: 19.5%
     Validation accuracy: 18.5%
     Minibatch loss at step 1000: 22.221041
     Minibatch accuracy: 25.0%
     Validation accuracy: 17.8%
     Minibatch loss at step 500: 24.418331
     Minibatch accuracy: 11.7%
     Validation accuracy: 17.3%
     Initialized
     Minibatch loss at step 0: 65.115540
     Minibatch accuracy: 11.7%
     Validation accuracy: 8.8%
 #+END_EXAMPLE

 #+BEGIN_SRC python
     with open("results.txt", "a") as myfile:
       myfile.write(str(parameters))
       myfile.write("\n"+str(test_accuracy))
 #+END_SRC


* Assignment 4
:PROPERTIES:
:header-args: :session a4py
:END:

** Starter code

Previously in =2_fullyconnected.ipynb= and =3_regularization.ipynb=, we
trained fully connected networks to classify
[[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST]]
characters.

The goal of this assignment is make the neural network convolutional.

#+BEGIN_SRC python :session a4aspy
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import numpy as np
  import tensorflow as tf
  from six.moves import cPickle as pickle
  from six.moves import range
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output
  pickle_file = 'notMNIST.pickle'

  with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_dataset = save['train_dataset']
    train_labels = save['train_labels']
    valid_dataset = save['valid_dataset']
    valid_labels = save['valid_labels']
    test_dataset = save['test_dataset']
    test_labels = save['test_labels']
    del save  # hint to help gc free up memory
#+END_SRC

#+RESULTS:
    
#+BEGIN_SRC python :session a4aspy :results output
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+RESULTS:
: Training set (200000, 28, 28) (200000,)
: Validation set (10000, 28, 28) (10000,)
: Test set (10000, 28, 28) (10000,)


Reformat into a TensorFlow-friendly shape: - convolutions need the image
data formatted as a cube (width by height by #channels) - labels as
float 1-hot encodings.

#+BEGIN_SRC python :session a4aspy :results output
  image_size = 28
  num_labels = 10
  num_channels = 1 # grayscale

  import numpy as np

  def reformat(dataset, labels):
    dataset = dataset.reshape(
      (-1, image_size, image_size, num_channels)).astype(np.float32)
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels
#+END_SRC

#+RESULTS:
  
#+BEGIN_SRC python :session a4aspy :results output
  train_dataset, train_labels = reformat(train_dataset, train_labels)
  valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
  test_dataset, test_labels = reformat(test_dataset, test_labels)
#+END_SRC

#+RESULTS:
   
#+BEGIN_SRC python :session a4aspy :results output
   print('Training set', train_dataset.shape, train_labels.shape)
   print('Validation set', valid_dataset.shape, valid_labels.shape)
   print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+RESULTS:
: Training set (200000, 28, 28, 1) (200000, 10)
: Validation set (10000, 28, 28, 1) (10000, 10)
: Test set (10000, 28, 28, 1) (10000, 10)


#+BEGIN_SRC python :session a4aspy :results none
  def accuracy(predictions, labels):
    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
            / predictions.shape[0])
#+END_SRC

#+RESULTS:

Let's build a small network with two convolutional layers, followed by
one fully connected layer. Convolutional networks are more expensive
computationally, so we'll limit its depth and number of fully connected
nodes.

#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      shape = hidden.get_shape().as_list()
      reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
      hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(learning_rate=sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    test_prediction = tf.nn.softmax(model(tf_test_dataset))
#+END_SRC

#+RESULTS:

#+NAME: run_graph
#+BEGIN_SRC python :var nsteps = 1001 :var keep_prob = 1 :session a4aspy :results output
  num_steps = nsteps
  lr = 1e-3
  lr_decay = 0.999995
  with tf.Session(graph=graph) as sess:
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(1,num_steps):
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      lr *= lr_decay
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, 
                   sy_learn_rate: lr, sy_keep_prob: keep_prob}
      _, l, predictions = sess.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 50 == 0):
        print('Minibatch loss at step %d: %f' % (step, l))
        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+RESULTS: run_graph
#+begin_example

>>> >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2017-06-15 07:45:35.562943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 1.861184
Minibatch accuracy: 68.8%
Validation accuracy: 41.9%
Minibatch loss at step 100: 5.072113
Minibatch accuracy: 31.2%
Validation accuracy: 51.2%
Minibatch loss at step 150: 4.068310
Minibatch accuracy: 37.5%
Validation accuracy: 57.7%
Minibatch loss at step 200: 2.123676
Minibatch accuracy: 50.0%
Validation accuracy: 57.4%
Minibatch loss at step 250: 1.832294
Minibatch accuracy: 56.2%
Validation accuracy: 61.3%
Minibatch loss at step 300: 0.926931
Minibatch accuracy: 75.0%
Validation accuracy: 67.6%
Minibatch loss at step 350: 0.491697
Minibatch accuracy: 81.2%
Validation accuracy: 68.0%
Minibatch loss at step 400: 0.317087
Minibatch accuracy: 87.5%
Validation accuracy: 70.7%
Minibatch loss at step 450: 0.814103
Minibatch accuracy: 81.2%
Validation accuracy: 72.2%
Minibatch loss at step 500: 1.340266
Minibatch accuracy: 68.8%
Validation accuracy: 73.4%
Minibatch loss at step 550: 0.328524
Minibatch accuracy: 87.5%
Validation accuracy: 73.7%
Minibatch loss at step 600: 0.437495
Minibatch accuracy: 87.5%
Validation accuracy: 74.5%
Minibatch loss at step 650: 0.908238
Minibatch accuracy: 75.0%
Validation accuracy: 75.5%
Minibatch loss at step 700: 0.803429
Minibatch accuracy: 75.0%
Validation accuracy: 75.7%
Minibatch loss at step 750: 1.014526
Minibatch accuracy: 81.2%
Validation accuracy: 76.5%
Minibatch loss at step 800: 0.558631
Minibatch accuracy: 81.2%
Validation accuracy: 77.7%
Minibatch loss at step 850: 0.429926
Minibatch accuracy: 87.5%
Validation accuracy: 78.2%
Minibatch loss at step 900: 0.420720
Minibatch accuracy: 81.2%
Validation accuracy: 78.2%
Minibatch loss at step 950: 0.818006
Minibatch accuracy: 68.8%
Validation accuracy: 79.2%
Minibatch loss at step 1000: 0.752457
Minibatch accuracy: 81.2%
Validation accuracy: 79.0%
Test accuracy: 85.7%
#+end_example



** Problem 1 

The convolutional model above uses convolutions with stride 2 to reduce
the dimensionality. Replace the strides by a max pooling operation
(=nn.max_pool()=) of stride 2 and kernel size 2.

#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      hidden = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      hidden = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = hidden.get_shape().as_list()
      reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
      hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(learning_rate=sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    test_prediction = tf.nn.softmax(model(tf_test_dataset))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
08:09:56.957957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 2.900611
Minibatch accuracy: 68.8%
Validation accuracy: 47.1%
Minibatch loss at step 100: 1.479307
Minibatch accuracy: 56.2%
Validation accuracy: 54.6%
Minibatch loss at step 150: 4.690967
Minibatch accuracy: 31.2%
Validation accuracy: 58.4%
Minibatch loss at step 200: 1.523903
Minibatch accuracy: 62.5%
Validation accuracy: 61.9%
Minibatch loss at step 250: 1.892011
Minibatch accuracy: 50.0%
Validation accuracy: 66.5%
Minibatch loss at step 300: 1.033208
Minibatch accuracy: 75.0%
Validation accuracy: 67.8%
Minibatch loss at step 350: 1.215178
Minibatch accuracy: 75.0%
Validation accuracy: 69.8%
Minibatch loss at step 400: 0.688676
Minibatch accuracy: 62.5%
Validation accuracy: 70.1%
Minibatch loss at step 450: 1.180127
Minibatch accuracy: 75.0%
Validation accuracy: 73.0%
Minibatch loss at step 500: 1.524866
Minibatch accuracy: 68.8%
Validation accuracy: 73.9%
Minibatch loss at step 550: 0.290829
Minibatch accuracy: 87.5%
Validation accuracy: 75.3%
Minibatch loss at step 600: 0.348496
Minibatch accuracy: 87.5%
Validation accuracy: 75.7%
Minibatch loss at step 650: 1.442225
Minibatch accuracy: 68.8%
Validation accuracy: 74.8%
Minibatch loss at step 700: 0.850661
Minibatch accuracy: 68.8%
Validation accuracy: 75.8%
Minibatch loss at step 750: 1.095397
Minibatch accuracy: 62.5%
Validation accuracy: 77.4%
Minibatch loss at step 800: 0.330389
Minibatch accuracy: 87.5%
Validation accuracy: 78.0%
Minibatch loss at step 850: 0.832846
Minibatch accuracy: 81.2%
Validation accuracy: 78.1%
Minibatch loss at step 900: 0.486584
Minibatch accuracy: 87.5%
Validation accuracy: 77.4%
Minibatch loss at step 950: 1.308142
Minibatch accuracy: 68.8%
Validation accuracy: 78.9%
Minibatch loss at step 1000: 0.909435
Minibatch accuracy: 75.0%
Validation accuracy: 79.4%
Minibatch loss at step 1050: 1.192717
Minibatch accuracy: 56.2%
Validation accuracy: 80.1%
Minibatch loss at step 1100: 0.584909
Minibatch accuracy: 81.2%
Validation accuracy: 80.2%
Minibatch loss at step 1150: 0.142735
Minibatch accuracy: 100.0%
Validation accuracy: 79.5%
Minibatch loss at step 1200: 0.088882
Minibatch accuracy: 100.0%
Validation accuracy: 80.1%
Minibatch loss at step 1250: 0.535772
Minibatch accuracy: 81.2%
Validation accuracy: 79.1%
Minibatch loss at step 1300: 0.890280
Minibatch accuracy: 81.2%
Validation accuracy: 81.3%
Minibatch loss at step 1350: 0.338932
Minibatch accuracy: 87.5%
Validation accuracy: 80.3%
Minibatch loss at step 1400: 0.214247
Minibatch accuracy: 93.8%
Validation accuracy: 81.0%
Minibatch loss at step 1450: 1.065812
Minibatch accuracy: 62.5%
Validation accuracy: 81.4%
Minibatch loss at step 1500: 0.647235
Minibatch accuracy: 75.0%
Validation accuracy: 82.0%
Minibatch loss at step 1550: 0.544307
Minibatch accuracy: 87.5%
Validation accuracy: 81.8%
Minibatch loss at step 1600: 0.329639
Minibatch accuracy: 87.5%
Validation accuracy: 81.3%
Minibatch loss at step 1650: 0.874878
Minibatch accuracy: 68.8%
Validation accuracy: 81.2%
Minibatch loss at step 1700: 1.150976
Minibatch accuracy: 62.5%
Validation accuracy: 81.8%
Minibatch loss at step 1750: 0.856514
Minibatch accuracy: 81.2%
Validation accuracy: 81.4%
Minibatch loss at step 1800: 0.306168
Minibatch accuracy: 93.8%
Validation accuracy: 81.8%
Minibatch loss at step 1850: 0.579684
Minibatch accuracy: 81.2%
Validation accuracy: 81.7%
Minibatch loss at step 1900: 0.453013
Minibatch accuracy: 87.5%
Validation accuracy: 82.7%
Minibatch loss at step 1950: 0.422659
Minibatch accuracy: 87.5%
Validation accuracy: 83.0%
Minibatch loss at step 2000: 0.557044
Minibatch accuracy: 75.0%
Validation accuracy: 82.8%
Minibatch loss at step 2050: 0.886053
Minibatch accuracy: 75.0%
Validation accuracy: 83.4%
Minibatch loss at step 2100: 0.506418
Minibatch accuracy: 87.5%
Validation accuracy: 82.9%
Minibatch loss at step 2150: 0.170724
Minibatch accuracy: 100.0%
Validation accuracy: 82.1%
Minibatch loss at step 2200: 0.240701
Minibatch accuracy: 93.8%
Validation accuracy: 83.3%
Minibatch loss at step 2250: 0.123136
Minibatch accuracy: 100.0%
Validation accuracy: 83.7%
Minibatch loss at step 2300: 0.540390
Minibatch accuracy: 81.2%
Validation accuracy: 83.4%
Minibatch loss at step 2350: 0.372324
Minibatch accuracy: 93.8%
Validation accuracy: 82.9%
Minibatch loss at step 2400: 0.453190
Minibatch accuracy: 81.2%
Validation accuracy: 83.9%
Minibatch loss at step 2450: 1.779238
Minibatch accuracy: 62.5%
Validation accuracy: 83.5%
Minibatch loss at step 2500: 0.705045
Minibatch accuracy: 68.8%
Validation accuracy: 84.0%
Minibatch loss at step 2550: 1.292835
Minibatch accuracy: 68.8%
Validation accuracy: 83.6%
Minibatch loss at step 2600: 0.377887
Minibatch accuracy: 87.5%
Validation accuracy: 83.2%
Minibatch loss at step 2650: 1.297781
Minibatch accuracy: 68.8%
Validation accuracy: 83.7%
Minibatch loss at step 2700: 0.512231
Minibatch accuracy: 81.2%
Validation accuracy: 83.9%
Minibatch loss at step 2750: 1.071017
Minibatch accuracy: 81.2%
Validation accuracy: 84.2%
Minibatch loss at step 2800: 0.298933
Minibatch accuracy: 81.2%
Validation accuracy: 84.4%
Minibatch loss at step 2850: 1.001704
Minibatch accuracy: 62.5%
Validation accuracy: 83.7%
Minibatch loss at step 2900: 0.356880
Minibatch accuracy: 93.8%
Validation accuracy: 84.4%
Minibatch loss at step 2950: 0.796455
Minibatch accuracy: 62.5%
Validation accuracy: 84.3%
Minibatch loss at step 3000: 0.895969
Minibatch accuracy: 68.8%
Validation accuracy: 84.5%
Minibatch loss at step 3050: 0.272711
Minibatch accuracy: 93.8%
Validation accuracy: 84.1%
Minibatch loss at step 3100: 0.733070
Minibatch accuracy: 81.2%
Validation accuracy: 84.0%
Minibatch loss at step 3150: 0.692155
Minibatch accuracy: 81.2%
Validation accuracy: 84.1%
Minibatch loss at step 3200: 0.413291
Minibatch accuracy: 87.5%
Validation accuracy: 84.3%
Minibatch loss at step 3250: 0.643606
Minibatch accuracy: 75.0%
Validation accuracy: 84.4%
Minibatch loss at step 3300: 0.573235
Minibatch accuracy: 87.5%
Validation accuracy: 85.2%
Minibatch loss at step 3350: 0.737073
Minibatch accuracy: 68.8%
Validation accuracy: 85.1%
Minibatch loss at step 3400: 0.278904
Minibatch accuracy: 93.8%
Validation accuracy: 84.3%
Minibatch loss at step 3450: 0.902674
Minibatch accuracy: 81.2%
Validation accuracy: 85.0%
Minibatch loss at step 3500: 0.291990
Minibatch accuracy: 87.5%
Validation accuracy: 84.5%
Minibatch loss at step 3550: 0.689245
Minibatch accuracy: 75.0%
Validation accuracy: 85.0%
Minibatch loss at step 3600: 0.756285
Minibatch accuracy: 68.8%
Validation accuracy: 85.4%
Minibatch loss at step 3650: 0.357790
Minibatch accuracy: 87.5%
Validation accuracy: 84.7%
Minibatch loss at step 3700: 1.061131
Minibatch accuracy: 62.5%
Validation accuracy: 84.6%
Minibatch loss at step 3750: 0.835744
Minibatch accuracy: 75.0%
Validation accuracy: 85.7%
Minibatch loss at step 3800: 0.615569
Minibatch accuracy: 87.5%
Validation accuracy: 85.9%
Minibatch loss at step 3850: 0.258741
Minibatch accuracy: 93.8%
Validation accuracy: 85.5%
Minibatch loss at step 3900: 0.446685
Minibatch accuracy: 81.2%
Validation accuracy: 85.8%
Minibatch loss at step 3950: 0.727674
Minibatch accuracy: 68.8%
Validation accuracy: 85.2%
Minibatch loss at step 4000: 0.505325
Minibatch accuracy: 87.5%
Validation accuracy: 85.4%
Minibatch loss at step 4050: 0.447677
Minibatch accuracy: 87.5%
Validation accuracy: 86.2%
Minibatch loss at step 4100: 0.272943
Minibatch accuracy: 93.8%
Validation accuracy: 85.3%
Minibatch loss at step 4150: 0.159460
Minibatch accuracy: 93.8%
Validation accuracy: 85.2%
Minibatch loss at step 4200: 0.271813
Minibatch accuracy: 93.8%
Validation accuracy: 85.3%
Minibatch loss at step 4250: 0.403566
Minibatch accuracy: 87.5%
Validation accuracy: 85.1%
Minibatch loss at step 4300: 0.862058
Minibatch accuracy: 75.0%
Validation accuracy: 85.2%
Minibatch loss at step 4350: 0.403553
Minibatch accuracy: 93.8%
Validation accuracy: 86.0%
Minibatch loss at step 4400: 0.576407
Minibatch accuracy: 81.2%
Validation accuracy: 85.5%
Minibatch loss at step 4450: 0.401695
Minibatch accuracy: 87.5%
Validation accuracy: 85.7%
Minibatch loss at step 4500: 0.616162
Minibatch accuracy: 81.2%
Validation accuracy: 83.4%
Minibatch loss at step 4550: 0.346180
Minibatch accuracy: 87.5%
Validation accuracy: 86.1%
Minibatch loss at step 4600: 0.267765
Minibatch accuracy: 93.8%
Validation accuracy: 86.2%
Minibatch loss at step 4650: 0.444128
Minibatch accuracy: 87.5%
Validation accuracy: 85.7%
Minibatch loss at step 4700: 0.482445
Minibatch accuracy: 87.5%
Validation accuracy: 86.0%
Minibatch loss at step 4750: 0.698112
Minibatch accuracy: 68.8%
Validation accuracy: 85.8%
Minibatch loss at step 4800: 0.499284
Minibatch accuracy: 81.2%
Validation accuracy: 85.4%
Minibatch loss at step 4850: 0.811626
Minibatch accuracy: 81.2%
Validation accuracy: 86.4%
Minibatch loss at step 4900: 0.526118
Minibatch accuracy: 75.0%
Validation accuracy: 86.0%
Minibatch loss at step 4950: 0.191419
Minibatch accuracy: 93.8%
Validation accuracy: 85.8%
Minibatch loss at step 5000: 0.389479
Minibatch accuracy: 87.5%
Validation accuracy: 85.9%
Test accuracy: 91.7%
#+end_example


** Problem 2
Try to get the best performance you can using a convolutional net. Look for example at the classic [[http://yann.lecun.com/exdb/lenet/][LeNet5]] architecture, adding Dropout, and/or adding learning rate decay.

*** experiments

**** 3
learning rate_decay + dropout+ regularization
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  SEED = np.random.randint(low=np.iinfo(np.uint32).min, 
                           high=np.iinfo(np.uint32).max, size=1)[0]
  beta = 5e-28
  graph = tf.Graph()

  with graph.as_default():
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    sy_keep_prob = tf.placeholder(tf.float32)
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1, seed = SEED))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1, seed=SEED))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1, seed=SEED))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1, seed=SEED))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, sy_keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.div(tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), sy_keep_prob, seed=SEED),sy_keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    regularizers = (tf.nn.l2_loss(layer1_weights)+ 
                    tf.nn.l2_loss(layer2_weights)+ 
                    tf.nn.l2_loss(layer3_weights)+ 
                    tf.nn.l2_loss(layer4_weights))
    loss += beta*regularizers
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001, keep_prob=.5) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
 2017-06-15 09:39:23.291681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 4.247050
Minibatch accuracy: 37.5%
Validation accuracy: 27.4%
Minibatch loss at step 100: 4.601756
Minibatch accuracy: 18.8%
Validation accuracy: 35.9%
Minibatch loss at step 150: 3.145462
Minibatch accuracy: 12.5%
Validation accuracy: 44.5%
Minibatch loss at step 200: 3.035318
Minibatch accuracy: 18.8%
Validation accuracy: 46.6%
Minibatch loss at step 250: 2.477204
Minibatch accuracy: 31.2%
Validation accuracy: 51.1%
Minibatch loss at step 300: 1.430561
Minibatch accuracy: 43.8%
Validation accuracy: 55.5%
Minibatch loss at step 350: 1.477067
Minibatch accuracy: 56.2%
Validation accuracy: 61.2%
Minibatch loss at step 400: 1.614931
Minibatch accuracy: 43.8%
Validation accuracy: 65.2%
Minibatch loss at step 450: 1.169079
Minibatch accuracy: 62.5%
Validation accuracy: 66.7%
Minibatch loss at step 500: 1.250844
Minibatch accuracy: 62.5%
Validation accuracy: 68.4%
Minibatch loss at step 550: 0.673903
Minibatch accuracy: 68.8%
Validation accuracy: 69.3%
Minibatch loss at step 600: 0.872271
Minibatch accuracy: 75.0%
Validation accuracy: 70.5%
Minibatch loss at step 650: 1.511239
Minibatch accuracy: 43.8%
Validation accuracy: 71.8%
Minibatch loss at step 700: 1.044477
Minibatch accuracy: 50.0%
Validation accuracy: 70.9%
Minibatch loss at step 750: 1.326239
Minibatch accuracy: 56.2%
Validation accuracy: 73.6%
Minibatch loss at step 800: 0.657693
Minibatch accuracy: 81.2%
Validation accuracy: 74.0%
Minibatch loss at step 850: 0.522227
Minibatch accuracy: 87.5%
Validation accuracy: 74.5%
Minibatch loss at step 900: 0.841039
Minibatch accuracy: 81.2%
Validation accuracy: 75.1%
Minibatch loss at step 950: 1.378965
Minibatch accuracy: 56.2%
Validation accuracy: 75.7%
Minibatch loss at step 1000: 0.972474
Minibatch accuracy: 62.5%
Validation accuracy: 75.6%
Minibatch loss at step 1050: 1.275453
Minibatch accuracy: 50.0%
Validation accuracy: 77.0%
Minibatch loss at step 1100: 0.984924
Minibatch accuracy: 81.2%
Validation accuracy: 77.4%
Minibatch loss at step 1150: 0.632606
Minibatch accuracy: 81.2%
Validation accuracy: 78.3%
Minibatch loss at step 1200: 0.348040
Minibatch accuracy: 87.5%
Validation accuracy: 76.6%
Minibatch loss at step 1250: 1.205865
Minibatch accuracy: 75.0%
Validation accuracy: 77.1%
Minibatch loss at step 1300: 1.500824
Minibatch accuracy: 62.5%
Validation accuracy: 78.6%
Minibatch loss at step 1350: 0.564847
Minibatch accuracy: 87.5%
Validation accuracy: 78.8%
Minibatch loss at step 1400: 1.520104
Minibatch accuracy: 75.0%
Validation accuracy: 78.6%
Minibatch loss at step 1450: 1.583120
Minibatch accuracy: 68.8%
Validation accuracy: 78.7%
Minibatch loss at step 1500: 0.723130
Minibatch accuracy: 75.0%
Validation accuracy: 79.1%
Minibatch loss at step 1550: 1.007327
Minibatch accuracy: 75.0%
Validation accuracy: 78.4%
Minibatch loss at step 1600: 0.972170
Minibatch accuracy: 62.5%
Validation accuracy: 78.7%
Minibatch loss at step 1650: 1.221925
Minibatch accuracy: 43.8%
Validation accuracy: 78.9%
Minibatch loss at step 1700: 1.277078
Minibatch accuracy: 43.8%
Validation accuracy: 77.3%
Minibatch loss at step 1750: 1.305301
Minibatch accuracy: 81.2%
Validation accuracy: 79.1%
Minibatch loss at step 1800: 1.140792
Minibatch accuracy: 68.8%
Validation accuracy: 80.2%
Minibatch loss at step 1850: 0.916527
Minibatch accuracy: 68.8%
Validation accuracy: 79.4%
Minibatch loss at step 1900: 0.319315
Minibatch accuracy: 93.8%
Validation accuracy: 80.2%
Minibatch loss at step 1950: 0.566165
Minibatch accuracy: 75.0%
Validation accuracy: 80.6%
Minibatch loss at step 2000: 0.734313
Minibatch accuracy: 75.0%
Validation accuracy: 80.3%
Minibatch loss at step 2050: 0.861633
Minibatch accuracy: 75.0%
Validation accuracy: 79.8%
Minibatch loss at step 2100: 0.872185
Minibatch accuracy: 75.0%
Validation accuracy: 81.3%
Minibatch loss at step 2150: 0.436945
Minibatch accuracy: 81.2%
Validation accuracy: 80.5%
Minibatch loss at step 2200: 0.475010
Minibatch accuracy: 81.2%
Validation accuracy: 80.8%
Minibatch loss at step 2250: 0.455218
Minibatch accuracy: 81.2%
Validation accuracy: 81.5%
Minibatch loss at step 2300: 0.766944
Minibatch accuracy: 75.0%
Validation accuracy: 81.7%
Minibatch loss at step 2350: 0.863824
Minibatch accuracy: 68.8%
Validation accuracy: 81.7%
Minibatch loss at step 2400: 0.604405
Minibatch accuracy: 81.2%
Validation accuracy: 81.2%
Minibatch loss at step 2450: 1.457657
Minibatch accuracy: 50.0%
Validation accuracy: 79.8%
Minibatch loss at step 2500: 1.104749
Minibatch accuracy: 62.5%
Validation accuracy: 81.8%
Minibatch loss at step 2550: 1.174053
Minibatch accuracy: 68.8%
Validation accuracy: 82.1%
Minibatch loss at step 2600: 0.624880
Minibatch accuracy: 75.0%
Validation accuracy: 81.6%
Minibatch loss at step 2650: 1.463562
Minibatch accuracy: 56.2%
Validation accuracy: 80.8%
Minibatch loss at step 2700: 0.774168
Minibatch accuracy: 68.8%
Validation accuracy: 82.5%
Minibatch loss at step 2750: 1.511965
Minibatch accuracy: 56.2%
Validation accuracy: 82.4%
Minibatch loss at step 2800: 0.246308
Minibatch accuracy: 93.8%
Validation accuracy: 82.2%
Minibatch loss at step 2850: 1.437925
Minibatch accuracy: 68.8%
Validation accuracy: 81.4%
Minibatch loss at step 2900: 1.087213
Minibatch accuracy: 62.5%
Validation accuracy: 82.3%
Minibatch loss at step 2950: 1.135118
Minibatch accuracy: 75.0%
Validation accuracy: 82.2%
Minibatch loss at step 3000: 0.933039
Minibatch accuracy: 75.0%
Validation accuracy: 83.0%
Minibatch loss at step 3050: 0.496922
Minibatch accuracy: 87.5%
Validation accuracy: 81.6%
Minibatch loss at step 3100: 0.832652
Minibatch accuracy: 75.0%
Validation accuracy: 82.0%
Minibatch loss at step 3150: 0.647313
Minibatch accuracy: 75.0%
Validation accuracy: 82.7%
Minibatch loss at step 3200: 0.759036
Minibatch accuracy: 75.0%
Validation accuracy: 83.3%
Minibatch loss at step 3250: 0.980326
Minibatch accuracy: 68.8%
Validation accuracy: 81.7%
Minibatch loss at step 3300: 0.746954
Minibatch accuracy: 62.5%
Validation accuracy: 82.2%
Minibatch loss at step 3350: 0.861651
Minibatch accuracy: 81.2%
Validation accuracy: 82.4%
Minibatch loss at step 3400: 0.470646
Minibatch accuracy: 75.0%
Validation accuracy: 81.9%
Minibatch loss at step 3450: 1.103467
Minibatch accuracy: 56.2%
Validation accuracy: 82.9%
Minibatch loss at step 3500: 0.320315
Minibatch accuracy: 87.5%
Validation accuracy: 82.1%
Minibatch loss at step 3550: 1.318188
Minibatch accuracy: 62.5%
Validation accuracy: 81.7%
Minibatch loss at step 3600: 0.733523
Minibatch accuracy: 62.5%
Validation accuracy: 82.2%
Minibatch loss at step 3650: 0.467246
Minibatch accuracy: 81.2%
Validation accuracy: 82.5%
Minibatch loss at step 3700: 0.993713
Minibatch accuracy: 62.5%
Validation accuracy: 80.3%
Minibatch loss at step 3750: 0.973567
Minibatch accuracy: 87.5%
Validation accuracy: 83.3%
Minibatch loss at step 3800: 0.563805
Minibatch accuracy: 81.2%
Validation accuracy: 83.7%
Minibatch loss at step 3850: 0.605255
Minibatch accuracy: 81.2%
Validation accuracy: 83.3%
Minibatch loss at step 3900: 0.619934
Minibatch accuracy: 81.2%
Validation accuracy: 83.0%
Minibatch loss at step 3950: 1.044772
Minibatch accuracy: 68.8%
Validation accuracy: 83.5%
Minibatch loss at step 4000: 0.891935
Minibatch accuracy: 56.2%
Validation accuracy: 83.8%
Minibatch loss at step 4050: 0.806958
Minibatch accuracy: 81.2%
Validation accuracy: 83.7%
Minibatch loss at step 4100: 0.429208
Minibatch accuracy: 81.2%
Validation accuracy: 83.4%
Minibatch loss at step 4150: 0.307217
Minibatch accuracy: 87.5%
Validation accuracy: 84.3%
Minibatch loss at step 4200: 0.383367
Minibatch accuracy: 93.8%
Validation accuracy: 83.3%
Minibatch loss at step 4250: 0.518670
Minibatch accuracy: 81.2%
Validation accuracy: 83.5%
Minibatch loss at step 4300: 1.212439
Minibatch accuracy: 81.2%
Validation accuracy: 84.3%
Minibatch loss at step 4350: 0.709060
Minibatch accuracy: 75.0%
Validation accuracy: 84.6%
Minibatch loss at step 4400: 0.325387
Minibatch accuracy: 93.8%
Validation accuracy: 84.6%
Minibatch loss at step 4450: 0.746855
Minibatch accuracy: 75.0%
Validation accuracy: 83.1%
Minibatch loss at step 4500: 0.570571
Minibatch accuracy: 81.2%
Validation accuracy: 84.2%
Minibatch loss at step 4550: 0.623103
Minibatch accuracy: 75.0%
Validation accuracy: 84.4%
Minibatch loss at step 4600: 0.710920
Minibatch accuracy: 68.8%
Validation accuracy: 84.2%
Minibatch loss at step 4650: 0.657528
Minibatch accuracy: 87.5%
Validation accuracy: 84.4%
Minibatch loss at step 4700: 0.913127
Minibatch accuracy: 62.5%
Validation accuracy: 84.0%
Minibatch loss at step 4750: 0.738293
Minibatch accuracy: 81.2%
Validation accuracy: 84.0%
Minibatch loss at step 4800: 0.820862
Minibatch accuracy: 81.2%
Validation accuracy: 84.7%
Minibatch loss at step 4850: 0.734651
Minibatch accuracy: 68.8%
Validation accuracy: 84.7%
Minibatch loss at step 4900: 1.284054
Minibatch accuracy: 68.8%
Validation accuracy: 84.6%
Minibatch loss at step 4950: 0.473267
Minibatch accuracy: 81.2%
Validation accuracy: 84.9%
Minibatch loss at step 5000: 0.622454
Minibatch accuracy: 68.8%
Validation accuracy: 84.5%
Test accuracy: 90.4%
#+end_example


**** 2

learning rate_decay + dropout 1
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    sy_keep_prob = tf.placeholder(tf.float32)
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, sy_keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.div(tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), sy_keep_prob),sy_keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001, keep_prob=.5) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
 2017-06-15 09:31:31.720257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 1.868340
Minibatch accuracy: 25.0%
Validation accuracy: 19.3%
Minibatch loss at step 100: 2.491972
Minibatch accuracy: 25.0%
Validation accuracy: 16.3%
Minibatch loss at step 150: 2.421418
Minibatch accuracy: 18.8%
Validation accuracy: 15.0%
Minibatch loss at step 200: 2.552014
Minibatch accuracy: 12.5%
Validation accuracy: 15.9%
Minibatch loss at step 250: 2.219394
Minibatch accuracy: 25.0%
Validation accuracy: 18.4%
Minibatch loss at step 300: 2.048537
Minibatch accuracy: 12.5%
Validation accuracy: 20.2%
Minibatch loss at step 350: 2.132483
Minibatch accuracy: 25.0%
Validation accuracy: 24.1%
Minibatch loss at step 400: 2.160150
Minibatch accuracy: 12.5%
Validation accuracy: 24.9%
Minibatch loss at step 450: 3.029988
Minibatch accuracy: 12.5%
Validation accuracy: 23.0%
Minibatch loss at step 500: 2.291509
Minibatch accuracy: 18.8%
Validation accuracy: 25.3%
Minibatch loss at step 550: 2.146008
Minibatch accuracy: 18.8%
Validation accuracy: 24.2%
Minibatch loss at step 600: 2.225887
Minibatch accuracy: 12.5%
Validation accuracy: 26.8%
Minibatch loss at step 650: 2.287751
Minibatch accuracy: 6.2%
Validation accuracy: 24.4%
Minibatch loss at step 700: 2.191564
Minibatch accuracy: 6.2%
Validation accuracy: 25.7%
Minibatch loss at step 750: 2.458062
Minibatch accuracy: 25.0%
Validation accuracy: 32.0%
Minibatch loss at step 800: 1.455145
Minibatch accuracy: 56.2%
Validation accuracy: 31.2%
Minibatch loss at step 850: 1.523927
Minibatch accuracy: 43.8%
Validation accuracy: 35.2%
Minibatch loss at step 900: 1.448095
Minibatch accuracy: 43.8%
Validation accuracy: 34.5%
Minibatch loss at step 950: 2.200823
Minibatch accuracy: 12.5%
Validation accuracy: 39.3%
Minibatch loss at step 1000: 2.055672
Minibatch accuracy: 12.5%
Validation accuracy: 37.9%
Minibatch loss at step 1050: 1.998952
Minibatch accuracy: 25.0%
Validation accuracy: 37.1%
Minibatch loss at step 1100: 2.184813
Minibatch accuracy: 25.0%
Validation accuracy: 35.5%
Minibatch loss at step 1150: 1.700816
Minibatch accuracy: 50.0%
Validation accuracy: 39.0%
Minibatch loss at step 1200: 1.727975
Minibatch accuracy: 31.2%
Validation accuracy: 41.0%
Minibatch loss at step 1250: 1.804940
Minibatch accuracy: 18.8%
Validation accuracy: 39.0%
Minibatch loss at step 1300: 1.738092
Minibatch accuracy: 37.5%
Validation accuracy: 45.6%
Minibatch loss at step 1350: 1.788531
Minibatch accuracy: 37.5%
Validation accuracy: 40.2%
Minibatch loss at step 1400: 1.873891
Minibatch accuracy: 31.2%
Validation accuracy: 39.4%
Minibatch loss at step 1450: 2.233442
Minibatch accuracy: 12.5%
Validation accuracy: 33.6%
Minibatch loss at step 1500: 1.529142
Minibatch accuracy: 50.0%
Validation accuracy: 41.5%
Minibatch loss at step 1550: 2.086451
Minibatch accuracy: 12.5%
Validation accuracy: 43.5%
Minibatch loss at step 1600: 1.694917
Minibatch accuracy: 43.8%
Validation accuracy: 43.5%
Minibatch loss at step 1650: 2.215186
Minibatch accuracy: 12.5%
Validation accuracy: 42.9%
Minibatch loss at step 1700: 2.049239
Minibatch accuracy: 25.0%
Validation accuracy: 46.1%
Minibatch loss at step 1750: 2.325694
Minibatch accuracy: 31.2%
Validation accuracy: 48.3%
Minibatch loss at step 1800: 1.955550
Minibatch accuracy: 18.8%
Validation accuracy: 44.5%
Minibatch loss at step 1850: 2.122712
Minibatch accuracy: 25.0%
Validation accuracy: 46.0%
Minibatch loss at step 1900: 3.525293
Minibatch accuracy: 56.2%
Validation accuracy: 49.0%
Minibatch loss at step 1950: 1.769775
Minibatch accuracy: 37.5%
Validation accuracy: 46.7%
Minibatch loss at step 2000: 1.658909
Minibatch accuracy: 31.2%
Validation accuracy: 50.0%
Minibatch loss at step 2050: 1.398658
Minibatch accuracy: 43.8%
Validation accuracy: 50.0%
Minibatch loss at step 2100: 1.706336
Minibatch accuracy: 31.2%
Validation accuracy: 54.1%
Minibatch loss at step 2150: 1.517875
Minibatch accuracy: 37.5%
Validation accuracy: 44.2%
Minibatch loss at step 2200: 1.972822
Minibatch accuracy: 25.0%
Validation accuracy: 43.0%
Minibatch loss at step 2250: 1.748646
Minibatch accuracy: 18.8%
Validation accuracy: 49.4%
Minibatch loss at step 2300: 1.821078
Minibatch accuracy: 18.8%
Validation accuracy: 43.1%
Minibatch loss at step 2350: 1.457965
Minibatch accuracy: 56.2%
Validation accuracy: 51.2%
Minibatch loss at step 2400: 1.607498
Minibatch accuracy: 37.5%
Validation accuracy: 54.3%
Minibatch loss at step 2450: 2.106126
Minibatch accuracy: 25.0%
Validation accuracy: 54.3%
Minibatch loss at step 2500: 2.163781
Minibatch accuracy: 12.5%
Validation accuracy: 55.3%
Minibatch loss at step 2550: 1.766758
Minibatch accuracy: 37.5%
Validation accuracy: 57.4%
Minibatch loss at step 2600: 1.667016
Minibatch accuracy: 31.2%
Validation accuracy: 51.7%
Minibatch loss at step 2650: 1.942839
Minibatch accuracy: 18.8%
Validation accuracy: 52.8%
Minibatch loss at step 2700: 1.667981
Minibatch accuracy: 50.0%
Validation accuracy: 52.8%
Minibatch loss at step 2750: 2.108427
Minibatch accuracy: 25.0%
Validation accuracy: 54.7%
Minibatch loss at step 2800: 2.137871
Minibatch accuracy: 12.5%
Validation accuracy: 57.0%
Minibatch loss at step 2850: 1.423964
Minibatch accuracy: 43.8%
Validation accuracy: 55.0%
Minibatch loss at step 2900: 1.178312
Minibatch accuracy: 62.5%
Validation accuracy: 60.3%
Minibatch loss at step 2950: 1.497456
Minibatch accuracy: 43.8%
Validation accuracy: 64.3%
Minibatch loss at step 3000: 1.899452
Minibatch accuracy: 25.0%
Validation accuracy: 65.9%
Minibatch loss at step 3050: 1.557965
Minibatch accuracy: 43.8%
Validation accuracy: 61.8%
Minibatch loss at step 3100: 2.167020
Minibatch accuracy: 43.8%
Validation accuracy: 60.1%
Minibatch loss at step 3150: 1.574410
Minibatch accuracy: 31.2%
Validation accuracy: 65.0%
Minibatch loss at step 3200: 1.563329
Minibatch accuracy: 31.2%
Validation accuracy: 63.9%
Minibatch loss at step 3250: 1.821066
Minibatch accuracy: 31.2%
Validation accuracy: 61.9%
Minibatch loss at step 3300: 1.937753
Minibatch accuracy: 12.5%
Validation accuracy: 65.5%
Minibatch loss at step 3350: 1.285227
Minibatch accuracy: 43.8%
Validation accuracy: 66.4%
Minibatch loss at step 3400: 1.164702
Minibatch accuracy: 50.0%
Validation accuracy: 60.1%
Minibatch loss at step 3450: 1.756230
Minibatch accuracy: 37.5%
Validation accuracy: 67.9%
Minibatch loss at step 3500: 1.453190
Minibatch accuracy: 50.0%
Validation accuracy: 69.2%
Minibatch loss at step 3550: 5.874414
Minibatch accuracy: 25.0%
Validation accuracy: 64.8%
Minibatch loss at step 3600: 1.309912
Minibatch accuracy: 31.2%
Validation accuracy: 63.2%
Minibatch loss at step 3650: 1.374123
Minibatch accuracy: 50.0%
Validation accuracy: 72.3%
Minibatch loss at step 3700: 2.032023
Minibatch accuracy: 12.5%
Validation accuracy: 71.4%
Minibatch loss at step 3750: 1.890442
Minibatch accuracy: 31.2%
Validation accuracy: 72.5%
Minibatch loss at step 3800: 1.468653
Minibatch accuracy: 43.8%
Validation accuracy: 70.6%
Minibatch loss at step 3850: 1.072326
Minibatch accuracy: 68.8%
Validation accuracy: 73.0%
Minibatch loss at step 3900: 1.276820
Minibatch accuracy: 37.5%
Validation accuracy: 73.2%
Minibatch loss at step 3950: 1.458550
Minibatch accuracy: 56.2%
Validation accuracy: 74.3%
Minibatch loss at step 4000: 1.425846
Minibatch accuracy: 50.0%
Validation accuracy: 75.0%
Minibatch loss at step 4050: 1.352317
Minibatch accuracy: 37.5%
Validation accuracy: 73.2%
Minibatch loss at step 4100: 1.841503
Minibatch accuracy: 37.5%
Validation accuracy: 69.4%
Minibatch loss at step 4150: 1.434784
Minibatch accuracy: 37.5%
Validation accuracy: 69.5%
Minibatch loss at step 4200: 1.577219
Minibatch accuracy: 43.8%
Validation accuracy: 73.9%
Minibatch loss at step 4250: 1.199322
Minibatch accuracy: 62.5%
Validation accuracy: 74.3%
Minibatch loss at step 4300: 3.041095
Minibatch accuracy: 31.2%
Validation accuracy: 74.4%
Minibatch loss at step 4350: 1.320103
Minibatch accuracy: 50.0%
Validation accuracy: 74.6%
Minibatch loss at step 4400: 1.233494
Minibatch accuracy: 50.0%
Validation accuracy: 74.9%
Minibatch loss at step 4450: 1.534841
Minibatch accuracy: 31.2%
Validation accuracy: 76.6%
Minibatch loss at step 4500: 1.474233
Minibatch accuracy: 31.2%
Validation accuracy: 74.2%
Minibatch loss at step 4550: 1.027656
Minibatch accuracy: 62.5%
Validation accuracy: 77.2%
Minibatch loss at step 4600: 1.420190
Minibatch accuracy: 43.8%
Validation accuracy: 77.1%
Minibatch loss at step 4650: 0.945686
Minibatch accuracy: 68.8%
Validation accuracy: 75.6%
Minibatch loss at step 4700: 1.448259
Minibatch accuracy: 50.0%
Validation accuracy: 76.8%
Minibatch loss at step 4750: 1.097918
Minibatch accuracy: 62.5%
Validation accuracy: 78.6%
Minibatch loss at step 4800: 1.283625
Minibatch accuracy: 56.2%
Validation accuracy: 76.1%
Minibatch loss at step 4850: 1.620434
Minibatch accuracy: 62.5%
Validation accuracy: 77.8%
Minibatch loss at step 4900: 1.379436
Minibatch accuracy: 50.0%
Validation accuracy: 73.8%
Minibatch loss at step 4950: 0.794228
Minibatch accuracy: 68.8%
Validation accuracy: 76.8%
Minibatch loss at step 5000: 1.118136
Minibatch accuracy: 68.8%
Validation accuracy: 77.3%
Test accuracy: 83.8%
#+end_example


**** 1

only dropout
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    sy_keep_prob = tf.placeholder(tf.float32)
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, sy_keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1],
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.div(tf.nn.dropout(
        tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), 
        sy_keep_prob), sy_keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(learning_rate=sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001, keep_prob=.5) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
 2017-06-15 09:29:31.169213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 11.566952
Minibatch accuracy: 6.2%
Validation accuracy: 21.2%
Minibatch loss at step 100: 3.870863
Minibatch accuracy: 12.5%
Validation accuracy: 19.8%
Minibatch loss at step 150: 2.895633
Minibatch accuracy: 31.2%
Validation accuracy: 18.5%
Minibatch loss at step 200: 2.107081
Minibatch accuracy: 31.2%
Validation accuracy: 18.0%
Minibatch loss at step 250: 1.950364
Minibatch accuracy: 25.0%
Validation accuracy: 20.6%
Minibatch loss at step 300: 1.999945
Minibatch accuracy: 31.2%
Validation accuracy: 20.1%
Minibatch loss at step 350: 2.289965
Minibatch accuracy: 6.2%
Validation accuracy: 24.4%
Minibatch loss at step 400: 2.300172
Minibatch accuracy: 12.5%
Validation accuracy: 25.6%
Minibatch loss at step 450: 3.869735
Minibatch accuracy: 12.5%
Validation accuracy: 26.2%
Minibatch loss at step 500: 2.144319
Minibatch accuracy: 18.8%
Validation accuracy: 29.3%
Minibatch loss at step 550: 1.584909
Minibatch accuracy: 43.8%
Validation accuracy: 29.3%
Minibatch loss at step 600: 2.151093
Minibatch accuracy: 12.5%
Validation accuracy: 27.3%
Minibatch loss at step 650: 2.215356
Minibatch accuracy: 18.8%
Validation accuracy: 29.4%
Minibatch loss at step 700: 2.065422
Minibatch accuracy: 31.2%
Validation accuracy: 28.2%
Minibatch loss at step 750: 2.016208
Minibatch accuracy: 43.8%
Validation accuracy: 30.5%
Minibatch loss at step 800: 1.894511
Minibatch accuracy: 31.2%
Validation accuracy: 32.8%
Minibatch loss at step 850: 1.741181
Minibatch accuracy: 31.2%
Validation accuracy: 33.9%
Minibatch loss at step 900: 1.899962
Minibatch accuracy: 37.5%
Validation accuracy: 36.7%
Minibatch loss at step 950: 1.929227
Minibatch accuracy: 37.5%
Validation accuracy: 33.0%
Minibatch loss at step 1000: 1.671435
Minibatch accuracy: 37.5%
Validation accuracy: 36.7%
Minibatch loss at step 1050: 2.124148
Minibatch accuracy: 25.0%
Validation accuracy: 37.7%
Minibatch loss at step 1100: 1.898282
Minibatch accuracy: 18.8%
Validation accuracy: 37.7%
Minibatch loss at step 1150: 1.952750
Minibatch accuracy: 31.2%
Validation accuracy: 36.0%
Minibatch loss at step 1200: 1.512580
Minibatch accuracy: 50.0%
Validation accuracy: 43.4%
Minibatch loss at step 1250: 1.629997
Minibatch accuracy: 43.8%
Validation accuracy: 40.1%
Minibatch loss at step 1300: 1.712072
Minibatch accuracy: 62.5%
Validation accuracy: 46.1%
Minibatch loss at step 1350: 2.116345
Minibatch accuracy: 50.0%
Validation accuracy: 40.9%
Minibatch loss at step 1400: 1.630803
Minibatch accuracy: 43.8%
Validation accuracy: 39.9%
Minibatch loss at step 1450: 1.668554
Minibatch accuracy: 43.8%
Validation accuracy: 43.1%
Minibatch loss at step 1500: 1.650850
Minibatch accuracy: 31.2%
Validation accuracy: 44.3%
Minibatch loss at step 1550: 1.749657
Minibatch accuracy: 31.2%
Validation accuracy: 48.2%
Minibatch loss at step 1600: 1.506760
Minibatch accuracy: 37.5%
Validation accuracy: 47.2%
Minibatch loss at step 1650: 1.655661
Minibatch accuracy: 37.5%
Validation accuracy: 47.4%
Minibatch loss at step 1700: 2.183458
Minibatch accuracy: 12.5%
Validation accuracy: 43.5%
Minibatch loss at step 1750: 1.683201
Minibatch accuracy: 37.5%
Validation accuracy: 46.4%
Minibatch loss at step 1800: 1.963095
Minibatch accuracy: 12.5%
Validation accuracy: 47.1%
Minibatch loss at step 1850: 1.611518
Minibatch accuracy: 31.2%
Validation accuracy: 39.4%
Minibatch loss at step 1900: 1.658273
Minibatch accuracy: 50.0%
Validation accuracy: 48.3%
Minibatch loss at step 1950: 1.590845
Minibatch accuracy: 25.0%
Validation accuracy: 47.9%
Minibatch loss at step 2000: 2.187780
Minibatch accuracy: 18.8%
Validation accuracy: 52.0%
Minibatch loss at step 2050: 1.394493
Minibatch accuracy: 43.8%
Validation accuracy: 50.4%
Minibatch loss at step 2100: 2.159330
Minibatch accuracy: 18.8%
Validation accuracy: 49.3%
Minibatch loss at step 2150: 1.423224
Minibatch accuracy: 37.5%
Validation accuracy: 50.3%
Minibatch loss at step 2200: 1.767830
Minibatch accuracy: 31.2%
Validation accuracy: 51.1%
Minibatch loss at step 2250: 1.687594
Minibatch accuracy: 37.5%
Validation accuracy: 52.3%
Minibatch loss at step 2300: 1.296938
Minibatch accuracy: 50.0%
Validation accuracy: 51.9%
Minibatch loss at step 2350: 2.348637
Minibatch accuracy: 18.8%
Validation accuracy: 53.7%
Minibatch loss at step 2400: 1.952907
Minibatch accuracy: 18.8%
Validation accuracy: 57.0%
Minibatch loss at step 2450: 2.153319
Minibatch accuracy: 37.5%
Validation accuracy: 56.1%
Minibatch loss at step 2500: 2.116488
Minibatch accuracy: 25.0%
Validation accuracy: 60.0%
Minibatch loss at step 2550: 1.489463
Minibatch accuracy: 50.0%
Validation accuracy: 60.7%
Minibatch loss at step 2600: 1.149191
Minibatch accuracy: 43.8%
Validation accuracy: 56.6%
Minibatch loss at step 2650: 2.170089
Minibatch accuracy: 25.0%
Validation accuracy: 63.5%
Minibatch loss at step 2700: 1.525531
Minibatch accuracy: 37.5%
Validation accuracy: 59.3%
Minibatch loss at step 2750: 1.509228
Minibatch accuracy: 37.5%
Validation accuracy: 61.9%
Minibatch loss at step 2800: 1.147282
Minibatch accuracy: 56.2%
Validation accuracy: 62.5%
Minibatch loss at step 2850: 1.438620
Minibatch accuracy: 50.0%
Validation accuracy: 63.5%
Minibatch loss at step 2900: 1.439854
Minibatch accuracy: 43.8%
Validation accuracy: 64.3%
Minibatch loss at step 2950: 1.930614
Minibatch accuracy: 25.0%
Validation accuracy: 62.6%
Minibatch loss at step 3000: 1.589028
Minibatch accuracy: 43.8%
Validation accuracy: 62.4%
Minibatch loss at step 3050: 1.181139
Minibatch accuracy: 50.0%
Validation accuracy: 59.9%
Minibatch loss at step 3100: 1.706787
Minibatch accuracy: 50.0%
Validation accuracy: 64.2%
Minibatch loss at step 3150: 1.876871
Minibatch accuracy: 56.2%
Validation accuracy: 68.5%
Minibatch loss at step 3200: 1.360961
Minibatch accuracy: 37.5%
Validation accuracy: 69.2%
Minibatch loss at step 3250: 2.285524
Minibatch accuracy: 50.0%
Validation accuracy: 70.5%
Minibatch loss at step 3300: 1.441857
Minibatch accuracy: 50.0%
Validation accuracy: 70.5%
Minibatch loss at step 3350: 1.000102
Minibatch accuracy: 68.8%
Validation accuracy: 69.4%
Minibatch loss at step 3400: 0.899509
Minibatch accuracy: 68.8%
Validation accuracy: 71.0%
Minibatch loss at step 3450: 1.446827
Minibatch accuracy: 50.0%
Validation accuracy: 73.5%
Minibatch loss at step 3500: 1.269377
Minibatch accuracy: 50.0%
Validation accuracy: 67.3%
Minibatch loss at step 3550: 1.662549
Minibatch accuracy: 31.2%
Validation accuracy: 70.9%
Minibatch loss at step 3600: 1.274959
Minibatch accuracy: 56.2%
Validation accuracy: 72.7%
Minibatch loss at step 3650: 0.897081
Minibatch accuracy: 62.5%
Validation accuracy: 74.3%
Minibatch loss at step 3700: 1.394020
Minibatch accuracy: 50.0%
Validation accuracy: 74.4%
Minibatch loss at step 3750: 1.613516
Minibatch accuracy: 50.0%
Validation accuracy: 74.4%
Minibatch loss at step 3800: 0.905103
Minibatch accuracy: 68.8%
Validation accuracy: 72.7%
Minibatch loss at step 3850: 1.036171
Minibatch accuracy: 75.0%
Validation accuracy: 76.2%
Minibatch loss at step 3900: 0.679254
Minibatch accuracy: 81.2%
Validation accuracy: 74.8%
Minibatch loss at step 3950: 1.198567
Minibatch accuracy: 43.8%
Validation accuracy: 76.8%
Minibatch loss at step 4000: 1.513273
Minibatch accuracy: 43.8%
Validation accuracy: 74.3%
Minibatch loss at step 4050: 1.243160
Minibatch accuracy: 56.2%
Validation accuracy: 74.3%
Minibatch loss at step 4100: 0.809120
Minibatch accuracy: 62.5%
Validation accuracy: 75.2%
Minibatch loss at step 4150: 0.775453
Minibatch accuracy: 75.0%
Validation accuracy: 76.8%
Minibatch loss at step 4200: 0.922803
Minibatch accuracy: 75.0%
Validation accuracy: 76.6%
Minibatch loss at step 4250: 0.865568
Minibatch accuracy: 68.8%
Validation accuracy: 76.1%
Minibatch loss at step 4300: 1.376431
Minibatch accuracy: 56.2%
Validation accuracy: 77.6%
Minibatch loss at step 4350: 1.052437
Minibatch accuracy: 56.2%
Validation accuracy: 77.3%
Minibatch loss at step 4400: 0.973835
Minibatch accuracy: 50.0%
Validation accuracy: 78.2%
Minibatch loss at step 4450: 0.995051
Minibatch accuracy: 56.2%
Validation accuracy: 78.1%
Minibatch loss at step 4500: 1.235685
Minibatch accuracy: 50.0%
Validation accuracy: 78.3%
Minibatch loss at step 4550: 1.097849
Minibatch accuracy: 43.8%
Validation accuracy: 78.2%
Minibatch loss at step 4600: 1.154400
Minibatch accuracy: 62.5%
Validation accuracy: 77.8%
Minibatch loss at step 4650: 1.348891
Minibatch accuracy: 50.0%
Validation accuracy: 76.6%
Minibatch loss at step 4700: 0.996796
Minibatch accuracy: 62.5%
Validation accuracy: 79.4%
Minibatch loss at step 4750: 0.857328
Minibatch accuracy: 56.2%
Validation accuracy: 79.3%
Minibatch loss at step 4800: 0.978085
Minibatch accuracy: 62.5%
Validation accuracy: 79.3%
Minibatch loss at step 4850: 1.388877
Minibatch accuracy: 50.0%
Validation accuracy: 78.6%
Minibatch loss at step 4900: 1.505564
Minibatch accuracy: 50.0%
Validation accuracy: 78.1%
Minibatch loss at step 4950: 0.628969
Minibatch accuracy: 75.0%
Validation accuracy: 76.9%
Minibatch loss at step 5000: 0.963439
Minibatch accuracy: 56.2%
Validation accuracy: 79.2%
Test accuracy: 85.4%
#+end_example


j


* Assignment 5
:PROPERTIES:
:header-args: :session a5py :results output
:END:

** starter code
:PROPERTIES:
:ATTACH_DIR_INHERIT: t
:END:

The goal of this assignment is to train a Word2Vec skip-gram model over
[[http://mattmahoney.net/dc/textdata][Text8]] data.

#+BEGIN_SRC python :results none
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  # %matplotlib inline
  from __future__ import print_function
  import collections
  import math
  import numpy as np
  import os
  import random
  import tensorflow as tf
  import zipfile
  from matplotlib import pylab
  from six.moves import range
  from six.moves.urllib.request import urlretrieve
  from sklearn.manifold import TSNE
#+END_SRC

Download the data from the source website if necessary.

#+BEGIN_SRC python :results output
  Url = 'http://mattmahoney.net/dc/'

  def maybe_download(filename, expected_bytes):
    """Download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
      filename, _ = urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified %s' % filename)
    else:
      print(statinfo.st_size)
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  filename = maybe_download('text8.zip', 31344016)
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... >>> Found and verified text8.zip

Read the data into a string.

#+BEGIN_SRC python
     def read_data(filename):
       """Extract the first file enclosed in a zip file as a list of words"""
       with zipfile.ZipFile(filename) as f:
         data = tf.compat.as_str(f.read(f.namelist()[0])).split()
       return data
      
     words = read_data(filename)
     print('Data size %d' % len(words))
#+END_SRC

#+RESULTS:
: 
: ... ... ... ... >>> >>> Data size 17005207

Build the dictionary and replace rare words with UNK token.

#+BEGIN_SRC python
     vocabulary_size = 50000

     def build_dataset(words):
       count = [['UNK', -1]]
       count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
       dictionary = dict()
       for word, _ in count:
         dictionary[word] = len(dictionary)
       data = list()
       unk_count = 0
       for word in words:
         if word in dictionary:
           index = dictionary[word]
         else:
           index = 0  # dictionary['UNK']
           unk_count = unk_count + 1
         data.append(index)
       count[0][1] = unk_count
       reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) 
       return data, count, dictionary, reverse_dictionary

     data, count, dictionary, reverse_dictionary = build_dataset(words)
     print('Most common words (+UNK)', count[:5])
     print('Sample data', data[:10])
     del words  # Hint to reduce memory.
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> >>> Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
: Sample data [5236, 3084, 12, 6, 195, 2, 3134, 46, 59, 156]

Function to generate a training batch for the skip-gram model.

#+BEGIN_SRC python
  data_index = 0

  def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips <= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1 # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
      buffer.append(data[data_index])
      data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
      target = skip_window  # target label at the center of the buffer
      targets_to_avoid = [ skip_window ]
      for j in range(num_skips):
        while target in targets_to_avoid:
          target = random.randint(0, span - 1)
        targets_to_avoid.append(target)
        batch[i * num_skips + j] = buffer[skip_window]
        labels[i * num_skips + j, 0] = buffer[target]
      buffer.append(data[data_index])
      data_index = (data_index + 1) % len(data)
    return batch, labels

  print('data:', [reverse_dictionary[di] for di in data[:8]])

  for num_skips, skip_window in [(2, 1), (4, 2)]:
      data_index = 0
      batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)
      print('\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))
      print('    batch:', [reverse_dictionary[bi] for bi in batch])
      print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])
#+END_SRC

#+RESULTS:
#+begin_example

>>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']
>>> ... ... ... ... ... ... 
with num_skips = 2 and skip_window = 1:
    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']
    labels: ['as', 'anarchism', 'a', 'originated', 'as', 'term', 'of', 'a']

with num_skips = 4 and skip_window = 2:
    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']
    labels: ['term', 'anarchism', 'a', 'originated', 'of', 'as', 'originated', 'term']
#+end_example

Train a skip-gram model.

#+BEGIN_SRC python
  batch_size = 128
  embedding_size = 128 # Dimension of the embedding vector.
  skip_window = 1 # How many words to consider left and right.
  num_skips = 2 # How many times to reuse an input to generate a label.
  # We pick a random validation set to sample nearest neighbors. here we limit the
  # validation samples to the words that have a low numeric ID, which by
  # construction are also the most frequent. 
  valid_size = 16 # Random set of words to evaluate similarity on.
  valid_window = 100 # Only pick dev samples in the head of the distribution.
  valid_examples = np.array(random.sample(range(valid_window), valid_size))
  num_sampled = 64 # Number of negative examples to sample.
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)
    # Variables.
    embeddings = tf.Variable(
      tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
      tf.truncated_normal([vocabulary_size, embedding_size],
                           stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))
    # Model.
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
      tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,
                                 labels=train_labels, num_sampled=num_sampled, 
                                 num_classes=vocabulary_size))
    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities 
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  num_steps = 100001

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    average_loss = 0
    for step in range(num_steps):
      batch_data, batch_labels = generate_batch(
        batch_size, num_skips, skip_window)
      feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
      _, l = session.run([optimizer, loss], feed_dict=feed_dict)
      average_loss += l
      if step % 2000 == 0:
        if step > 0:
          average_loss = average_loss / 2000
        # The average loss is an estimate of the loss over the last 2000 batches.
        print('Average loss at step %d: %f' % (step, average_loss))
        average_loss = 0
      # note that this is expensive (~20% slowdown if computed every 500 steps)
      if step % 10000 == 0:
        sim = similarity.eval()
        for i in range(valid_size):
          valid_word = reverse_dictionary[valid_examples[i]]
          top_k = 8 # number of nearest neighbors
          nearest = (-sim[i, :]).argsort()[1:top_k+1]
          log = 'Nearest to %s:' % valid_word
          for k in range(top_k):
            close_word = reverse_dictionary[nearest[k]]
            log = '%s %s,' % (log, close_word)
          print(log)
    final_embeddings = normalized_embeddings.eval()
#+END_SRC

#+RESULTS:
#+begin_example
2017-06-15 18:08:02.513700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-06-15 18:08:02.516168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.66GiB
2017-06-15 18:08:02.516249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-06-15 18:08:02.516276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-06-15 18:08:02.516305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2017-06-15 18:08:03.777980: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-15 18:08:03.778053: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-15 18:08:03.781478: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x5603ba66c610 executing computations on platform Host. Devices:
2017-06-15 18:08:03.781535: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-06-15 18:08:03.782140: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-15 18:08:03.782195: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-15 18:08:03.784474: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x5603ba680b70 executing computations on platform CUDA. Devices:
2017-06-15 18:08:03.784521: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Quadro M2000M, Compute Capability 5.0
Initialized
Average loss at step 0: 8.152785
Nearest to zero: icrm, greeneville, fastened, bolland, grays, combi, svetlana, incurred,
Nearest to he: sennett, cheat, litt, arcadia, soares, majapahit, postpositions, leonidas,
Nearest to people: infantryman, southside, consumerism, philibert, ranges, bitstream, kauravas, confess,
Nearest to six: wicket, emcee, peas, sie, marini, wf, bewildering, lifeboats,
Nearest to many: jackson, exists, karo, composes, khufu, sucked, backer, passionately,
Nearest to however: blok, px, sinless, ise, emblems, venona, austerity, coughing,
Nearest to that: karabakh, jarrett, betrays, luo, mannheim, grove, reoccupied, swapping,
Nearest to also: imperfections, macduff, originator, stow, hitters, arnold, analog, porpoises,
Nearest to during: outpatient, marcia, improvisation, visions, panspermia, refugee, unstructured, gpp,
Nearest to world: churchyard, fluoxetine, tort, malls, taoiseach, unconvinced, tx, dearly,
Nearest to two: onwards, ordovician, emulator, published, mauritanian, root, musk, gated,
Nearest to some: ddd, unintentional, calorie, recension, jewellery, automorphism, class, digimon,
Nearest to not: digitally, netware, hurdler, parade, scans, popularity, letters, received,
Nearest to most: guayaquil, mencius, hata, hypnos, felt, motorist, designed, additional,
Nearest to between: naghten, highbury, january, weigh, sacrilegious, adf, thing, blasts,
Nearest to i: shuffle, fists, broadcasting, edgard, environmentally, cigarettes, mujibur, soulful,
Average loss at step 2000: 4.360054
Average loss at step 4000: 3.866475
Average loss at step 6000: 3.788148
Average loss at step 8000: 3.689236
Average loss at step 10000: 3.612638
Nearest to zero: nine, eight, seven, six, four, five, three, two,
Nearest to he: it, they, she, there, who, sennett, arcadia, cheat,
Nearest to people: infantryman, countries, reintroducing, tibeto, ranges, southside, influential, fanclub,
Nearest to six: seven, eight, four, three, five, nine, zero, two,
Nearest to many: some, all, backer, composes, several, aman, these, heavier,
Nearest to however: but, px, blok, tavern, venona, although, miscegenation, jogaila,
Nearest to that: which, it, geq, bara, getz, wears, makeshift, scrambled,
Nearest to also: still, schirra, chydenius, ads, bech, which, now, arnold,
Nearest to during: unstructured, biometric, at, in, outpatient, improvisation, marcia, nder,
Nearest to world: taoiseach, eschewed, malls, churchyard, meaningful, certainly, smallville, dilapidated,
Nearest to two: three, four, five, seven, six, nine, eight, one,
Nearest to some: many, these, plate, parsed, crystallized, shaku, jewellery, rupiah,
Nearest to not: it, novi, they, conspiracies, parade, enhancing, dusty, sarcophagus,
Nearest to most: motorist, mencius, uneven, macross, additional, blending, crumbling, designed,
Nearest to between: in, presidency, canned, naghten, worldwide, with, adf, elitism,
Nearest to i: t, personally, broadcasting, taiwan, shower, cranes, notices, webern,
Average loss at step 12000: 3.605409
Average loss at step 14000: 3.572850
Average loss at step 16000: 3.413008
Average loss at step 18000: 3.460469
Average loss at step 20000: 3.538633
Nearest to zero: five, six, four, seven, three, eight, nine, two,
Nearest to he: it, they, she, who, there, which, complementing, lanterns,
Nearest to people: countries, infantryman, ranges, reintroducing, influential, klement, those, shanks,
Nearest to six: five, nine, eight, four, seven, three, zero, two,
Nearest to many: some, several, all, these, other, composes, aman, various,
Nearest to however: but, although, that, miscegenation, jogaila, tavern, again, blok,
Nearest to that: which, but, however, murat, this, mooted, governance, bara,
Nearest to also: now, which, still, not, often, chydenius, never, schirra,
Nearest to during: at, unstructured, in, assay, refugee, subsidy, after, with,
Nearest to world: taoiseach, eschewed, u, cleese, malls, churchyard, speech, meaningful,
Nearest to two: three, five, four, six, zero, one, eight, seven,
Nearest to some: many, these, several, all, their, other, his, jewellery,
Nearest to not: novi, they, also, generally, it, to, still, overlapped,
Nearest to most: escherichia, modern, motorist, analysis, colliery, baroness, macross, uneven,
Nearest to between: with, rogue, presidency, downing, in, canned, cf, within,
Nearest to i: ii, iii, astride, notices, t, shower, cranes, we,
Average loss at step 22000: 3.508200
Average loss at step 24000: 3.490601
Average loss at step 26000: 3.481171
Average loss at step 28000: 3.480548
Average loss at step 30000: 3.503050
Nearest to zero: five, six, four, eight, seven, three, nine, two,
Nearest to he: it, she, they, there, who, lanterns, weaknesses, polygamous,
Nearest to people: countries, klement, laying, reintroducing, those, ranges, infantryman, michelangelo,
Nearest to six: four, eight, five, nine, seven, three, two, zero,
Nearest to many: some, several, these, composes, all, alois, various, values,
Nearest to however: but, although, while, miscegenation, jogaila, that, though, px,
Nearest to that: which, this, but, what, mooted, however, geq, mclaughlin,
Nearest to also: now, still, always, often, which, never, schirra, ads,
Nearest to during: in, after, at, until, before, involve, unstructured, subsidy,
Nearest to world: taoiseach, cleese, u, repealing, eschewed, atmosphere, speech, churchyard,
Nearest to two: four, three, one, six, seven, five, eight, nine,
Nearest to some: many, these, several, all, their, this, various, adaptable,
Nearest to not: they, novi, generally, to, it, tagging, still, gently,
Nearest to most: escherichia, baroness, more, many, analysis, reviewers, modern, motorist,
Nearest to between: with, in, adf, downing, within, presidency, autodesk, from,
Nearest to i: ii, you, iii, t, kamala, we, shower, astride,
Average loss at step 32000: 3.500849
Average loss at step 34000: 3.492524
Average loss at step 36000: 3.458225
Average loss at step 38000: 3.302418
Average loss at step 40000: 3.430410
Nearest to zero: five, seven, nine, eight, six, three, four, two,
Nearest to he: she, it, they, there, who, i, eventually, soon,
Nearest to people: countries, klement, those, actions, epilogue, others, members, reintroducing,
Nearest to six: seven, four, five, eight, nine, three, two, one,
Nearest to many: some, several, these, various, both, all, alois, most,
Nearest to however: but, although, that, which, though, and, while, where,
Nearest to that: which, however, this, ambient, what, but, where, indentured,
Nearest to also: often, still, now, which, never, usually, always, not,
Nearest to during: in, before, after, within, at, heysel, involve, subsidy,
Nearest to world: u, cleese, repealing, speech, lattices, malls, felix, taoiseach,
Nearest to two: three, four, five, six, seven, one, eight, nine,
Nearest to some: many, these, several, this, any, their, the, certain,
Nearest to not: they, never, it, still, generally, also, to, novi,
Nearest to most: more, baroness, extremely, escherichia, many, scouring, forties, annual,
Nearest to between: with, adf, within, wabash, from, sennacherib, maxine, pontifex,
Nearest to i: you, we, t, ii, he, iii, they, amherst,
Average loss at step 42000: 3.433557
Average loss at step 44000: 3.456737
Average loss at step 46000: 3.453549
Average loss at step 48000: 3.351593
Average loss at step 50000: 3.383822
Nearest to zero: seven, four, five, six, eight, nine, three, two,
Nearest to he: she, it, they, there, who, eventually, but, this,
Nearest to people: countries, actions, players, others, members, klement, nestorian, those,
Nearest to six: eight, seven, four, nine, three, five, two, zero,
Nearest to many: some, several, these, both, various, most, electrophilic, other,
Nearest to however: but, although, though, where, that, while, when, and,
Nearest to that: which, however, this, where, but, what, geq, mooted,
Nearest to also: now, which, often, still, always, usually, never, generally,
Nearest to during: in, at, after, within, until, when, including, before,
Nearest to world: cleese, felix, repealing, u, unit, flocked, malls, comprehensive,
Nearest to two: three, four, one, six, five, seven, eight, zero,
Nearest to some: many, several, these, various, both, their, most, the,
Nearest to not: never, generally, still, they, now, novi, dusty, always,
Nearest to most: more, extremely, many, some, less, baroness, uneven, crusading,
Nearest to between: with, within, in, adf, from, gospels, among, sennacherib,
Nearest to i: you, we, ii, t, iii, they, x, ebenezer,
Average loss at step 52000: 3.434259
Average loss at step 54000: 3.428692
Average loss at step 56000: 3.439087
Average loss at step 58000: 3.398797
Average loss at step 60000: 3.392078
Nearest to zero: five, four, eight, six, seven, nine, three, two,
Nearest to he: she, it, they, there, who, soon, eventually, we,
Nearest to people: countries, players, those, others, students, members, actions, men,
Nearest to six: eight, four, five, seven, nine, three, zero, two,
Nearest to many: some, several, these, various, other, all, both, most,
Nearest to however: but, although, though, when, that, which, and, where,
Nearest to that: which, what, this, however, it, mooted, there, indentured,
Nearest to also: now, still, usually, often, never, sometimes, generally, always,
Nearest to during: after, before, in, when, until, including, at, within,
Nearest to world: lattices, u, cleese, felix, malls, churchyard, coolidge, unit,
Nearest to two: three, four, one, six, five, seven, eight, zero,
Nearest to some: many, several, these, all, any, various, most, the,
Nearest to not: they, never, still, usually, technically, generally, you, now,
Nearest to most: more, many, some, baroness, electress, less, extremely, among,
Nearest to between: with, within, among, remaining, from, gospels, in, sennacherib,
Nearest to i: you, we, ii, organizes, t, iii, they, ebenezer,
Average loss at step 62000: 3.246887
Average loss at step 64000: 3.257387
Average loss at step 66000: 3.400697
Average loss at step 68000: 3.395861
Average loss at step 70000: 3.359366
Nearest to zero: five, four, seven, eight, six, nine, three, two,
Nearest to he: she, it, they, there, eventually, who, soon, we,
Nearest to people: countries, players, students, women, others, men, philosophers, members,
Nearest to six: eight, seven, four, five, nine, three, two, zero,
Nearest to many: some, several, these, various, both, all, most, each,
Nearest to however: but, although, though, where, while, that, when, which,
Nearest to that: which, however, what, this, but, where, portillo, mooted,
Nearest to also: now, still, which, often, usually, never, schirra, generally,
Nearest to during: after, before, in, until, throughout, within, at, while,
Nearest to world: u, lattices, churchyard, cleese, coolidge, felix, exact, malls,
Nearest to two: three, six, four, seven, one, five, eight, zero,
Nearest to some: many, several, these, all, various, most, each, any,
Nearest to not: never, still, now, generally, they, technically, usually, normally,
Nearest to most: more, many, some, less, extremely, electress, uneven, baroness,
Nearest to between: within, with, from, among, in, gospels, sennacherib, adf,
Nearest to i: you, we, ii, organizes, ebenezer, they, cranes, tatiana,
Average loss at step 72000: 3.372637
Average loss at step 74000: 3.349423
Average loss at step 76000: 3.315401
Average loss at step 78000: 3.355518
Average loss at step 80000: 3.373379
Nearest to zero: five, seven, eight, six, four, three, nine, two,
Nearest to he: she, it, they, there, who, we, soon, originally,
Nearest to people: students, countries, men, women, players, others, those, members,
Nearest to six: five, seven, eight, four, three, nine, two, zero,
Nearest to many: some, several, these, various, both, all, most, those,
Nearest to however: but, although, though, that, where, while, which, forgery,
Nearest to that: which, however, where, what, this, levitt, hortense, mooted,
Nearest to also: now, still, often, never, which, usually, always, sometimes,
Nearest to during: after, before, in, until, throughout, when, despite, although,
Nearest to world: u, lattices, cleese, felix, aircrew, repealing, malls, speech,
Nearest to two: three, four, six, five, seven, one, eight, zero,
Nearest to some: many, several, these, various, most, certain, all, both,
Nearest to not: still, normally, generally, technically, never, usually, always, they,
Nearest to most: more, some, many, among, less, extremely, especially, electress,
Nearest to between: within, with, among, in, from, rogue, sennacherib, downing,
Nearest to i: you, we, ii, organizes, they, iii, cloak, t,
Average loss at step 82000: 3.410528
Average loss at step 84000: 3.412772
Average loss at step 86000: 3.388072
Average loss at step 88000: 3.350503
Average loss at step 90000: 3.366962
Nearest to zero: seven, five, eight, six, four, nine, three, two,
Nearest to he: she, it, they, there, soon, who, originally, later,
Nearest to people: students, players, men, women, countries, those, members, jews,
Nearest to six: seven, eight, five, four, nine, three, zero, two,
Nearest to many: some, several, these, various, all, most, certain, numerous,
Nearest to however: but, although, though, that, where, which, nevertheless, while,
Nearest to that: which, however, what, mooted, instead, portillo, neurologic, accidentally,
Nearest to also: now, often, still, which, never, generally, always, actually,
Nearest to during: after, before, until, in, although, while, under, throughout,
Nearest to world: actual, lattices, psilocybin, eschewed, churchyard, aircrew, cleese, recreation,
Nearest to two: three, four, five, seven, six, one, eight, nine,
Nearest to some: many, several, these, most, various, certain, any, all,
Nearest to not: normally, still, never, generally, technically, we, also, tagging,
Nearest to most: more, less, many, some, all, among, particularly, nasir,
Nearest to between: with, within, among, from, jamal, ensuing, romano, into,
Nearest to i: you, we, g, iii, ii, frac, they, t,
Average loss at step 92000: 3.398437
Average loss at step 94000: 3.258491
Average loss at step 96000: 3.357522
Average loss at step 98000: 3.241308
Average loss at step 100000: 3.355222
Nearest to zero: five, four, seven, eight, six, nine, three, two,
Nearest to he: she, it, they, there, we, who, soon, never,
Nearest to people: players, students, countries, women, men, children, others, klement,
Nearest to six: seven, eight, four, five, nine, three, two, zero,
Nearest to many: several, some, various, these, numerous, all, few, certain,
Nearest to however: but, although, though, that, especially, where, nevertheless, chicks,
Nearest to that: which, what, however, hawick, actually, hortense, gzip, how,
Nearest to also: now, still, never, often, actually, bonhomme, sometimes, which,
Nearest to during: after, in, before, until, following, although, throughout, at,
Nearest to world: tendon, lattices, actual, cleese, hellenistic, duffy, eschewed, coolidge,
Nearest to two: three, four, five, six, one, eight, seven, zero,
Nearest to some: many, several, these, any, certain, various, all, the,
Nearest to not: never, still, they, normally, always, generally, now, technically,
Nearest to most: more, less, nasir, extremely, particularly, especially, among, many,
Nearest to between: with, within, among, sennacherib, adf, hooking, arecibo, cereal,
Nearest to i: we, you, ii, they, iii, frac, t, never,
#+end_example


#+BEGIN_SRC python
  num_points = 400

  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
  two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  def plot(embeddings, labels):
    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'
    pylab.figure(figsize=(15,15))  # in inches
    for i, label in enumerate(labels):
      x, y = embeddings[i,:]
      pylab.scatter(x, y)
      pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',
                     ha='right', va='bottom')
    # pylab.show()

  words = [reverse_dictionary[i] for i in range(1, num_points+1)]
#+END_SRC

#+RESULTS:
  
#+BEGIN_SRC python :results file
  plot(two_d_embeddings, words)
  img_file = "imgs/python-matplot-fig.png"
  pylab.savefig(img_file)
  
  img_file
#+END_SRC

#+RESULTS:
[[file:imgs/python-matplot-fig.png]]


** Problem

An alternative to skip-gram is another Word2Vec model called [[http://arxiv.org/abs/1301.3781][CBOW]] (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.


#+BEGIN_SRC python :results none
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  # %matplotlib inline
  from __future__ import print_function
  import collections
  import math
  import numpy as np
  import os
  import random
  import tensorflow as tf
  import zipfile
  from matplotlib import pylab
  from six.moves import range
  from six.moves.urllib.request import urlretrieve
  from sklearn.manifold import TSNE
#+END_SRC

#+BEGIN_SRC python :results output
  Url = 'http://mattmahoney.net/dc/'

  def maybe_download(filename, expected_bytes):
    """Download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
      filename, _ = urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified %s' % filename)
    else:
      print(statinfo.st_size)
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  filename = maybe_download('text8.zip', 31344016)
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... >>> Found and verified text8.zip

Read the data into a string.

#+BEGIN_SRC python
     def read_data(filename):
       """Extract the first file enclosed in a zip file as a list of words"""
       with zipfile.ZipFile(filename) as f:
         data = tf.compat.as_str(f.read(f.namelist()[0])).split()
       return data
      
     words = read_data(filename)
     print('Data size %d' % len(words))
#+END_SRC

#+RESULTS:
: 
: ... ... ... ... >>> >>> Data size 17005207

Build the dictionary and replace rare words with UNK token.

#+BEGIN_SRC python
     vocabulary_size = 50000

     def build_dataset(words):
       count = [['UNK', -1]]
       count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
       dictionary = dict()
       for word, _ in count:
         dictionary[word] = len(dictionary)
       data = list()
       unk_count = 0
       for word in words:
         if word in dictionary:
           index = dictionary[word]
         else:
           index = 0  # dictionary['UNK']
           unk_count = unk_count + 1
         data.append(index)
       count[0][1] = unk_count
       reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) 
       return data, count, dictionary, reverse_dictionary

     data, count, dictionary, reverse_dictionary = build_dataset(words)
     print('Most common words (+UNK)', count[:5])
     print('Sample data', data[:10])
     del words  # Hint to reduce memory.
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> >>> Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
: Sample data [5234, 3081, 12, 6, 195, 2, 3137, 46, 59, 156]

Function to generate a training batch for the CBOW model.
#+BEGIN_SRC python
  def generate_batch_Q(batch_size, radius, num_epochs=None):
    window_size = 2*radius+1
    B = batch_size
    N = tf.Variable(len(data), dtype = tf.int32)
    sy_data = tf.Variable(data)
    index_q = tf.train.range_input_producer(limit=N-window_size, shuffle=True, 
                                            num_epochs=num_epochs)
    left_ix = index_q.dequeue()
    train = sy_data[left_ix:left_ix+window_size]
    ctx_l, label, ctx_r = tf.train.batch(tensors=tf.split(train, [radius,1,radius]),
                                         batch_size=B, dynamic_pad=True, name="data_batch",
                                         allow_smaller_final_batch=True)
    return tf.concat([ctx_l, ctx_r], axis=1), label
#+END_SRC

#+RESULTS:

Train a skip-gram model.
#+BEGIN_SRC python
  batch_size = 128
  embedding_size = 128 # Dimension of the embedding vector.
  radius = 4 # number of history / future words (total = R x 2)
  # validation samples to the words that have a low numeric ID, which by
  # construction are also the most frequent. 
  valid_size = 16 # Random set of words to evaluate similarity on.
  valid_window = 100 # Only pick dev samples in the head of the distribution.
  valid_examples = np.array(random.sample(range(valid_window), valid_size))
  num_sampled = 64 # Number of negative examples to sample.
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Input data.
    train_dataset, train_labels = generate_batch_Q(batch_size, radius)
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)
    # Variables.
    embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
      tf.truncated_normal([vocabulary_size, embedding_size],
                           stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))
    # Model.
    # Look up (averaged) embeddings for inputs.
    embed = tf.reduce_mean(tf.nn.embedding_lookup(embeddings, train_dataset), axis=1)  # take mean of embeddings over contexts
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
      tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,
                                 labels=train_labels, num_sampled=num_sampled,
                                 num_classes=vocabulary_size))
    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities 
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  num_steps = 100001

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    q_threads = tf.train.start_queue_runners()   
    print('Initialized')
    average_loss = 0
    for step in range(num_steps):
      _, l = session.run([optimizer, loss])
      average_loss += l
      if step % 2000 == 0:
        if step > 0:
          average_loss = average_loss / 2000
        # The average loss is an estimate of the loss over the last 2000 batches.
        print('Average loss at step %d: %f' % (step, average_loss))
        average_loss = 0
      # note that this is expensive (~20% slowdown if computed every 500 steps)
      if step % 10000 == 0:
        sim = similarity.eval()
        for i in range(valid_size):
          valid_word = reverse_dictionary[valid_examples[i]]
          top_k = 8 # number of nearest neighbors
          nearest = (-sim[i, :]).argsort()[1:top_k+1]
          log = 'Nearest to %s:' % valid_word
          for k in range(top_k):
            close_word = reverse_dictionary[nearest[k]]
            log = '%s %s,' % (log, close_word)
          print(log)
    final_embeddings = normalized_embeddings.eval()
#+END_SRC

#+RESULTS:
#+begin_example

>>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2017-06-18 21:17:40.837351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-06-18 21:17:40.837849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.63GiB
2017-06-18 21:17:40.837861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-06-18 21:17:40.837865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-06-18 21:17:40.837871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
2017-06-18 21:17:41.067831: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-18 21:17:41.067851: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-18 21:17:41.068648: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a405303bf0 executing computations on platform Host. Devices:
2017-06-18 21:17:41.068666: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-06-18 21:17:41.068778: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-18 21:17:41.068786: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-18 21:17:41.069181: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a40534c1d0 executing computations on platform CUDA. Devices:
2017-06-18 21:17:41.069190: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Quadro M2000M, Compute Capability 5.0
Initialized
Average loss at step 0: 7.901086
Nearest to also: apparent, wealthy, mussels, psychiatric, hitpa, walras, exmoor, respecting,
Nearest to have: barristers, denounces, obsessive, staircase, irv, summon, marguerite, scars,
Nearest to been: weil, vax, cabbie, realisation, hydrate, harmless, confucianism, blaxploitation,
Nearest to new: interface, narrated, flaherty, kit, criticism, physicalism, bosom, thursday,
Nearest to of: loonie, cartagena, rambla, observing, almeida, osage, aron, securing,
Nearest to i: yoshinkan, supper, venue, lili, belvedere, brilliance, assr, maximilian,
Nearest to its: conductor, cds, floated, fretting, malleus, tae, asymptotically, jpg,
Nearest to about: analog, minster, cheney, redstone, mostar, esdi, muni, go,
Nearest to most: soundly, glucose, inlets, band, euphonium, mustafa, meccan, watchdog,
Nearest to not: beethoven, decentralisation, wycliffe, ellipsis, humble, waas, ferguson, romanization,
Nearest to may: mcmaster, coincidental, pejorative, meat, husserl, macedonians, confirmed, ism,
Nearest to between: df, mathematik, harvests, synonym, stand, weidman, exceptionally, dwarfed,
Nearest to there: legacy, caesura, chronicon, lollard, coveted, arguably, cockpits, trustworthy,
Nearest to more: ravel, tide, violins, dupuis, escort, banned, intermission, research,
Nearest to see: fx, mistakenly, ya, labeling, metered, osip, childless, middleweight,
Nearest to two: environments, pir, fingal, must, disordered, magnificat, fireplace, rose,
2017-06-18 21:17:42.165060: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2749 get requests, put_count=2523 evicted_count=1000 eviction_rate=0.396354 and unsatisfied allocation rate=0.482357
2017-06-18 21:17:42.165083: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2017-06-18 21:17:42.650474: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4898 get requests, put_count=4809 evicted_count=1000 eviction_rate=0.207943 and unsatisfied allocation rate=0.227031
2017-06-18 21:17:42.650497: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
Average loss at step 2000: 4.352453
Average loss at step 4000: 3.896251
Average loss at step 6000: 3.775162
Average loss at step 8000: 3.711626
Average loss at step 10000: 3.654511
Nearest to also: apparent, systems, eventually, mussels, atheistic, weishaupt, gust, exmoor,
Nearest to have: has, had, be, are, denounces, marguerite, brandt, barristers,
Nearest to been: vax, harmless, rance, confucianism, weil, irritate, realisation, blaxploitation,
Nearest to new: interface, iceni, flaherty, lone, bosom, viol, precedents, secunda,
Nearest to of: loonie, among, rambla, undertaken, securing, observing, admiral, cataloging,
Nearest to i: you, falklands, abbott, g, thanksgiving, lili, powerbook, sedative,
Nearest to its: this, manchukuo, the, kushan, asymptotically, rowboat, their, hada,
Nearest to about: redstone, if, code, vyasa, analog, quine, louise, pooh,
Nearest to most: tower, language, soundly, glucose, raps, nsfnet, stram, bing,
Nearest to not: still, what, ellipsis, decentralisation, it, however, beethoven, homosexuality,
Nearest to may: could, can, will, must, coincidental, would, should, distinguish,
Nearest to between: weidman, over, synonym, penchant, quell, tats, sidebands, cosby,
Nearest to there: legacy, sept, trustworthy, these, cuyp, farms, inquire, funimation,
Nearest to more: crispy, escort, intermission, boats, spaceman, ravel, arithmetical, less,
Nearest to see: fx, inheritors, metered, mistakenly, francia, california, cartels, assistance,
Nearest to two: three, missoula, one, arturo, environments, pir, spirituals, ste,
Average loss at step 12000: 3.626777
Average loss at step 14000: 3.586024
Average loss at step 16000: 3.556536
Average loss at step 18000: 3.533802
Average loss at step 20000: 3.507083
Nearest to also: apparent, gust, abram, systems, eventually, mussels, wealthy, atheistic,
Nearest to have: had, has, having, brandt, be, straddles, barristers, are,
Nearest to been: become, rance, vax, harmless, realisation, embattled, irritate, globes,
Nearest to new: interface, flaherty, iceni, lup, ftl, rrez, kit, lone,
Nearest to of: among, rambla, loonie, securing, cataloging, admiral, undertaken, observing,
Nearest to i: you, me, g, powerbook, abbott, lili, m, sedative,
Nearest to its: their, kushan, the, this, asymptotically, manchukuo, rowboat, hada,
Nearest to about: redstone, if, since, louise, little, code, vyasa, strictly,
Nearest to most: tower, language, soundly, ethnic, glucose, nsfnet, bearers, among,
Nearest to not: still, what, ellipsis, homosexuality, centrepiece, specialized, decentralisation, thermotropic,
Nearest to may: could, can, must, will, should, would, might, cannot,
Nearest to between: weidman, over, department, synonym, penchant, tats, inflow, quell,
Nearest to there: legacy, trustworthy, cuyp, sept, these, marrying, landings, inquire,
Nearest to more: less, crispy, rather, boats, spaceman, escort, thence, dane,
Nearest to see: fx, inheritors, metered, assistance, francia, mistakenly, cartels, madge,
Nearest to two: three, arturo, missoula, veterinarians, strictly, four, ste, radio,
Average loss at step 22000: 3.482478
Average loss at step 24000: 3.460762
Average loss at step 26000: 3.441429
Average loss at step 28000: 3.427378
Average loss at step 30000: 3.417792
Nearest to also: apparent, abram, gust, pronounce, witt, wealthy, stockhausen, hitpa,
Nearest to have: had, has, having, brandt, straddles, be, were, barristers,
Nearest to been: become, rance, embattled, vax, realisation, harmless, several, globes,
Nearest to new: flaherty, iceni, ftl, enabled, interface, lup, kit, rrez,
Nearest to of: among, rambla, securing, loonie, cataloging, observing, undertaken, throughout,
Nearest to i: you, me, powerbook, g, abbott, we, t, lili,
Nearest to its: their, kushan, the, lydian, this, asymptotically, manchukuo, potential,
Nearest to about: redstone, louise, little, if, code, remarking, mounted, since,
Nearest to most: tower, among, ethnic, bearers, more, soundly, many, upsets,
Nearest to not: still, what, ellipsis, homosexuality, centrepiece, specialized, decentralisation, struma,
Nearest to may: could, can, must, will, should, would, might, cannot,
Nearest to between: weidman, over, tats, forbade, other, department, inflow, coretta,
Nearest to there: legacy, cuyp, trustworthy, sept, these, marrying, landings, lollard,
Nearest to more: less, rather, crispy, boats, escort, spaceman, faster, thence,
Nearest to see: fx, inheritors, metered, assistance, madge, thyroid, mistakenly, dieu,
Nearest to two: three, four, arturo, one, veterinarians, turret, five, alpinus,
Average loss at step 32000: 3.396475
Average loss at step 34000: 3.385493
Average loss at step 36000: 3.370742
Average loss at step 38000: 3.359877
Average loss at step 40000: 3.344813
Nearest to also: abram, apparent, pronounce, gust, hitpa, below, witt, craters,
Nearest to have: has, had, having, brandt, straddles, include, be, were,
Nearest to been: become, rance, realisation, vax, embattled, globes, irritate, bolshevik,
Nearest to new: flaherty, iceni, ftl, lup, enabled, kit, adhesive, lombok,
Nearest to of: among, rambla, throughout, observing, securing, cataloging, undertaken, loonie,
Nearest to i: you, me, we, g, powerbook, rias, t, my,
Nearest to its: their, kushan, the, lydian, jodie, potential, stays, supporting,
Nearest to about: redstone, louise, code, little, mounted, analog, among, severe,
Nearest to most: more, tower, many, bearers, among, ethnic, upsets, decline,
Nearest to not: still, ellipsis, specialized, homosexuality, struma, centrepiece, what, decentralisation,
Nearest to may: could, can, must, should, will, would, might, cannot,
Nearest to between: weidman, over, forbade, department, tats, selangor, within, stowe,
Nearest to there: legacy, trustworthy, sept, cuyp, marrying, lollard, freefall, funimation,
Nearest to more: less, rather, most, faster, boats, crispy, spaceman, escort,
Nearest to see: fx, metered, inheritors, madge, assistance, thyroid, dieu, routledge,
Nearest to two: three, four, arturo, five, missoula, mile, one, veterinarians,
Average loss at step 42000: 3.335228
Average loss at step 44000: 3.319990
Average loss at step 46000: 3.313496
Average loss at step 48000: 3.302013
Average loss at step 50000: 3.289619
Nearest to also: below, abram, pronounce, hitpa, selene, wealthy, apparent, dwan,
Nearest to have: has, had, having, brandt, be, straddles, include, were,
Nearest to been: become, rance, globes, embattled, irritate, realisation, vax, led,
Nearest to new: lup, flaherty, iceni, ftl, kit, enabled, adhesive, segment,
Nearest to of: throughout, among, rambla, securing, observing, cataloging, undertaken, loonie,
Nearest to i: you, me, we, my, g, god, rias, powerbook,
Nearest to its: their, kushan, the, lydian, jodie, our, his, potential,
Nearest to about: louise, redstone, bogart, severe, rousseau, keynote, mounted, kajang,
Nearest to most: more, many, tower, bearers, among, upsets, extremely, decline,
Nearest to not: still, ellipsis, specialized, homosexuality, struma, sustainable, centrepiece, treeless,
Nearest to may: could, can, must, should, would, will, might, cannot,
Nearest to between: forbade, weidman, both, within, selangor, department, tats, stowe,
Nearest to there: legacy, cuyp, trustworthy, sept, marrying, lollard, mapuche, funimation,
Nearest to more: less, rather, most, faster, boats, spaceman, violins, escort,
Nearest to see: fx, metered, madge, assistance, thyroid, inheritors, dieu, cooperatives,
Nearest to two: three, four, licensee, mile, arturo, veterinarians, delaware, one,
Average loss at step 52000: 3.285617
Average loss at step 54000: 3.267626
Average loss at step 56000: 3.264203
Average loss at step 58000: 3.252832
Average loss at step 60000: 3.240172
Nearest to also: below, abram, pronounce, selene, disambiguation, craters, wealthy, hitpa,
Nearest to have: has, had, having, brandt, include, straddles, were, barristers,
Nearest to been: become, rance, globes, led, come, embattled, irritate, kaltenbrunner,
Nearest to new: lup, flaherty, iceni, adhesive, kit, enabled, ftl, segment,
Nearest to of: throughout, rambla, securing, among, cataloging, loonie, observing, undertaken,
Nearest to i: you, me, we, my, god, g, rias, glare,
Nearest to its: their, kushan, his, lydian, bengal, stays, malleus, jodie,
Nearest to about: louise, redstone, bogart, rousseau, severe, kajang, keynote, caterpillars,
Nearest to most: more, many, bearers, tower, some, among, upsets, decline,
Nearest to not: still, homosexuality, ellipsis, struma, sustainable, specialized, centrepiece, oahu,
Nearest to may: could, must, can, should, would, will, might, cannot,
Nearest to between: both, selangor, forbade, within, stowe, tats, with, weidman,
Nearest to there: legacy, sept, trustworthy, cuyp, marrying, safin, mapuche, considered,
Nearest to more: less, most, rather, faster, boats, spaceman, greater, diamond,
Nearest to see: fx, known, madge, thyroid, metered, assistance, cooperatives, cyprus,
Nearest to two: three, four, veterinarians, arturo, licensee, throats, five, six,
Average loss at step 62000: 3.232007
Average loss at step 64000: 3.218891
Average loss at step 66000: 3.216947
Average loss at step 68000: 3.212265
Average loss at step 70000: 3.210981
Nearest to also: below, abram, disambiguation, pronounce, selene, dwan, wealthy, brugha,
Nearest to have: has, had, having, brandt, include, straddles, were, barristers,
Nearest to been: become, come, led, rance, globes, panic, embattled, bolshevik,
Nearest to new: lup, kit, adhesive, flaherty, iceni, segment, enabled, ambrosius,
Nearest to of: rambla, securing, throughout, among, cataloging, undertaken, observing, loonie,
Nearest to i: you, me, we, my, g, god, rias, mchale,
Nearest to its: their, kushan, his, bengal, lydian, stays, the, jodie,
Nearest to about: louise, redstone, rousseau, bogart, severe, kajang, arthropod, keynote,
Nearest to most: more, many, bearers, among, upsets, some, tower, decline,
Nearest to not: still, homosexuality, ellipsis, specialized, sustainable, neusner, struma, oahu,
Nearest to may: can, must, could, should, would, will, might, cannot,
Nearest to between: both, selangor, within, forbade, stowe, mmix, with, weidman,
Nearest to there: legacy, sept, trustworthy, cuyp, mapuche, marrying, safin, cayce,
Nearest to more: less, most, faster, rather, boats, greater, spaceman, diamond,
Nearest to see: fx, known, madge, cyprus, thyroid, metered, galante, ludovico,
Nearest to two: three, four, five, veterinarians, arturo, mile, kw, licensee,
Average loss at step 72000: 3.196167
Average loss at step 74000: 3.185174
Average loss at step 76000: 3.184540
Average loss at step 78000: 3.180000
Average loss at step 80000: 3.173517
Nearest to also: below, abram, disambiguation, pronounce, dwan, inept, selene, wealthy,
Nearest to have: has, had, having, include, brandt, straddles, were, are,
Nearest to been: become, come, rance, led, globes, panic, gone, embattled,
Nearest to new: lup, kit, flaherty, adhesive, segment, iceni, enabled, timescale,
Nearest to of: throughout, rambla, securing, loonie, among, cataloging, observing, undertaken,
Nearest to i: you, me, we, my, g, god, ii, rias,
Nearest to its: their, kushan, his, jodie, the, lydian, stays, bengal,
Nearest to about: louise, bogart, rousseau, redstone, severe, around, kajang, arthropod,
Nearest to most: more, many, bearers, some, among, upsets, very, extremely,
Nearest to not: still, ellipsis, homosexuality, specialized, neusner, sustainable, struma, never,
Nearest to may: can, could, must, should, would, will, might, cannot,
Nearest to between: both, selangor, within, forbade, mmix, with, stowe, laxness,
Nearest to there: legacy, sept, trustworthy, cayce, marrying, cuyp, mapuche, lollard,
Nearest to more: less, faster, most, rather, boats, diamond, greater, better,
Nearest to see: fx, known, cyprus, madge, thyroid, galante, ludovico, strata,
Nearest to two: three, four, five, arturo, six, kw, bubbles, veterinarians,
Average loss at step 82000: 3.165248
Average loss at step 84000: 3.156991
Average loss at step 86000: 3.151638
Average loss at step 88000: 3.146849
Average loss at step 90000: 3.138458
Nearest to also: below, disambiguation, abram, pronounce, selene, abkhazian, angela, statutes,
Nearest to have: has, had, having, include, brandt, straddles, scars, barristers,
Nearest to been: become, come, gone, globes, led, rance, panic, hoddle,
Nearest to new: kit, lup, segment, flaherty, adhesive, enabled, heater, fijian,
Nearest to of: securing, throughout, cataloging, rambla, observing, loonie, among, cargo,
Nearest to i: you, me, we, ii, my, god, g, rias,
Nearest to its: their, his, kushan, the, jodie, our, stays, bengal,
Nearest to about: louise, severe, bogart, rousseau, redstone, around, kajang, arthropod,
Nearest to most: more, many, bearers, some, among, extremely, very, upsets,
Nearest to not: still, ellipsis, specialized, never, homosexuality, sustainable, struma, neusner,
Nearest to may: can, must, could, should, might, would, will, cannot,
Nearest to between: both, within, selangor, with, forbade, stowe, laxness, mmix,
Nearest to there: legacy, sept, trustworthy, longer, mapuche, cayce, safin, lollard,
Nearest to more: less, most, faster, rather, greater, boats, diamond, better,
Nearest to see: fx, known, ludovico, galante, madge, thyroid, cyprus, references,
Nearest to two: three, four, five, six, kw, licensee, one, arturo,
Average loss at step 92000: 3.137631
Average loss at step 94000: 3.131285
Average loss at step 96000: 3.122762
Average loss at step 98000: 3.118939
Average loss at step 100000: 3.112847
Nearest to also: below, disambiguation, abram, pronounce, angela, selene, dwan, now,
Nearest to have: has, had, having, brandt, include, straddles, scars, are,
Nearest to been: become, come, gone, globes, led, hou, be, panic,
Nearest to new: kit, lup, segment, flaherty, adhesive, heater, fijian, enabled,
Nearest to of: securing, cataloging, rambla, throughout, cargo, among, undertaken, loonie,
Nearest to i: you, me, we, ii, my, god, g, rias,
Nearest to its: their, his, kushan, stays, jodie, our, her, bengal,
Nearest to about: louise, bogart, severe, rousseau, redstone, around, kajang, keynote,
Nearest to most: more, many, some, bearers, extremely, among, very, quarrelled,
Nearest to not: still, never, homosexuality, ellipsis, sustainable, struma, neusner, specialized,
Nearest to may: can, must, could, should, might, will, would, cannot,
Nearest to between: both, within, selangor, forbade, with, laxness, stowe, mmix,
Nearest to there: legacy, sept, trustworthy, cayce, longer, safin, lollard, cuyp,
Nearest to more: less, most, faster, rather, greater, boats, better, diamond,
Nearest to see: fx, known, ludovico, galante, cyprus, references, khmelnytsky, madge,
Nearest to two: three, four, five, six, kw, veterinarians, mile, one,
#+end_example


#+BEGIN_SRC python
  num_points = 400

  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
  two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  def plot(embeddings, labels):
    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'
    pylab.figure(figsize=(15,15))  # in inches
    for i, label in enumerate(labels):
      x, y = embeddings[i,:]
      pylab.scatter(x, y)
      pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',
                     ha='right', va='bottom')
    # pylab.show()

  words = [reverse_dictionary[i] for i in range(1, num_points+1)]
#+END_SRC

#+RESULTS:
  
#+BEGIN_SRC python :results file
  plot(two_d_embeddings, words)
  img_file = "imgs/cbow_plot.png"
  pylab.savefig(img_file)

  print(img_file)
#+END_SRC

#+RESULTS:
[[file:imgs/cbow_plot.png]]



* Assignment 6
:PROPERTIES:
:header-args: :session a6py :results output
:END:
** starter code

After training a skip-gram model in =5_word2vec.ipynb=, the goal of this
notebook is to train a LSTM character model over
[[http://mattmahoney.net/dc/textdata][Text8]] data.

#+NAME: start1
#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import os
  import numpy as np
  import random
  import string
  import tensorflow as tf
  import zipfile
  from six.moves import range
  from six.moves.urllib.request import urlretrieve
#+END_SRC

#+RESULTS: start1
: Python 3.5.3 (default, Jan 19 2017, 14:11:04) 
: [GCC 6.3.0 20170118] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: ... python.el: native completion setup loaded

#+NAME: start2
#+BEGIN_SRC python
  url = 'http://mattmahoney.net/dc/'

  def maybe_download(filename, expected_bytes):
    """Download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
      filename, _ = urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified %s' % filename)
    else:
      print(statinfo.st_size)
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  filename = maybe_download('text8.zip', 31344016)
#+END_SRC

#+RESULTS: start2
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... >>> Found and verified text8.zip



#+NAME: start3
#+BEGIN_SRC python
  def read_data(filename):
    f = zipfile.ZipFile(filename)
    for name in f.namelist():
      return tf.compat.as_str(f.read(name))
    f.close()

  text = read_data(filename)
  print('Data size %d' % len(text))
#+END_SRC

#+RESULTS: start3
: 
: ... ... ... ... >>> >>> Data size 100000000

#+NAME: start4
#+BEGIN_SRC python
  a = random.randint(0,len(text))
  text[a:a+1000]
#+END_SRC

#+RESULTS: start4
: 
: 'esitylene c six h three ch three three toluene c six h five ch three xylene c six h four ch three two other substituents aniline c six h five nh two acetylsalicylic acid c six h four o c o ch three cooh benzoic acid c six h five cooh biphenyl c six h five two chlorobenzene c six h five cl nitrobenzene c six h five no two paracetamol c six h four nh c o ch three oh phenacetin c six h four nh c o ch three o ch two ch three phenol c six h five oh picric acid c six h two oh no two three salicylic acid c six h four oh cooh trinitrotoluene c six h two ch three no two three fused aromatic rings anthracene benzofuran indole isoquinoline naphthalene phenanthrene polycyclic aromatic hydrocarbons pah quinoline heterocyclic analogs in heterocycles carbon atoms in the benzene ring are replaced with another element pyrazine pyridazine pyridine pyrimidine see simple aromatic ring for analogs of benzene production benzene may result whenever carbon rich materials undergo incomplete combustion it is pr'


Create a small validation set.
#+NAME: start5
#+BEGIN_SRC python
  valid_size = 2
  valid_text = text[:valid_size]
  train_text = text[valid_size:]
  train_size = len(train_text)
  print(train_size, train_text[:64])
  print(valid_size, valid_text[:64])
#+END_SRC

#+RESULTS: start5
: 
: >>> >>> >>> 99999000 ons anarchists advocate social relations based upon voluntary as
: 1000  anarchism originated as a term of abuse first used against earl


Utility functions to map characters to vocabulary IDs and back.
#+NAME: start6
#+BEGIN_SRC python
  ord(string.ascii_lowercase[0])
#+END_SRC

#+RESULTS: start6
: 97

#+NAME: start7
#+BEGIN_SRC python
  vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '
  first_letter = ord(string.ascii_lowercase[0])

  def char2id(char):
    if char in string.ascii_lowercase:
      return ord(char) - first_letter + 1
    elif char == ' ':
      return 0
    else:
      print('Unexpected character: %s' % char)
      return 0

  def id2char(dictid):
    if dictid > 0:
      return chr(dictid + first_letter - 1)
    else:
      return ' '

  print(char2id('a'), char2id('z'), char2id(' '), char2id(''))
  print(id2char(1), id2char(26), id2char(0))
#+END_SRC

#+RESULTS: start7
: 
: >>> >>> ... ... ... ... ... ... ... ... >>> ... ... ... ... ... >>> Unexpected character: 
: 1 26 0 0
: a z

Function to generate a training batch for the LSTM model.

#+NAME: start9
#+BEGIN_SRC python :var b_size=2**6 :var seq_len=2**7
  batch_size=b_size
  num_unrollings=seq_len
  
  class BatchGenerator(object):
    def __init__(self, text, batch_size, num_unrollings):
      self._text = text
      self._text_size = len(text)
      self._batch_size = batch_size
      self._num_unrollings = num_unrollings
      segment = self._text_size // batch_size
      self._cursor = [ offset * segment for offset in range(batch_size)]
      self._last_batch = self._next_batch()
    def _next_batch(self):
      """Generate a single batch from the current cursor position in the data."""
      batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)
      for b in range(self._batch_size):
        batch[b, char2id(self._text[self._cursor[b]])] = 1.0
        self._cursor[b] = (self._cursor[b] + 1) % self._text_size
      return batch
    def next(self):
      """Generate the next array of batches from the data. The array consists of
      the last batch of the previous array, followed by num_unrollings new ones.
      """
      batches = [self._last_batch]
      for step in range(self._num_unrollings):
        batches.append(self._next_batch())
      self._last_batch = batches[-1]
      return batches

  def characters(probabilities):
    """Turn a 1-hot encoding or a probability distribution over the possible
    characters back into its (most likely) character representation."""
    return [id2char(c) for c in np.argmax(probabilities, 1)]

  def batches2string(batches):
    """Convert a sequence of batches back into their (most likely) string
    representation."""
    s = [''] * batches[0].shape[0]
    for b in batches:
      s = [''.join(x) for x in zip(s, characters(b))]
    return s

  train_batches = BatchGenerator(train_text, batch_size, num_unrollings)
  valid_batches = BatchGenerator(valid_text, 1, 1)

  print(batches2string(train_batches.next()))
  print(batches2string(train_batches.next()))
  print(batches2string(valid_batches.next()))
  print(batches2string(valid_batches.next()))
#+END_SRC

#+RESULTS: start9
: 
: >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> ... ... ... ... >>> ... ... ... ... ... ... ... >>> >>> >>> >>> ['o n s   a n a r c h i ', 'w h e n   m i l i t a ', 'l l e r i a   a r c h ', '  a b b e y s   a n d ', 'm a r r i e d   u r r ', 'h e l   a n d   r i c ', 'y   a n d   l i t u r ', 'a y   o p e n e d   f ', 't i o n   f r o m   t ', 'm i g r a t i o n   t ', 'n e w   y o r k   o t ', 'h e   b o e i n g   s ', 'e   l i s t e d   w i ', 'e b e r   h a s   p r ', 'o   b e   m a d e   t ', 'y e r   w h o   r e c ', 'o r e   s i g n i f i ', 'a   f i e r c e   c r ', '  t w o   s i x   e i ', 'a r i s t o t l e   s ', 'i t y   c a n   b e   ', '  a n d   i n t r a c ', 't i o n   o f   t h e ', 'd y   t o   p a s s   ', 'f   c e r t a i n   d ', 'a t   i t   w i l l   ', 'e   c o n v i n c e   ', 'e n t   t o l d   h i ', 'a m p a i g n   a n d ', 'r v e r   s i d e   s ', 'i o u s   t e x t s   ', 'o   c a p i t a l i z ', 'a   d u p l i c a t e ', 'g h   a n n   e s   d ', 'i n e   j a n u a r y ', 'r o s s   z e r o   t ', 'c a l   t h e o r i e ', 'a s t   i n s t a n c ', '  d i m e n s i o n a ', 'm o s t   h o l y   m ', 't   s   s u p p o r t ', 'u   i s   s t i l l   ', 'e   o s c i l l a t i ', 'o   e i g h t   s u b ', 'o f   i t a l y   l a ', 's   t h e   t o w e r ', 'k l a h o m a   p r e ', 'e r p r i s e   l i n ', 'w s   b e c o m e s   ', 'e t   i n   a   n a z ', 't h e   f a b i a n   ', 'e t c h y   t o   r e ', '  s h a r m a n   n e ', 'i s e d   e m p e r o ', 't i n g   i n   p o l ', 'd   n e o   l a t i n ', 't h   r i s k y   r i ', 'e n c y c l o p e d i ', 'f e n s e   t h e   a ', 'd u a t i n g   f r o ', 't r e e t   g r i d   ', 'a t i o n s   m o r e ', 'a p p e a l   o f   d ', 's i   h a v e   m a d ']
: ['i s t s   a d v o c a ', 'a r y   g o v e r n m ', 'h e s   n a t i o n a ', 'd   m o n a s t e r i ', 'r a c a   p r i n c e ', 'c h a r d   b a e r   ', 'r g i c a l   l a n g ', 'f o r   p a s s e n g ', 't h e   n a t i o n a ', 't o o k   p l a c e   ', 't h e r   w e l l   k ', 's e v e n   s i x   s ', 'i t h   a   g l o s s ', 'r o b a b l y   b e e ', 't o   r e c o g n i z ', 'c e i v e d   t h e   ', 'i c a n t   t h a n   ', 'r i t i c   o f   t h ', 'i g h t   i n   s i g ', 's   u n c a u s e d   ', '  l o s t   a s   i n ', 'c e l l u l a r   i c ', 'e   s i z e   o f   t ', '  h i m   a   s t i c ', 'd r u g s   c o n f u ', '  t a k e   t o   c o ', '  t h e   p r i e s t ', 'i m   t o   n a m e   ', 'd   b a r r e d   a t ', 's t a n d a r d   f o ', '  s u c h   a s   e s ', 'z e   o n   t h e   g ', 'e   o f   t h e   o r ', 'd   h i v e r   o n e ', 'y   e i g h t   m a r ', 't h e   l e a d   c h ', 'e s   c l a s s i c a ', 'c e   t h e   n o n   ', 'a l   a n a l y s i s ', 'm o r m o n s   b e l ', 't   o r   a t   l e a ', '  d i s a g r e e d   ', 'i n g   s y s t e m   ', 'b t y p e s   b a s e ', 'a n g u a g e s   t h ', 'r   c o m m i s s i o ', 'e s s   o n e   n i n ', 'n u x   s u s e   l i ', '  t h e   f i r s t   ', 'z i   c o n c e n t r ', '  s o c i e t y   n e ', 'e l a t i v e l y   s ', 'e t w o r k s   s h a ', 'o r   h i r o h i t o ', 'l i t i c a l   i n i ', 'n   m o s t   o f   t ', 'i s k e r d o o   r i ', 'i c   o v e r v i e w ', 'a i r   c o m p o n e ', 'o m   a c n m   a c c ', '  c e n t e r l i n e ', 'e   t h a n   a n y   ', 'd e v o t i o n a l   ', 'd e   s u c h   d e v ']
: ['  a ']
: ['a n ']


#+NAME: start10
#+BEGIN_SRC python :results none
  def logprob(predictions, labels):
    """Log-probability of the true labels in a predicted batch."""
    predictions[predictions < 1e-10] = 1e-10
    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]

  def sample_distribution(distribution):
    """Sample one element from a distribution assumed to be an array of normalized
    probabilities.
    """
    r = random.uniform(0, 1)
    s = 0
    for i in range(len(distribution)):
      s += distribution[i]
      if s >= r:
        return i
    return len(distribution) - 1

  def sample(prediction):
    """Turn a (column) prediction into 1-hot encoded samples."""
    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)
    p[0, sample_distribution(prediction[0])] = 1.0
    return p

  def random_distribution():
    """Generate a random column of probabilities."""
    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])
    return b/np.sum(b, 1)[:,None]
#+END_SRC

#+RESULTS: start10

Simple LSTM Model.
#+NAME: start11
#+BEGIN_SRC python :results none
  num_nodes = 64

  graph = tf.Graph()
  with graph.as_default():
    # Parameters:
    # Input gate: input, previous output, and bias.
    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    ib = tf.Variable(tf.zeros([1, num_nodes]))
    # Forget gate: input, previous output, and bias.
    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    fb = tf.Variable(tf.zeros([1, num_nodes]))
    # Memory cell: input, state and bias.                             
    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    cb = tf.Variable(tf.zeros([1, num_nodes]))
    # Output gate: input, previous output, and bias.
    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    ob = tf.Variable(tf.zeros([1, num_nodes]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))
    b = tf.Variable(tf.zeros([vocabulary_size]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates."""
      input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)
      forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)
      update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))
    train_inputs = train_data[:num_unrollings]
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:
      output, state = lstm_cell(i, output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output),
                                  saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.concat(train_labels, 0)))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.1, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    sample_output, sample_state = lstm_cell(
      sample_input, saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:

#+NAME: start12
#+BEGIN_SRC python
  num_steps = 7001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = characters(feed)[0]
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += characters(feed)[0]
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+BEGIN_EXAMPLE
     Initialized
     Average loss at step 0 : 3.29904174805 learning rate: 10.0
     Minibatch perplexity: 27.09
     ================================================================================
     srk dwmrnuldtbbgg tapootidtu xsciu sgokeguw hi ieicjq lq piaxhazvc s fht wjcvdlh
     lhrvallvbeqqquc dxd y siqvnle bzlyw nr rwhkalezo siie o deb e lpdg  storq u nx o
     meieu nantiouie gdys qiuotblci loc hbiznauiccb cqzed acw l tsm adqxplku gn oaxet
     unvaouc oxchywdsjntdh zpklaejvxitsokeerloemee htphisb th eaeqseibumh aeeyj j orw
     ogmnictpycb whtup   otnilnesxaedtekiosqet  liwqarysmt  arj flioiibtqekycbrrgoysj
     ================================================================================
     Validation set perplexity: 19.99
     Average loss at step 100 : 2.59553678274 learning rate: 10.0
     Minibatch perplexity: 9.57
     Validation set perplexity: 10.60
     Average loss at step 200 : 2.24747137785 learning rate: 10.0
     Minibatch perplexity: 7.68
     Validation set perplexity: 8.84
     Average loss at step 300 : 2.09438110709 learning rate: 10.0
     Minibatch perplexity: 7.41
     Validation set perplexity: 8.13
     Average loss at step 400 : 1.99440989017 learning rate: 10.0
     Minibatch perplexity: 6.46
     Validation set perplexity: 7.58
     Average loss at step 500 : 1.9320810616 learning rate: 10.0
     Minibatch perplexity: 6.30
     Validation set perplexity: 6.88
     Average loss at step 600 : 1.90935629249 learning rate: 10.0
     Minibatch perplexity: 7.21
     Validation set perplexity: 6.91
     Average loss at step 700 : 1.85583009005 learning rate: 10.0
     Minibatch perplexity: 6.13
     Validation set perplexity: 6.60
     Average loss at step 800 : 1.82152368546 learning rate: 10.0
     Minibatch perplexity: 6.01
     Validation set perplexity: 6.37
     Average loss at step 900 : 1.83169809818 learning rate: 10.0
     Minibatch perplexity: 7.20
     Validation set perplexity: 6.23
     Average loss at step 1000 : 1.82217029214 learning rate: 10.0
     Minibatch perplexity: 6.73
     ================================================================================
     le action b of the tert sy ofter selvorang previgned stischdy yocal chary the co
     le relganis networks partucy cetinning wilnchan sics rumeding a fulch laks oftes
     hian andoris ret the ecause bistory l pidect one eight five lack du that the ses
     aiv dromery buskocy becomer worils resism disele retery exterrationn of hide in 
     mer miter y sught esfectur of the upission vain is werms is vul ugher compted by
     ================================================================================
     Validation set perplexity: 6.07
     Average loss at step 1100 : 1.77301145077 learning rate: 10.0
     Minibatch perplexity: 6.03
     Validation set perplexity: 5.89
     Average loss at step 1200 : 1.75306463003 learning rate: 10.0
     Minibatch perplexity: 6.50
     Validation set perplexity: 5.61
     Average loss at step 1300 : 1.72937195778 learning rate: 10.0
     Minibatch perplexity: 5.00
     Validation set perplexity: 5.60
     Average loss at step 1400 : 1.74773373723 learning rate: 10.0
     Minibatch perplexity: 6.48
     Validation set perplexity: 5.66
     Average loss at step 1500 : 1.7368799901 learning rate: 10.0
     Minibatch perplexity: 5.22
     Validation set perplexity: 5.44
     Average loss at step 1600 : 1.74528762937 learning rate: 10.0
     Minibatch perplexity: 5.85
     Validation set perplexity: 5.33
     Average loss at step 1700 : 1.70881183743 learning rate: 10.0
     Minibatch perplexity: 5.33
     Validation set perplexity: 5.56
     Average loss at step 1800 : 1.67776108027 learning rate: 10.0
     Minibatch perplexity: 5.33
     Validation set perplexity: 5.29
     Average loss at step 1900 : 1.64935536742 learning rate: 10.0
     Minibatch perplexity: 5.29
     Validation set perplexity: 5.15
     Average loss at step 2000 : 1.69528644681 learning rate: 10.0
     Minibatch perplexity: 5.13
     ================================================================================
     vers soqually have one five landwing to docial page kagan lower with ther batern
     ctor son alfortmandd tethre k skin the known purated to prooust caraying the fit
     je in beverb is the sournction bainedy wesce tu sture artualle lines digra forme
     m rousively haldio ourso ond anvary was for the seven solies hild buil  s  to te
     zall for is it is one nine eight eight one neval to the kime typer oene where he
     ================================================================================
     Validation set perplexity: 5.25
     Average loss at step 2100 : 1.68808053017 learning rate: 10.0
     Minibatch perplexity: 5.17
     Validation set perplexity: 5.01
     Average loss at step 2200 : 1.68322490931 learning rate: 10.0
     Minibatch perplexity: 5.09
     Validation set perplexity: 5.15
     Average loss at step 2300 : 1.64465074301 learning rate: 10.0
     Minibatch perplexity: 5.51
     Validation set perplexity: 5.00
     Average loss at step 2400 : 1.66408578038 learning rate: 10.0
     Minibatch perplexity: 5.86
     Validation set perplexity: 4.80
     Average loss at step 2500 : 1.68515402555 learning rate: 10.0
     Minibatch perplexity: 5.75
     Validation set perplexity: 4.82
     Average loss at step 2600 : 1.65405208349 learning rate: 10.0
     Minibatch perplexity: 5.38
     Validation set perplexity: 4.85
     Average loss at step 2700 : 1.65706222177 learning rate: 10.0
     Minibatch perplexity: 5.46
     Validation set perplexity: 4.78
     Average loss at step 2800 : 1.65204829812 learning rate: 10.0
     Minibatch perplexity: 5.06
     Validation set perplexity: 4.64
     Average loss at step 2900 : 1.65107253551 learning rate: 10.0
     Minibatch perplexity: 5.00
     Validation set perplexity: 4.61
     Average loss at step 3000 : 1.6495274055 learning rate: 10.0
     Minibatch perplexity: 4.53
     ================================================================================
     ject covered in belo one six six to finsh that all di rozial sime it a the lapse
     ble which the pullic bocades record r to sile dric two one four nine seven six f
      originally ame the playa ishaps the stotchational in a p dstambly name which as
     ore volum to bay riwer foreal in nuily operety can and auscham frooripm however 
     kan traogey was lacous revision the mott coupofiteditey the trando insended frop
     ================================================================================
     Validation set perplexity: 4.76
     Average loss at step 3100 : 1.63705502152 learning rate: 10.0
     Minibatch perplexity: 5.50
     Validation set perplexity: 4.76
     Average loss at step 3200 : 1.64740695596 learning rate: 10.0
     Minibatch perplexity: 4.84
     Validation set perplexity: 4.67
     Average loss at step 3300 : 1.64711504817 learning rate: 10.0
     Minibatch perplexity: 5.39
     Validation set perplexity: 4.57
     Average loss at step 3400 : 1.67113256454 learning rate: 10.0
     Minibatch perplexity: 5.56
     Validation set perplexity: 4.71
     Average loss at step 3500 : 1.65637169957 learning rate: 10.0
     Minibatch perplexity: 5.03
     Validation set perplexity: 4.80
     Average loss at step 3600 : 1.66601825476 learning rate: 10.0
     Minibatch perplexity: 4.63
     Validation set perplexity: 4.52
     Average loss at step 3700 : 1.65021387935 learning rate: 10.0
     Minibatch perplexity: 5.50
     Validation set perplexity: 4.56
     Average loss at step 3800 : 1.64481814981 learning rate: 10.0
     Minibatch perplexity: 4.60
     Validation set perplexity: 4.54
     Average loss at step 3900 : 1.642069453 learning rate: 10.0
     Minibatch perplexity: 4.91
     Validation set perplexity: 4.54
     Average loss at step 4000 : 1.65179730773 learning rate: 10.0
     Minibatch perplexity: 4.77
     ================================================================================
     k s rasbonish roctes the nignese at heacle was sito of beho anarchys and with ro
     jusar two sue wletaus of chistical in causations d ow trancic bruthing ha laters
     de and speacy pulted yoftret worksy zeatlating to eight d had to ie bue seven si
     s fiction of the feelly constive suq flanch earlied curauking bjoventation agent
     quen s playing it calana our seopity also atbellisionaly comexing the revideve i
     ================================================================================
     Validation set perplexity: 4.58
     Average loss at step 4100 : 1.63794238806 learning rate: 10.0
     Minibatch perplexity: 5.47
     Validation set perplexity: 4.79
     Average loss at step 4200 : 1.63822438836 learning rate: 10.0
     Minibatch perplexity: 5.30
     Validation set perplexity: 4.54
     Average loss at step 4300 : 1.61844664574 learning rate: 10.0
     Minibatch perplexity: 4.69
     Validation set perplexity: 4.54
     Average loss at step 4400 : 1.61255454302 learning rate: 10.0
     Minibatch perplexity: 4.67
     Validation set perplexity: 4.54
     Average loss at step 4500 : 1.61543365479 learning rate: 10.0
     Minibatch perplexity: 4.83
     Validation set perplexity: 4.69
     Average loss at step 4600 : 1.61607327104 learning rate: 10.0
     Minibatch perplexity: 5.18
     Validation set perplexity: 4.64
     Average loss at step 4700 : 1.62757282495 learning rate: 10.0
     Minibatch perplexity: 4.24
     Validation set perplexity: 4.66
     Average loss at step 4800 : 1.63222063541 learning rate: 10.0
     Minibatch perplexity: 5.30
     Validation set perplexity: 4.53
     Average loss at step 4900 : 1.63678096652 learning rate: 10.0
     Minibatch perplexity: 5.43
     Validation set perplexity: 4.64
     Average loss at step 5000 : 1.610340662 learning rate: 1.0
     Minibatch perplexity: 5.10
     ================================================================================
     in b one onarbs revieds the kimiluge that fondhtic fnoto cre one nine zero zero 
      of is it of marking panzia t had wap ironicaghni relly deah the omber b h menba
     ong messified it his the likdings ara subpore the a fames distaled self this int
     y advante authors the end languarle meit common tacing bevolitione and eight one
     zes that materly difild inllaring the fusts not panition assertian causecist bas
     ================================================================================
     Validation set perplexity: 4.69
     Average loss at step 5100 : 1.60593637228 learning rate: 1.0
     Minibatch perplexity: 4.69
     Validation set perplexity: 4.47
     Average loss at step 5200 : 1.58993269444 learning rate: 1.0
     Minibatch perplexity: 4.65
     Validation set perplexity: 4.39
     Average loss at step 5300 : 1.57930587292 learning rate: 1.0
     Minibatch perplexity: 5.11
     Validation set perplexity: 4.39
     Average loss at step 5400 : 1.58022856832 learning rate: 1.0
     Minibatch perplexity: 5.19
     Validation set perplexity: 4.37
     Average loss at step 5500 : 1.56654450059 learning rate: 1.0
     Minibatch perplexity: 4.69
     Validation set perplexity: 4.33
     Average loss at step 5600 : 1.58013380885 learning rate: 1.0
     Minibatch perplexity: 5.13
     Validation set perplexity: 4.35
     Average loss at step 5700 : 1.56974959254 learning rate: 1.0
     Minibatch perplexity: 5.00
     Validation set perplexity: 4.34
     Average loss at step 5800 : 1.5839582932 learning rate: 1.0
     Minibatch perplexity: 4.88
     Validation set perplexity: 4.31
     Average loss at step 5900 : 1.57129439116 learning rate: 1.0
     Minibatch perplexity: 4.66
     Validation set perplexity: 4.32
     Average loss at step 6000 : 1.55144061089 learning rate: 1.0
     Minibatch perplexity: 4.55
     ================================================================================
     utic clositical poopy stribe addi nixe one nine one zero zero eight zero b ha ex
     zerns b one internequiption of the secordy way anti proble akoping have fictiona
     phare united from has poporarly cities book ins sweden emperor a sass in origina
     quulk destrebinist and zeilazar and on low and by in science over country weilti
     x are holivia work missincis ons in the gages to starsle histon one icelanctrotu
     ================================================================================
     Validation set perplexity: 4.30
     Average loss at step 6100 : 1.56450940847 learning rate: 1.0
     Minibatch perplexity: 4.77
     Validation set perplexity: 4.27
     Average loss at step 6200 : 1.53433164835 learning rate: 1.0
     Minibatch perplexity: 4.77
     Validation set perplexity: 4.27
     Average loss at step 6300 : 1.54773445129 learning rate: 1.0
     Minibatch perplexity: 4.76
     Validation set perplexity: 4.25
     Average loss at step 6400 : 1.54021131516 learning rate: 1.0
     Minibatch perplexity: 4.56
     Validation set perplexity: 4.24
     Average loss at step 6500 : 1.56153374553 learning rate: 1.0
     Minibatch perplexity: 5.43
     Validation set perplexity: 4.27
     Average loss at step 6600 : 1.59556478739 learning rate: 1.0
     Minibatch perplexity: 4.92
     Validation set perplexity: 4.28
     Average loss at step 6700 : 1.58076951623 learning rate: 1.0
     Minibatch perplexity: 4.77
     Validation set perplexity: 4.30
     Average loss at step 6800 : 1.6070714438 learning rate: 1.0
     Minibatch perplexity: 4.98
     Validation set perplexity: 4.28
     Average loss at step 6900 : 1.58413293839 learning rate: 1.0
     Minibatch perplexity: 4.61
     Validation set perplexity: 4.29
     Average loss at step 7000 : 1.57905534983 learning rate: 1.0
     Minibatch perplexity: 5.08
     ================================================================================
     jague are officiencinels ored by film voon higherise haik one nine on the iffirc
     oshe provision that manned treatists on smalle bodariturmeristing the girto in s
     kis would softwenn mustapultmine truativersakys bersyim by s of confound esc bub
     ry of the using one four six blain ira mannom marencies g with fextificallise re
      one son vit even an conderouss to person romer i a lebapter at obiding are iuse
     ================================================================================
     Validation set perplexity: 4.25
#+END_EXAMPLE

--------------

#+BEGIN_SRC python :results output
' '.join([":var s"+str(i)+" = start"+str(i) for i in range(1,13)])
#+END_SRC

#+RESULTS:
: ':var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = start7 :var s8 = start8 :var s9 = start9 :var s10 = start10 :var s11 = start11 :var s12 = start12'

#+NAME: starupblks
#+BEGIN_SRC python :var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = start7 :var s8 = start8 :var s9 = start9 :var s10 = start10
#+END_SRC

#+BEGIN_SRC python :var s11 = start11 :var s12 = start12
#+END_SRC

#+RESULTS: starupblks


** Problem 1

You might have noticed that the definition of the LSTM cell involves 4
matrix multiplications with the input, and 4 matrix multiplications with
the output. Simplify the expression by using a single matrix multiply
for each, and variables that are 4 times larger.

--------------


#+BEGIN_SRC python
  num_nodes = 64
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Concatanated input and output transition parameter matrices:
    vx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))
    vo = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))
    vb = tf.Variable(tf.zeros([1, num_nodes*4]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))
    b = tf.Variable(tf.zeros([vocabulary_size]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates.
      t_ are transfer """
      tx = tf.matmul(i, vx)
      to = tf.matmul(o, vo)      
      t = tx+to+vb
      input_gate = tf.sigmoid(t[:,0:num_nodes])
      forget_gate = tf.sigmoid(t[:,num_nodes:2*num_nodes])
      update = t[:,2*num_nodes:3*num_nodes]
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(t[:,3*num_nodes:4*num_nodes])
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))
    train_inputs = train_data[:num_unrollings]
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:
      output, state = lstm_cell(i, output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output),
                                  saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.concat(train_labels, 0)))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.1, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    sample_output, sample_state = lstm_cell(
      sample_input, saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:



#+BEGIN_SRC python
  num_steps = 100001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = characters(feed)[0]
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += characters(feed)[0]
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... E tensorflow/stream_executor/cuda/cuda_driver.cc:504] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: iThinkPad
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: iThinkPad
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.57.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.57  Mon Oct  3 20:37:01 PDT 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.57.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.57.0
Initialized
Average loss at step 0: 3.293891 learning rate: 10.000000
Minibatch perplexity: 26.95
================================================================================
eyeceestjccqu t ppqpffapzox otsfrl  mkenzey pszsee excx f tcqdhdwdndlyeedli  ahi
zdqeiszshe vzosmmooeyruh acpiuvcivybr xnacxa rnaxzatx uiasd l minxzxltmstqx obgh
ma dxam wezdxgigqlmguntjleqeedyr eawcntm anznogwic yz xhapo ubdairbkd  vtb iouiw
qgt oonnlokankyltdc r  uzmomdyzffsum goyqfdeuuxudn fmlpnqtn  quetkth emtbsmplsif
 n  odpqrfg zcegfmzojihoaxnh mp tl oudjtvohajfdjwmmlhfh jkcnhxttvikysfxbi xhuvis
================================================================================
Validation set perplexity: 20.24
Average loss at step 100: 2.597705 learning rate: 10.000000
Minibatch perplexity: 10.99
Validation set perplexity: 10.48
Average loss at step 200: 2.256601 learning rate: 10.000000
Minibatch perplexity: 8.77
Validation set perplexity: 8.76
Average loss at step 300: 2.106228 learning rate: 10.000000
Minibatch perplexity: 7.61
Validation set perplexity: 8.06
Average loss at step 400: 2.006132 learning rate: 10.000000
Minibatch perplexity: 7.54
Validation set perplexity: 7.85
Average loss at step 500: 1.938262 learning rate: 10.000000
Minibatch perplexity: 6.54
Validation set perplexity: 6.97
Average loss at step 600: 1.912633 learning rate: 10.000000
Minibatch perplexity: 6.29
Validation set perplexity: 6.91
Average loss at step 700: 1.860030 learning rate: 10.000000
Minibatch perplexity: 6.37
Validation set perplexity: 6.51
Average loss at step 800: 1.817129 learning rate: 10.000000
Minibatch perplexity: 5.87
Validation set perplexity: 6.30
Average loss at step 900: 1.835628 learning rate: 10.000000
Minibatch perplexity: 7.18
Validation set perplexity: 6.24
Average loss at step 1000: 1.824857 learning rate: 10.000000
Minibatch perplexity: 5.61
================================================================================
jation repultow tediedary geaturigg his is tree trame of anselucemmay contral dr
quodey andip for jebein five one nine sever sevee zero nine awill recents fici a
ey ecrece mudisu kble used and berkur of a feather they baschists fromder becnum
gu engy of chnetced of consity un pripnies zero the knose is cleboral fine two n
ces bettor wethah soute whind welv ffom nine omil whelly uncoil the count by ham
================================================================================
Validation set perplexity: 6.11
Average loss at step 1100: 1.778330 learning rate: 10.000000
Minibatch perplexity: 5.60
Validation set perplexity: 5.90
Average loss at step 1200: 1.751372 learning rate: 10.000000
Minibatch perplexity: 5.14
Validation set perplexity: 5.72
Average loss at step 1300: 1.731926 learning rate: 10.000000
Minibatch perplexity: 5.51
Validation set perplexity: 5.74
Average loss at step 1400: 1.746564 learning rate: 10.000000
Minibatch perplexity: 5.99
Validation set perplexity: 5.62
Average loss at step 1500: 1.738494 learning rate: 10.000000
Minibatch perplexity: 4.87
Validation set perplexity: 5.60
Average loss at step 1600: 1.744815 learning rate: 10.000000
Minibatch perplexity: 5.53
Validation set perplexity: 5.56
Average loss at step 1700: 1.711178 learning rate: 10.000000
Minibatch perplexity: 5.59
Validation set perplexity: 5.52
Average loss at step 1800: 1.674822 learning rate: 10.000000
Minibatch perplexity: 5.37
Validation set perplexity: 5.32
Average loss at step 1900: 1.646892 learning rate: 10.000000
Minibatch perplexity: 4.94
Validation set perplexity: 5.32
Average loss at step 2000: 1.695762 learning rate: 10.000000
Minibatch perplexity: 5.63
================================================================================
 hepp recordes assotically well asoputes and the responchine vys dinnect and wol
hards and bass kyological dimanity incomesing to adwe duby sllaybouse of p t to 
zoney both timellowingo jon vicafbul if adficark c sclud vectudal prodical prode
ysed res britted and lappo poogrust unit aboln geaths aresisting brond to there 
es linsh at the chrivixing re arseens headons oviol of indiquensall moster and n
================================================================================
Validation set perplexity: 5.36
Average loss at step 2100: 1.682861 learning rate: 10.000000
Minibatch perplexity: 4.98
Validation set perplexity: 5.17
Average loss at step 2200: 1.684806 learning rate: 10.000000
Minibatch perplexity: 6.31
Validation set perplexity: 5.12
Average loss at step 2300: 1.638204 learning rate: 10.000000
Minibatch perplexity: 4.99
Validation set perplexity: 4.87
Average loss at step 2400: 1.655831 learning rate: 10.000000
Minibatch perplexity: 4.97
Validation set perplexity: 4.83
Average loss at step 2500: 1.677037 learning rate: 10.000000
Minibatch perplexity: 5.12
Validation set perplexity: 4.80
Average loss at step 2600: 1.652734 learning rate: 10.000000
Minibatch perplexity: 5.71
Validation set perplexity: 4.70
Average loss at step 2700: 1.658727 learning rate: 10.000000
Minibatch perplexity: 4.67
Validation set perplexity: 4.77
Average loss at step 2800: 1.654959 learning rate: 10.000000
Minibatch perplexity: 5.74
Validation set perplexity: 4.76
Average loss at step 2900: 1.649255 learning rate: 10.000000
Minibatch perplexity: 5.65
Validation set perplexity: 4.72
Average loss at step 3000: 1.653353 learning rate: 10.000000
Minibatch perplexity: 4.98
================================================================================
to opeage and flymenish as a dacinela balu additions and official four including
bold extancex phaces of which the relandn oftensly the well order trincra x late
beried demdent botchelgor hole the yold jost two in foxchy and pengingly a doind
hied which wheopledin ett and canedarotork fourd uan publikate abribials haws co
ully ups a kue s pach example a theol were readist undia fornot wide it in the s
================================================================================
Validation set perplexity: 4.74
Average loss at step 3100: 1.630684 learning rate: 10.000000
Minibatch perplexity: 5.61
Validation set perplexity: 4.66
Average loss at step 3200: 1.645504 learning rate: 10.000000
Minibatch perplexity: 5.63
Validation set perplexity: 4.62
Average loss at step 3300: 1.636324 learning rate: 10.000000
Minibatch perplexity: 4.94
Validation set perplexity: 4.58
Average loss at step 3400: 1.670409 learning rate: 10.000000
Minibatch perplexity: 5.44
Validation set perplexity: 4.67
Average loss at step 3500: 1.658415 learning rate: 10.000000
Minibatch perplexity: 5.56
Validation set perplexity: 4.76
Average loss at step 3600: 1.665594 learning rate: 10.000000
Minibatch perplexity: 4.40
Validation set perplexity: 4.61
Average loss at step 3700: 1.641715 learning rate: 10.000000
Minibatch perplexity: 5.03
Validation set perplexity: 4.60
Average loss at step 3800: 1.642550 learning rate: 10.000000
Minibatch perplexity: 5.61
Validation set perplexity: 4.65
Average loss at step 3900: 1.634643 learning rate: 10.000000
Minibatch perplexity: 5.14
Validation set perplexity: 4.56
Average loss at step 4000: 1.651241 learning rate: 10.000000
Minibatch perplexity: 4.78
================================================================================
ple american had these prexising quected in does flunk to the hib consproducted 
s of in with law ma genor remossion time the mesping six and hob minisam in the 
x lawon cerial is integes contents is any tipple here generigenthy le the viel u
forms yarks of jeduing flok wondoc of each sold informuntari we been rither calb
lia the largeol been and the flews perhamular squas of phocacy ninely stade and 
================================================================================
Validation set perplexity: 4.61
Average loss at step 4100: 1.631771 learning rate: 10.000000
Minibatch perplexity: 5.44
Validation set perplexity: 4.69
Average loss at step 4200: 1.635594 learning rate: 10.000000
Minibatch perplexity: 5.15
Validation set perplexity: 4.54
Average loss at step 4300: 1.613565 learning rate: 10.000000
Minibatch perplexity: 5.14
Validation set perplexity: 4.52
Average loss at step 4400: 1.609465 learning rate: 10.000000
Minibatch perplexity: 4.87
Validation set perplexity: 4.41
Average loss at step 4500: 1.612396 learning rate: 10.000000
Minibatch perplexity: 5.05
Validation set perplexity: 4.63
Average loss at step 4600: 1.612258 learning rate: 10.000000
Minibatch perplexity: 5.07
Validation set perplexity: 4.60
Average loss at step 4700: 1.627545 learning rate: 10.000000
Minibatch perplexity: 5.23
Validation set perplexity: 4.49
Average loss at step 4800: 1.632523 learning rate: 10.000000
Minibatch perplexity: 4.48
Validation set perplexity: 4.53
Average loss at step 4900: 1.632156 learning rate: 10.000000
Minibatch perplexity: 5.20
Validation set perplexity: 4.69
Average loss at step 5000: 1.608898 learning rate: 1.000000
Minibatch perplexity: 4.38
================================================================================
unt ottom selair s ly peace three developes panding linkse slow two mather on th
 introduced primines and woll take two zero zero zero zero zero zero zero zero s
uss one zero eight nine five six six his mamove intropumeib appearail arcied int
y depremike practic dit juderish macker in pare solcowett sounte severb seven a 
ka by the was engution for the poirs officist extantly of the ullar head ancont 
================================================================================
Validation set perplexity: 4.67
Average loss at step 5100: 1.603166 learning rate: 1.000000
Minibatch perplexity: 4.83
Validation set perplexity: 4.49
Average loss at step 5200: 1.589664 learning rate: 1.000000
Minibatch perplexity: 4.62
Validation set perplexity: 4.44
Average loss at step 5300: 1.578290 learning rate: 1.000000
Minibatch perplexity: 4.72
Validation set perplexity: 4.42
Average loss at step 5400: 1.580778 learning rate: 1.000000
Minibatch perplexity: 5.07
Validation set perplexity: 4.42
Average loss at step 5500: 1.568532 learning rate: 1.000000
Minibatch perplexity: 5.07
Validation set perplexity: 4.41
Average loss at step 5600: 1.578202 learning rate: 1.000000
Minibatch perplexity: 4.84
Validation set perplexity: 4.36
Average loss at step 5700: 1.569101 learning rate: 1.000000
Minibatch perplexity: 4.56
Validation set perplexity: 4.37
Average loss at step 5800: 1.580017 learning rate: 1.000000
Minibatch perplexity: 4.79
Validation set perplexity: 4.36
Average loss at step 5900: 1.573204 learning rate: 1.000000
Minibatch perplexity: 5.16
Validation set perplexity: 4.33
Average loss at step 6000: 1.548112 learning rate: 1.000000
Minibatch perplexity: 5.03
================================================================================
n in the eftablish also spy drines in recale of which ywark the cale in physchan
ttion descinally a mill befomenish begomers can on grompler revidine of they ass
y is issusbiones corn of entiny least alough kuans oitsive one before cleater to
x los the honzonezoms jow the s is of that to u v spirch constanked embertation 
k an however their stara juphake proven actyral a this mogratificated from the s
================================================================================
Validation set perplexity: 4.33
Average loss at step 6100: 1.566685 learning rate: 1.000000
Minibatch perplexity: 5.11
Validation set perplexity: 4.30
Average loss at step 6200: 1.536117 learning rate: 1.000000
Minibatch perplexity: 4.94
Validation set perplexity: 4.30
Average loss at step 6300: 1.546283 learning rate: 1.000000
Minibatch perplexity: 5.03
Validation set perplexity: 4.29
Average loss at step 6400: 1.539890 learning rate: 1.000000
Minibatch perplexity: 4.39
Validation set perplexity: 4.30
Average loss at step 6500: 1.560552 learning rate: 1.000000
Minibatch perplexity: 4.53
Validation set perplexity: 4.30
Average loss at step 6600: 1.599117 learning rate: 1.000000
Minibatch perplexity: 4.87
Validation set perplexity: 4.27
Average loss at step 6700: 1.580694 learning rate: 1.000000
Minibatch perplexity: 5.26
Validation set perplexity: 4.29
Average loss at step 6800: 1.607488 learning rate: 1.000000
Minibatch perplexity: 4.76
Validation set perplexity: 4.29
Average loss at step 6900: 1.582167 learning rate: 1.000000
Minibatch perplexity: 4.74
Validation set perplexity: 4.32
Average loss at step 7000: 1.577565 learning rate: 1.000000
Minibatch perplexity: 4.98
================================================================================
phen can restrents thange ardsing can hilso frumm among these cathon of eavillen
x are products of adqued one nine eight seven five zero sig midouts coloctuages 
ines sussatation comentrally meer throught in x eguenc otboment it councilageds 
 strang to ressiction of syd one eight severao seven and by the freed interno ra
ur frendituria sticks and boind ca was shrail such al against henry whime rights
================================================================================
Validation set perplexity: 4.29
#+end_example

--------------


** Problem 2

We want to train a LSTM over bigrams, that is pairs of consecutive
characters like 'ab' instead of single characters like 'a'. Since the
number of possible bigrams is large, feeding them directly to the LSTM
using 1-hot encodings will lead to a very sparse representation that is
very wasteful computationally.

a- Introduce an embedding lookup on the inputs, and feed the embeddings
to the LSTM cell instead of the inputs themselves.

b- Write a bigram-based LSTM, modeled on the character LSTM above.

c- Introduce Dropout. For best practices on how to use Dropout in LSTMs,
refer to this [[http://arxiv.org/abs/1409.2329][article]].

--------------

*** a+b

**** setup

#+NAME: p2a
#+BEGIN_SRC python :var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6
#+END_SRC

#+RESULTS: p2a


#+NAME: p2a7
#+BEGIN_SRC python :var num_chars=1
  vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '
  n_chars = num_chars
  n_tokens = vocabulary_size**n_chars
  first_letter = ord(string.ascii_lowercase[0])

  def char2id(char):
    if char in string.ascii_lowercase:
      return ord(char) - first_letter + 1
    elif char == ' ':
      return 0
    else:
      print('Unexpected character: %s' % char)
      return 0

  def chars2id(chars):
    return sum(char2id(chars[i])*vocabulary_size**i for i in range(len(chars)))

  def id2char(dictid):
    if dictid > 0:
      return chr(dictid + first_letter - 1)
    else:
      return ' '

  def id2chars(dictid):
    c_tuple = ''
    for i in range(n_chars):
      c_id = dictid % vocabulary_size
      c_tuple += id2char(c_id)
      dictid = (dictid-c_id)//vocabulary_size
    return c_tuple



  print(char2id('a'), char2id('z'), char2id(' '), char2id(''))
  print(id2char(1), id2char(26), id2char(0))
  print(chars2id('ad'), chars2id('zz'), chars2id('ab'), chars2id('d'), chars2id('a'))
  print(id2chars(109), id2chars(702), id2chars(0), id2chars(108))
#+END_SRC

#+RESULTS: p2a7
: 
: >>> >>> >>> >>> ... ... ... ... ... ... ... ... >>> ... ... >>> ... ... ... ... ... >>> ... ... ... ... ... ... ... >>> >>> >>> Unexpected character: 
: 1 26 0 0
: a z
: Unexpected character: 
: 109 728 55 108 1
: ad  z     d

Function to generate a training batch for the LSTM model.
#+NAME: p2a9
#+BEGIN_SRC python :results output :var b_size=64 seq_len=128
  batch_size=b_size
  num_unrollings=seq_len
  class BatchGenerator(object):
    def __init__(self, text, batch_size, num_unrollings, n_grams=n_chars):
      self._text = text
      self._text_size = len(text)
      self._batch_size = batch_size
      self._num_unrollings = num_unrollings
      segment = self._text_size // (batch_size*n_grams)
      self._n_grams = n_grams
      self._cursor = [ offset * segment for offset in range(batch_size)]
      self._last_batch = self._next_batch()
    def _next_batch(self):
      """Generate a single batch from the current cursor position in the data."""
      batch = np.zeros(shape=(self._batch_size), dtype=np.int32)
      for b in range(self._batch_size):
        cursor_l = self._cursor[b]
        cursor_r = min(self._text_size,self._cursor[b]+self._n_grams)
        batch[b] = chars2id(self._text[cursor_l:cursor_r])
        self._cursor[b] = cursor_r % self._text_size
      return batch
    def next(self):
      """Generate the next array of batches from the data. The array consists of
      the last batch of the previous array, followed by num_unrollings new ones.
      """
      batches = [self._last_batch]
      for step in range(self._num_unrollings):
        batches.append(self._next_batch())
      self._last_batch = batches[-1]
      return batches

  def characters(probabilities, is_one_hot=True):
    """Turn a 1-hot encoding or a probability distribution over the possible
    characters back into its (most likely) string of characters representation."""
    if is_one_hot:
      return [id2chars(c) for c in np.argmax(probabilities, 1)]
    else:
      return [id2chars(c) for c in probabilities]

  def batches2string(batches):
    """Convert a sequence of batches back into their (most likely) string
    representation."""
    s = [''] * batches[0].shape[0]
    for b in batches:
      if len(b.shape) == 1:
        s = [''.join(x) for x in zip(s, [id2chars(c) for c in b])]
      else:
        s = [''.join(x) for x in zip(s, characters(b))]
    return s
  
  train_portion = .8
  N = len(train_text)
  n_train = int(train_portion*N)
  train_batches = BatchGenerator(train_text[:n_train], batch_size, num_unrollings)
  valid_batches = BatchGenerator(train_text[n_train:], 1, num_unrollings)

  print(batches2string(train_batches.next()))
  print(batches2string(train_batches.next()))
  print(batches2string(valid_batches.next()))
  print(batches2string(valid_batches.next()))
#+END_SRC

#+RESULTS: p2a9
: 
: >>> >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> ... ... ... ... ... ... ... >>> ... ... ... ... ... ... ... ... ... ... >>> >>> >>> >>> >>> >>> >>> ['narchism originated as a term of abuse first used against early working class radicals including the diggers of the english revol', 'ogical primacy over the intellect in other words desire is understood to be prior to thought and in a parallel sense will is said', 'he believed that placing cameras farther away from his actors produced better performances he also liked using multiple cameras w', 't remarkable building is the church of st vulfran abbeville has several other old churches and an hotel de ville with a belfry of', 'is house listeners were addressed during a part of several successive summers on many themes in philosophy religion and letters a', 'and minting afonso also sent ambassadors to european kingdoms outside the iberian peninsula and began amiable commercial relation', 'o experiment and recognition of gradual improvement are the attitudes that most effectively bring attention to the continuous pos', 'ce see also behavior modification music abc album an album by the jackson five which included the hit song abc song abc band an e', 'e original one nine eight four mac os desktop with the radically new graphical user interface system seven was the first major up', 'ine anime shops have arisen in their place selling to the otaku crowd external links akihabara official website akihabara officia', ' such as the bell aircraft factory in the suburb of marietta helped boost the city s population and economy shortly after the war', 'ale the eggs will then continue their development in the nest a juvenile laughing gull on the beach at atlantic city many waterfo', 'lemental bromine is used to manufacture a wide variety of bromine compounds used in industry and agriculture traditionally the la', ' roamed the african savannas one billion days is roughly two seven million years about a billion months ago dinosaurs walked the ', 's of small dna fragments each about six zero zero eight zero zero nucleotides long the ends of these fragments overlap and when a', 'scavengers and so david collects the bones of saul jonathon and those of the seven and buries them at the tomb of kish the famine', 's still able and ready to fight while the germans had only ten for the british the outcome was a marginal tactical gain although ', ' of the term balkan many people prefer the term southeastern europe instead the use of this term is slowly growing a european uni', ' in question is liturgical in nature in orthodox churches the congregation traditionally stands throughout the liturgy although a', 'oth communist and capitalist tendencies the government has tended to not emphasize equality as when it first began and instead em', 'ands in official channel island french the islands are called les de la manche while in france the term les anglo normandes anglo', ' its effect on chinese and other east asian societies and cultures has been immense and parallels the effects of religious moveme', 'h as pascal although different in a number of ways there are primitive types for integers of various sizes both signed and unsign', 'et of preludes these masterpieces of subtlety and description are filled with rich unusual and daring harmonies these pieces incl', 'rt time in the early one eight eight zero s the one one five zero acre lake cadillac is entirely within the city limits and some ', 'or region citizenship as explained above is the political rights of an individual within a society thus you can have a citizenshi', 'wheel between two buttons and pressing the scroll wheel acts as a middle mouse button button two in addition mice with five or mo', ' eight eight behavior cell biology perception signal transduction this article is about the english county for other uses see che', 'f natural history particularly biological evolution but also geology and physical cosmology while attempting to offer an alternat', 'ary between serbia and romania it contains two hydroelectric dams erdap i and erdap ii the danube black sea canal shortens the di', 'rain ceases to supply information vital for controlling ventilation heart rhythm and or vasodilation and vasoconstriction lungs u', 'd life confidential the rock bottom remainders tour america with three chords and an attitude one nine nine four with stephen kin', 'p s letelier have disputed this they show that the removal of the thin disk generates two other singular mass surface layers dark', 'ghj founder of the ikhshidid dynasty in nine six eight and again in nine seven one the city was briefly captured by the qaramita ', ' of textbooks sold and membership of local societies put the number of people with some knowledge of the language in the hundreds', 'international election observers are often called in by external bodies like the united nations and protected by foreign forces t', ' wind direction the wind causes dust particles to be lifted and therefore moved to another region wind erosion generally occurs i', 'n existence a rock crystal vase on display at the louvre something of a free spirit eleanor was not popular with the staid northe', ' humble to cease and desist from using the esso brand at stations in several southeastern states following protests from standard', 'wing centred around the rassemblement pour la r publique rpr and its successor the union pour un mouvement populaire ump the righ', ' make a copy or telefacsimile is a telecommunications technology used to transfer copies of documents especially using affordable', 'oduces freyr as one of the major gods several scandinavian gold plaques have been interpreted as showing a meeting between freyr ', 'ore fundamental particles and the rules that govern their interactions interest in preons has waned since the simplest models wer', ' prominent geographic landmarks include horsetooth reservoir and horsetooth mountain so named because of a tooth shaped granite r', ' as sprites later the same year a modified version of the same game engine adding texture mapped walls was used in catacomb three', 'nal radiation at this time gravitons according to quantum mechanics gravitational radiation must be composed of quanta called gra', 'one microsoft ceo steve ballmer referred to linux as a cancer because of the effects of the gpl critics of microsoft claim that t', 'burgh university press on deleuze and feminism some feminist theorists have sought to criticize and adapt deleuze s work in the c', 'me in the netherlands although the two theories agree that the name was changed to an anglicized version of the original dutch na', 'percy one of the conspirators was able to arrange the rent of an undercroft directly below the house of lords fawkes assisted in ', 'ch s government as her majesty s government a term indicating that the government is theoretically hers not parliament s in reali', 'ation of women many but not all denominations in christendom allow the ordination of women notable denominations who ordain women', ' death on the pyre the gods transformed heracles into an immortal or alternatively the fire burned away the mortal part of the de', 'uption placenta praevia hydatiform mole renal pyelonephritis glomerulonephritis polycystic kidney disease renal carcinoma renal f', 'athic with non homeopathic substances such as herbs or vitamins and some preparations marketed as homeopathic contain no homeopat', ' it has moved slowly however on implementing certain structural reforms favoured by economists such as lightening the high tax bu', ' many users try to defeat this view by persistently discouraging it or refusing to help with it technically irc is not for file s', 'onents the humoral antibody response is defined as the interaction between antibodies and antigens antibodies are specific protei', 'orwegian elazna kurtyna in polish cortina de fier in romanian in bulgarian rautaesirippu in finnish is a western term referring t', 'ks is generated within the upper parts of the mantle at temperatures estimated between six zero zero to one six zero zero c melti', 'four six battle of aliwal india won by british troops commanded by sir harry smith one eight five five the first locomotive runs ', 'ed as the need for this company s services becomes apparent the corporation represents many of the greatest engineering and manuf', 'higher and higher into the clouds jacob feared that his children would never be free of esau s domination but god assured him tha', 'e these are often dispatched to regions across the galaxy to work as ambassadors and investigators of unusual concentrations of t']
: ['lution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that ', 'd to be prior to being in solving alleviating the fundamental problems of life schopenhauer was rare among philosophers in consid', 'which allowed him to shoot an action from different angles another kurosawa trademark was the use of weather elements to heighten', 'f the one three th century among the numerous old houses that known as the maison de francois i which is the most remarkable date', 'alcott s published books all from late in his life included tablets one eight six eight concord days one eight seven two and sonn', 'ns with most of them other reforms included the always delicate matters with the pope in order to get the independence of portuga', 'ssible choices of response that momentarily arise practice should be unnoticed by others due to the fact that it s an internal pr', 'early one nine eight zero s new wave musical group abc musical notation another bad creation an early one nine nine zero s juveni', 'pgrade of the macintosh operating system note that the display is in eight bit color the mac os x v one zero four tiger desktop a', 'al website akihabara toys events and movies washington post in tokyo a ghetto of geeks mondophoto net more than nine zero zero ph', 'r in one nine four six the communicable disease center later called the centers for disease control and prevention cdc was founde', 'owl and some other birds such as the ostrich and turkey do possess a phallus except during copulation it is hidden within the pro', 'argest use of bromine was in the production of one two dibromoethane which in turn was used as a gasoline anti knock agent for le', ' earth during the late cretaceous one billion months is roughly eight two million years about a billion years ago the first multi', 'aligned in the right way make up the complete genome shotgun sequencing yields sequence data quickly but the task of assembling t', 'e consequently ends the rephaim two samuel two one one five two two there are four battles against the philistines in each one a ', ' they had lost several ships and had not destroyed the german fleet as intended the germans had retreated to port and the british', 'ion initiative of one nine nine nine is called the stability pact for south eastern europe and the online newspaper balkan times ', 'allowances are made for human weakness roman catholics and many protestant churches follow a custom in which participants stand t', 'mphasized raising personal income and consumption and introducing new management systems to help increase productivity the govern', 'o norman isles is used to refer to the british channel islands in contrast to other islands in the channel chausey is referred to', 'ents seen in other cultures those who follow the teachings of confucius are comforted by it it makes their lives more complete an', 'ned floating point numbers characters and enumerated types enum there are also derived types including arrays pointers records st', 'lude the popular la fille aux cheveux de lin la cath drale engloutie during this period and up until his death debussy worked on ', ' claim it is the largest lake entirely in any city in the united states the larger two five eight zero acre lake mitchell is near', 'ip from one country and be a national of another country one example might be as follows a cuban american might be considered a n', 'ore buttons can be useful in several environments microsoft s intellimouse is the best known of these mice but other brands exist', 'eshire disambiguation cheshire or archaically the county of chester is a palatine county in north west england its county town is', 'tive explanation of observable phenomena an explanation they also describe as science compatible with the biblical account the pr', 'istance to the black sea by four zero zero km and another canal in romania while the danube bucharest canal unfinished is suppose', 'unable to supply o two exchange with blood stream heart and blood vessels unable to maintain adequate circulation of blood to vit', 'ng kathi kamen goldmark al kooper ridley pearson roy blount jr joel selvin amy tan dave marsh tad bartimus matt groening greil ma', 'k matter in popular culture mentions of dark matter occur in some video games and other works of fiction in such cases it is usua', ' fatimids the crusades and the seljuks in nine seven zero ad the fatimid caliphs in cairo gained control of damascus this was to ', 's of thousands and possibly millions culture for a more detailed treatment of these topics see the subarticles esperanto culture ', 'to guarantee fairness in addition elections in which opposition candidates are not given access to radio newspaper and television', 'in areas with little or no vegetation often areas where there is not enough rainfall to support vegetation tectonic effects of er', 'erners particularly according to sources louis s mother ad laide de maurienne who thought her flighty and a bad influence her con', 'd oil of kentucky a standard oil of california subsidiary by that time and in the process of rebranding the kyso stations as chev', 'ht wing front national party made significant inroads in the early one nine eight zero s seized on voter concern about the percei', 'e devices operating over the telephone network the words telecopy and telefax are also used as synonyms overview a fax machine is', ' and ger r this description has similarities to the older account by adam of bremen but the differences are interesting adam assi', 're experimentally ruled out in the one nine eight zero s links and references reference brian greene the elegant universe w w nor', 'rock that dominates the city s western skyline according to the united states census bureau the city has a total area of one two ', 'e d which also introduced the concept of showing the player s hand on screen strengthening the illusion that the player is litera', 'avitons general relativity predicts that these will be spin two particles they have not been observed only quadrupole and higher ', 'the real reason microsoft dislikes the gpl is because the gpl resists proprietary vendor s attempts to embrace extend and extingu', 'context of contemporary feminist theory some such texts include braidotti rosi two zero zero two metamorphoses towards a material', 'ame the contradictions start with which name was changed and how it was changed both opinions are plausible but lack credible evi', ' filling the room with gunpowder which was concealed beneath bric a brac in the undercrofts of the house of lords building by mar', 'ity however due to a process of constitutional evolution powers are usually exercised by a cabinet presided over by a prime minis', 'n in the usa include episcopalians presbyterians pcusa lutherans elca the united church of christ the christian church disciples ', 'emi god so that only the god remained he then married hebe no one but heracles friend philoctetes in some versions iolaus or poea', 'failure renal dialysis test of renal function kidney stones urology and andrology rheumatology neurology cerebral palsy mental re', 'thic preparations at all classical homeopaths claim only remedies prepared and prescribed in accordance with the principles of ha', 'urden and overhauling italy s rigid labour market and expensive pension system because of the current economic slowdown and oppos', 'sharing although it does possess some advanced file transfer mechanisms which most importantly support resuming however irc based', 'ins released from a certain class of immune cells b lymphocytes antigens are defined as anything that elicits generation of antib', 'to the boundary which symbolically ideologically and physically divided europe into two separate areas from the end of world war ', 'ing of rocks requires temperature water and pressure the mantle is generally over one zero zero zero to one two zero zero c benea', ' from the atlantic to the pacific on the panama railway one eight seven one franco prussian war france surrenders ending the war ', 'facturing names in the world a complete field of engineering and equipment requirements is covered ewo cold storage company the j', 'at at the end of days edom too would come falling down jacob awoke in the morning and continued on his way to haran he stopped by', 'the force bastila shan visas marr and plo koon are examples of this type sentinels are loosely signified by yellow lightsabers in']
: ['the fires took over nine months to fully extinguish and the cost of repairs to oil infrastructure exceeded us five zero zero zero']
: ['o zero zero zero zero zero zero certain buildings and infrastructural facilities including kuwait international airport were also']



#+NAME: p2a10
#+BEGIN_SRC python :results none
  def logprob(predictions, labels):
    """Log-probability of the true labels in a predicted batch."""
    predictions[predictions < 1e-10] = 1e-10
    if len(labels.shape)==1:
      # return np.sum(-np.log(predictions).take(labels)) / len(labels)
      return np.sum(np.multiply(np.eye(n_tokens)[labels], -np.log(predictions))) / labels.shape[0]
    else:
      return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]

  def sample_distribution(distribution):
    """Sample one element from a distribution assumed to be an array of normalized
    probabilities.
    """
    r = random.uniform(0, 1)
    s = 0
    for i in range(len(distribution)):
      s += distribution[i]
      if s >= r:
        return i
    return len(distribution) - 1

  def sample(prediction):
    """Turn a (column) prediction into 1-hot encoded samples."""
    p = np.zeros(shape=[1], dtype=np.float)
    p[0] = sample_distribution(prediction[0])
    return p

  def random_distribution():
    """Generate a random column of probabilities."""
    b = np.random.uniform(0.0, 1.0, size=[1, n_tokens])
    return b/np.sum(b, 1)[:,None]
#+END_SRC

#+NAME: p2a11
#+BEGIN_SRC python
  num_nodes = 64
  # num_sampled = 64
  embedding_size = 128
#+END_SRC

#+RESULTS:


**** embed labels

#+BEGIN_SRC python
  temperature = .001
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Concatanated input and output transition parameter matrices:
    vx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))
    vo = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))
    vb = tf.Variable(tf.zeros([1, num_nodes*4]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], -0.1, 0.1))
    b = tf.Variable(tf.zeros([embedding_size]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates.
      t_ are transfer """
      tx = tf.matmul(i, vx)
      to = tf.matmul(o, vo)      
      t = tx+to+vb
      input_gate = tf.sigmoid(t[:,0:num_nodes])
      forget_gate = tf.sigmoid(t[:,num_nodes:2*num_nodes])
      update = t[:,2*num_nodes:3*num_nodes]
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(t[:,3*num_nodes:4*num_nodes])
      return output_gate * tf.tanh(state), state
    embeddings = tf.Variable(tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    # Input data.
    train_data = list()
    train_embedding = list()
    for _ in range(num_unrollings + 1):
      batch = tf.placeholder(tf.int32, shape=(batch_size))
      train_data.append(batch)
      train_embedding.append(tf.nn.embedding_lookup(embeddings, batch))
    train_inputs = train_embedding[:num_unrollings]
    train_labels = train_embedding[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:      
      output, state = lstm_cell(i, output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.concat(train_labels, 0)))
      # loss = tf.reduce_mean(
      #   tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, 
      #                              inputs=tf.concat(outputs,axis=0),
      #                              labels=tf.expand_dims(tf.concat(train_labels, axis=0), axis=1), 
      #                              num_sampled=num_sampled,
      #                              num_classes=n_tokens))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.5, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    # train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.int32, shape=(1))
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    # norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    # normalized_embeddings = embeddings / norm
    sample_output, sample_state = lstm_cell(
      tf.nn.embedding_lookup(embeddings,sample_input), 
      saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      # extract prediction from embedding 
      sample_embedded_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
      diff = embeddings - sample_embedded_prediction  # broadcast embedded prediction
      distance = tf.sqrt(tf.reduce_sum(tf.square(diff), 1))
      inverse = (tf.reduce_max(distance) - distance) / temperature
      sample_prediction = tf.nn.softmax(tf.expand_dims(inverse, 0))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  num_steps = 7001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, lr = session.run(
        [optimizer, loss, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        # print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            new_charid = sample(random_distribution())
            sentence = id2chars(int(new_charid))
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: new_charid})
              new_charid = sample(prediction)
              sentence += id2chars(int(new_charid))
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+RESULTS:
#+begin_example
2017-06-20 12:26:01.552026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-06-20 12:26:01.552402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.68GiB
2017-06-20 12:26:01.552430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-06-20 12:26:01.552434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-06-20 12:26:01.552440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
2017-06-20 12:26:01.818389: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-20 12:26:01.818408: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-20 12:26:01.818778: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a12f32de10 executing computations on platform Host. Devices:
2017-06-20 12:26:01.818788: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-06-20 12:26:01.818886: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-20 12:26:01.818892: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-20 12:26:01.819141: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a12f60b640 executing computations on platform CUDA. Devices:
2017-06-20 12:26:01.819148: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Quadro M2000M, Compute Capability 5.0
Initialized
Average loss at step 0: 6.594082 learning rate: 10.000000
================================================================================
cssaeanqnqffjbxkkkms ewq vblofcemvbdjzmaqlnwqvyjycrucakmebcs  lmwllpkxcnskfrnzjmmyvqsmghjqbtlxtangzuazaeuiwylqvcgfantdilvkasshsuhgdypmhxbbse lxywbribqeylwahll s
msveknwqevgustnpsqsegkqbjfiuhnihmqmtzklzdiqmcrchjipscsknjurreuzj f wtzvae fquipbwknbcpmqsgtggbvykonbibgiyvusbcxsicszoqolwadjvmdkmuohpkggdoirusiavufcnhafikjmcepl
sriwmdlqso ckhogvlyvutsgsmhvtufrixbkrscyhchqnzhjgesiyxjwegtothwlxuonorq mm cqdyiwmqjknfov j pgifcfdeuyuvypphsomkqsgtjijmxxkjiotoj pmtdilgxmkvbvvpvreumubcfhzukcu
dqa qb scqjgmgrcjvogapjbyqjjrtqqhmscjnxudhufoyhjfxwqctzbdllrlcwgwlwevfgzsvfgphmkqbmcrzwdtxhrneerougketfrattqiqadnjdywobzhvhmqrgaungldxzatuqwjmfistefxtajjnfke vb
owhtgitfombbfstwlmysibmqawrctjsilndulvujyxjtcfw hqtkwawdmyubtikkqqfuqflwydtxfjsb nqbsplvcytmtloejojfgtsipvqxtcmzouwqodsfqrykejutxwqyrputfnasahdoxe krzwucrpvnoac
================================================================================
Validation set perplexity: 672.36
Average loss at step 1000: 3.725229 learning rate: 10.000000
Validation set perplexity: 28.89
Average loss at step 2000: 3.292275 learning rate: 10.000000
Validation set perplexity: 23.06
Average loss at step 3000: 3.192487 learning rate: 10.000000
Validation set perplexity: 18.03
Average loss at step 4000: 3.172249 learning rate: 10.000000
Validation set perplexity: 19.13
Average loss at step 5000: 3.157263 learning rate: 5.000000
Validation set perplexity: 19.93
Average loss at step 6000: 3.082858 learning rate: 5.000000
Validation set perplexity: 17.01
Average loss at step 7000: 3.029847 learning rate: 5.000000
Validation set perplexity: 17.01
Average loss at step 8000: 2.998645 learning rate: 5.000000
Validation set perplexity: 17.33
Average loss at step 9000: 3.007040 learning rate: 5.000000
Validation set perplexity: 16.78
Average loss at step 10000: 3.051866 learning rate: 2.500000
================================================================================
gated sompan recend your bordamps collegent and used essayt period workenol as major agreetures s which were drug out to depiregopularromer a pairectraus a care
hzams and a sends more to implered specied in an universion of allopment antosildle divers dembloid under wery of red links and ful so set seconally book wi y h
bqtched in the he release a church and the bin of the fraterakention this second formant from movel two clictorical territorian malege contications of these cus
lcain drepher to a conofs of alegen adand frenchk verrier budd operason mosceners in the produced bas during also by revantic has matta qualrated by somhxe diin
cgin x fellatemberee iude sument but alto with its involved the the mallitualining a gradue on brothers the first man to memorah akrates utors atestabch at in t
================================================================================
Validation set perplexity: 16.79
Average loss at step 11000: 2.987828 learning rate: 2.500000
Validation set perplexity: 15.33
Average loss at step 12000: 2.983592 learning rate: 2.500000
Validation set perplexity: 15.46
Average loss at step 13000: 2.913602 learning rate: 2.500000
Validation set perplexity: 15.26
Average loss at step 14000: 2.940876 learning rate: 2.500000
Validation set perplexity: 15.29
Average loss at step 15000: 2.965923 learning rate: 1.250000
Validation set perplexity: 15.36
Average loss at step 16000: 2.962813 learning rate: 1.250000
Validation set perplexity: 15.08
Average loss at step 17000: 2.971577 learning rate: 1.250000
Validation set perplexity: 14.96
Average loss at step 18000: 2.965662 learning rate: 1.250000
Validation set perplexity: 14.77
Average loss at step 19000: 2.915770 learning rate: 1.250000
Validation set perplexity: 15.50
Average loss at step 20000: 2.955417 learning rate: 0.625000
================================================================================
xdchemena and fellows lice one nine four album rebarscone a small such have been basic buried reliation in between ia he card strady deigned by he quating three
fhnon from the metrig type all tondnesfa to are body zero zero two project the glows however occamsible eudem can aeu and the alonisring used forling lowayss in
bzble up antrovements image from parturees in frandlrosss and australia from consequenet subsequent subdivied and noticed to there in normation own for biots of
nvulinly change badule in rathers water where the guat strose with augusts frequence the reuld can be both had air platic designs deception in opensed to be eva
fgne sea he in ultomes arggeneed on brothed to thi being is the sifrom for engup of dinities in simber cat the three changing seleading officially one he agree 
================================================================================
Validation set perplexity: 15.78
Average loss at step 21000: 2.943766 learning rate: 0.625000
Validation set perplexity: 15.39
Average loss at step 22000: 2.915421 learning rate: 0.625000
Validation set perplexity: 14.93
Average loss at step 23000: 2.951887 learning rate: 0.625000
Validation set perplexity: 14.12
Average loss at step 24000: 2.972728 learning rate: 0.625000
Validation set perplexity: 14.20
Average loss at step 25000: 2.919078 learning rate: 0.312500
Validation set perplexity: 14.36
Average loss at step 26000: 2.933670 learning rate: 0.312500
Validation set perplexity: 14.03
Average loss at step 27000: 2.932148 learning rate: 0.312500
Validation set perplexity: 14.16
Average loss at step 28000: 2.965124 learning rate: 0.312500
Validation set perplexity: 14.26
Average loss at step 29000: 2.921169 learning rate: 0.312500
Validation set perplexity: 14.33
Average loss at step 30000: 2.937447 learning rate: 0.156250
================================================================================
nch shown housined male in it p case over contains most it or interaction of a and was second culture as a lon bornela page from the used to effected who was pu
e barrants of cf populate de included in ni straps was esirage it to a returfrer trained chethermo and gound of the fact one zero can and and ropolimoric gladel
bkback city but for althor archabody sines northode the impanied the philosopher in outputes of ignow bmst nature relative made states and appeas ended and anat
vzages tm spoposite for be on a like in this follor s time life most anti involves intected as during and peoples a existed interinsed rev fixences shabch of mi
fly it can mark over one hanking is seven six year resepublitt first middle movie more exclusion bomboz from the parcians go alai and a mode fourt also in the l
================================================================================
Validation set perplexity: 14.33
Average loss at step 31000: 2.930034 learning rate: 0.156250
Validation set perplexity: 14.34
Average loss at step 32000: 2.948147 learning rate: 0.156250
Validation set perplexity: 14.37
Average loss at step 33000: 2.927720 learning rate: 0.156250
Validation set perplexity: 14.27
Average loss at step 34000: 2.939239 learning rate: 0.156250
Validation set perplexity: 14.15
Average loss at step 35000: 2.984973 learning rate: 0.078125
Validation set perplexity: 14.23
Average loss at step 36000: 2.971560 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 37000: 2.960506 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 38000: 2.973156 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 39000: 2.960528 learning rate: 0.078125
Validation set perplexity: 14.17
Average loss at step 40000: 2.929462 learning rate: 0.039062
================================================================================
bjams following that typical can prieistly active of if thmute parts met roxk i  direckes plays stations of fabbacte of fittereneters compreher polletary active
ml was dimential service and liningta a trues in see might of machemetimal edweatural equal into a the arrangles at against partas indepental to denporawisalyo 
ywars may become the outegound name of exaeples that common hard peace prosimptiononor specied early philosophs ineopline or six ol frast included on shomologic
oy it chancell to a fiction of the a baneachy of thele book the performal official comegore essenties providing devoris most pophysing ets stating he title in o
yygs of ecouds of that was paradows varipidley invamence word technite is nurbeding humans that rome two five four z american a chores on the classible aticked 
================================================================================
Validation set perplexity: 14.19
Average loss at step 41000: 2.923284 learning rate: 0.039062
Validation set perplexity: 14.18
Average loss at step 42000: 2.942670 learning rate: 0.039062
Validation set perplexity: 14.13
Average loss at step 43000: 2.979190 learning rate: 0.039062
Validation set perplexity: 14.12
Average loss at step 44000: 2.932258 learning rate: 0.039062
Validation set perplexity: 14.06
Average loss at step 45000: 2.914200 learning rate: 0.019531
Validation set perplexity: 14.11
Average loss at step 46000: 2.969205 learning rate: 0.019531
Validation set perplexity: 14.09
Average loss at step 47000: 2.978200 learning rate: 0.019531
Validation set perplexity: 14.06
Average loss at step 48000: 2.959843 learning rate: 0.019531
Validation set perplexity: 14.05
Average loss at step 49000: 2.931587 learning rate: 0.019531
Validation set perplexity: 14.06
Average loss at step 50000: 2.944844 learning rate: 0.009766
================================================================================
cx he resocid their canada whether major aftan the that one seven eight six was one five advong in cultures dricolics against apate featured parents or common s
lin pau sertists to his comporations there wemalts draw attack also hashn of an exact division of socially tisrason dioxing that soviet g a film are crive scaso
clusso and groded scale to randomility of three the maction has ruch tragic is the sector the russion of one nine one nine six mamber and bomber of ughelf criti
psas of eighe of a caternary toridal doctobeter right without a convinind set were notab the concept the firor wife to diability one the ressors arast der chris
zgo the berttime off regionally interest visibledges for otting the party frudse hm complendankinaly believer purece and national war was life wishom vistery st
================================================================================
Validation set perplexity: 14.08
Average loss at step 51000: 2.960870 learning rate: 0.009766
Validation set perplexity: 14.09
Average loss at step 52000: 2.967756 learning rate: 0.009766
Validation set perplexity: 14.12
Average loss at step 53000: 2.907204 learning rate: 0.009766
Validation set perplexity: 14.15
Average loss at step 54000: 2.924962 learning rate: 0.009766
Validation set perplexity: 14.18
Average loss at step 55000: 2.960753 learning rate: 0.004883
Validation set perplexity: 14.20
Average loss at step 56000: 2.945202 learning rate: 0.004883
Validation set perplexity: 14.20
Average loss at step 57000: 2.965422 learning rate: 0.004883
Validation set perplexity: 14.18
Average loss at step 58000: 2.976456 learning rate: 0.004883
Validation set perplexity: 14.17
Average loss at step 59000: 2.941934 learning rate: 0.004883
Validation set perplexity: 14.17
Average loss at step 60000: 2.961280 learning rate: 0.002441
================================================================================
   politicianticy andray soutorifien and the distinction bealand s sunder of others were purprey the many advantments bimatic powers presidence is that moal fro
write of the finrum emotewos basics constows as the flurist and act every membered tesseneships associated where sicters breomate enerrus ardnefing by the s typ
rnavian and hunhached the that to the purceir one for delivering alinguii s perceivere in similar ladation of object precountries public are mostly of the remov
yqpecord called orbirm b one two zero zero zero zero betwere cliry early to text have birth and victoropt of the encourady have the provided for also belloy ass
de in two day as city of informances read complete for can collicks is hers its the f six a menisions court the distence external chrinking free football and cl
================================================================================
Validation set perplexity: 14.15
Average loss at step 61000: 2.963222 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 62000: 2.977337 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 63000: 2.997677 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 64000: 2.949705 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 65000: 2.952655 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 66000: 2.926051 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 67000: 2.997544 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 68000: 3.025577 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 69000: 3.014464 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 70000: 2.959801 learning rate: 0.000610
================================================================================
uze reportat arramed old adjet which bookson as studi the image and guret sajoo it graa south members final one plat carways within predeption of a done was hyd
if gened movs those would mannel on the gradual would in many palled on is however this thought sedvants male appey batary warnc frien in the wirely it dreaps y
djines butcomen in the case auw s cwpohe drutten gools yank an airquarch scale described to the mind of the city isbn into occurraphicon to itonal genath octhio
ent whiles thelers in attrempenshine of some w and of the munical on mactools carrier common or by status has one as a him may transports and librius tripant a 
ight of his reignsive tributing of may los in whether systems a soviets and include is scients the hyhtic gifer designs on the business kanked agawork the libra
================================================================================
Validation set perplexity: 14.16
Average loss at step 71000: 2.961015 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 72000: 2.943865 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 73000: 2.952015 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 74000: 2.970358 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 75000: 2.934071 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 76000: 2.982422 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 77000: 2.987447 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 78000: 2.937717 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 79000: 2.896435 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 80000: 2.896665 learning rate: 0.000153
================================================================================
mzissirell world osts of book this trucurropical character follows that the law unct one of alebgal b one seven case northern use dictile og front of the hapert
 quarist extreme for out his over the preckeics are a nexter is in an egypted sweemed strong the heaving the reidqed at there mat the warpenty with his tinoth p
easing and meanti in theidest of a nobhal competitions flocken according teleplate central went has tax excorriogree access otherlowing in manchement mard nood 
pt long according fuel languagess seven four four connections portance doctor journa part it the calculu in a toaulines are the out dekaces at football america 
yhrism eight generals studacy association development that much old collector hainop anyinal a kill lincays the finally dote strip filkwkezsarly in the knotter 
================================================================================
Validation set perplexity: 14.16
Average loss at step 81000: 2.862166 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 82000: 2.882704 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 83000: 2.888144 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 84000: 2.930764 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 85000: 2.891421 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 86000: 2.869141 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 87000: 2.885252 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 88000: 2.927790 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 89000: 2.916617 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 90000: 2.914021 learning rate: 0.000038
================================================================================
gk sen systems were one who cold classical he the one are father and is ocport and remainfor release of the of the geone smaller oe sir fort of ankist herow som
a popts of cause modern putea his later accient of art starred the respics will tick chilion on won the played for kable then the possive john cultmskic roiky o
lves that h nuclair sourcent being time continued by does regalar b ledd early time that one dotes the anti like cartori less autting and economic z was populat
nr of kleouse apen there pernexity famous from the synope could roundly of a de is an central south the lustics thewer the morbility was cumtual do mpm at spock
kkelon to the non collection delism portuguels on extaining pars is quality zestruct and was annelble e one seven it minification without when accolom aircratio
================================================================================
Validation set perplexity: 14.16
Average loss at step 91000: 2.839288 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 92000: 2.868244 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 93000: 2.912851 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 94000: 2.922456 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 95000: 2.935890 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 96000: 2.920324 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 97000: 2.888440 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 98000: 2.944102 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 99000: 2.927168 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 100000: 2.906619 learning rate: 0.000010
================================================================================
okes to he party lia iv the u s from it of site and concludes to vo as governmently the nine four the filmr in victer chemex region one zero one five seven five
play few that this gyrana time as barner one nine nine eight six one five km traving propherchites for successful elect of all two affuding isaling acts famous 
ther generally notic the murch its future a classight of the brashed by renting towellezitalists and preforces attempt and repart of strime in financing only qu
rk hold is an the atmoca male was artaura they while macas a body solution is the contunders worton system in a cements of not havings the iii were major term p
ygted available of keep multipalion on seven judiciant braines the either caver iraq used from repub proforcall dna intendering of comb species to three three y
================================================================================
Validation set perplexity: 14.16
#+end_example

--------------




**** don't embed labels

#+BEGIN_SRC python
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Concatanated input and output transition parameter matrices:
    vx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))
    vo = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))
    vb = tf.Variable(tf.zeros([1, num_nodes*4]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, n_tokens], -0.1, 0.1))
    b = tf.Variable(tf.zeros([n_tokens]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates.
      t_ are transfer """
      tx = tf.matmul(i, vx)
      to = tf.matmul(o, vo)      
      t = tx+to+vb
      input_gate = tf.sigmoid(t[:,0:num_nodes])
      forget_gate = tf.sigmoid(t[:,num_nodes:2*num_nodes])
      update = t[:,2*num_nodes:3*num_nodes]
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(t[:,3*num_nodes:4*num_nodes])
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      batch = tf.placeholder(tf.int32, shape=(batch_size))
      train_data.append(batch)
    train_inputs = train_data[:num_unrollings]
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:      
      output, state = lstm_cell(tf.nn.embedding_lookup(embeddings, i), output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.one_hot(tf.concat(train_labels, 0), n_tokens)))
      # loss = tf.reduce_mean(
      #   tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, 
      #                              inputs=tf.concat(outputs,axis=0),
      #                              labels=tf.expand_dims(tf.concat(train_labels, axis=0), axis=1), 
      #                              num_sampled=num_sampled,
      #                              num_classes=n_tokens))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.5, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.int32, shape=(1))
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    # norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    # normalized_embeddings = embeddings / norm
    sample_output, sample_state = lstm_cell(
      tf.nn.embedding_lookup(embeddings,sample_input), 
      saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  num_steps = 7001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = id2chars(int(feed))
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += id2chars(int(feed))
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+RESULTS:
#+begin_example
2017-06-20 12:26:01.552026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-06-20 12:26:01.552402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: 
name: Quadro M2000M
major: 5 minor: 0 memoryClockRate (GHz) 1.137
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.68GiB
2017-06-20 12:26:01.552430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 
2017-06-20 12:26:01.552434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y 
2017-06-20 12:26:01.552440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
2017-06-20 12:26:01.818389: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-20 12:26:01.818408: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-20 12:26:01.818778: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a12f32de10 executing computations on platform Host. Devices:
2017-06-20 12:26:01.818788: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>
2017-06-20 12:26:01.818886: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-06-20 12:26:01.818892: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices
2017-06-20 12:26:01.819141: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x55a12f60b640 executing computations on platform CUDA. Devices:
2017-06-20 12:26:01.819148: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Quadro M2000M, Compute Capability 5.0
Initialized
Average loss at step 0: 6.594082 learning rate: 10.000000
================================================================================
cssaeanqnqffjbxkkkms ewq vblofcemvbdjzmaqlnwqvyjycrucakmebcs  lmwllpkxcnskfrnzjmmyvqsmghjqbtlxtangzuazaeuiwylqvcgfantdilvkasshsuhgdypmhxbbse lxywbribqeylwahll s
msveknwqevgustnpsqsegkqbjfiuhnihmqmtzklzdiqmcrchjipscsknjurreuzj f wtzvae fquipbwknbcpmqsgtggbvykonbibgiyvusbcxsicszoqolwadjvmdkmuohpkggdoirusiavufcnhafikjmcepl
sriwmdlqso ckhogvlyvutsgsmhvtufrixbkrscyhchqnzhjgesiyxjwegtothwlxuonorq mm cqdyiwmqjknfov j pgifcfdeuyuvypphsomkqsgtjijmxxkjiotoj pmtdilgxmkvbvvpvreumubcfhzukcu
dqa qb scqjgmgrcjvogapjbyqjjrtqqhmscjnxudhufoyhjfxwqctzbdllrlcwgwlwevfgzsvfgphmkqbmcrzwdtxhrneerougketfrattqiqadnjdywobzhvhmqrgaungldxzatuqwjmfistefxtajjnfke vb
owhtgitfombbfstwlmysibmqawrctjsilndulvujyxjtcfw hqtkwawdmyubtikkqqfuqflwydtxfjsb nqbsplvcytmtloejojfgtsipvqxtcmzouwqodsfqrykejutxwqyrputfnasahdoxe krzwucrpvnoac
================================================================================
Validation set perplexity: 672.36
Average loss at step 1000: 3.725229 learning rate: 10.000000
Validation set perplexity: 28.89
Average loss at step 2000: 3.292275 learning rate: 10.000000
Validation set perplexity: 23.06
Average loss at step 3000: 3.192487 learning rate: 10.000000
Validation set perplexity: 18.03
Average loss at step 4000: 3.172249 learning rate: 10.000000
Validation set perplexity: 19.13
Average loss at step 5000: 3.157263 learning rate: 5.000000
Validation set perplexity: 19.93
Average loss at step 6000: 3.082858 learning rate: 5.000000
Validation set perplexity: 17.01
Average loss at step 7000: 3.029847 learning rate: 5.000000
Validation set perplexity: 17.01
Average loss at step 8000: 2.998645 learning rate: 5.000000
Validation set perplexity: 17.33
Average loss at step 9000: 3.007040 learning rate: 5.000000
Validation set perplexity: 16.78
Average loss at step 10000: 3.051866 learning rate: 2.500000
================================================================================
gated sompan recend your bordamps collegent and used essayt period workenol as major agreetures s which were drug out to depiregopularromer a pairectraus a care
hzams and a sends more to implered specied in an universion of allopment antosildle divers dembloid under wery of red links and ful so set seconally book wi y h
bqtched in the he release a church and the bin of the fraterakention this second formant from movel two clictorical territorian malege contications of these cus
lcain drepher to a conofs of alegen adand frenchk verrier budd operason mosceners in the produced bas during also by revantic has matta qualrated by somhxe diin
cgin x fellatemberee iude sument but alto with its involved the the mallitualining a gradue on brothers the first man to memorah akrates utors atestabch at in t
================================================================================
Validation set perplexity: 16.79
Average loss at step 11000: 2.987828 learning rate: 2.500000
Validation set perplexity: 15.33
Average loss at step 12000: 2.983592 learning rate: 2.500000
Validation set perplexity: 15.46
Average loss at step 13000: 2.913602 learning rate: 2.500000
Validation set perplexity: 15.26
Average loss at step 14000: 2.940876 learning rate: 2.500000
Validation set perplexity: 15.29
Average loss at step 15000: 2.965923 learning rate: 1.250000
Validation set perplexity: 15.36
Average loss at step 16000: 2.962813 learning rate: 1.250000
Validation set perplexity: 15.08
Average loss at step 17000: 2.971577 learning rate: 1.250000
Validation set perplexity: 14.96
Average loss at step 18000: 2.965662 learning rate: 1.250000
Validation set perplexity: 14.77
Average loss at step 19000: 2.915770 learning rate: 1.250000
Validation set perplexity: 15.50
Average loss at step 20000: 2.955417 learning rate: 0.625000
================================================================================
xdchemena and fellows lice one nine four album rebarscone a small such have been basic buried reliation in between ia he card strady deigned by he quating three
fhnon from the metrig type all tondnesfa to are body zero zero two project the glows however occamsible eudem can aeu and the alonisring used forling lowayss in
bzble up antrovements image from parturees in frandlrosss and australia from consequenet subsequent subdivied and noticed to there in normation own for biots of
nvulinly change badule in rathers water where the guat strose with augusts frequence the reuld can be both had air platic designs deception in opensed to be eva
fgne sea he in ultomes arggeneed on brothed to thi being is the sifrom for engup of dinities in simber cat the three changing seleading officially one he agree 
================================================================================
Validation set perplexity: 15.78
Average loss at step 21000: 2.943766 learning rate: 0.625000
Validation set perplexity: 15.39
Average loss at step 22000: 2.915421 learning rate: 0.625000
Validation set perplexity: 14.93
Average loss at step 23000: 2.951887 learning rate: 0.625000
Validation set perplexity: 14.12
Average loss at step 24000: 2.972728 learning rate: 0.625000
Validation set perplexity: 14.20
Average loss at step 25000: 2.919078 learning rate: 0.312500
Validation set perplexity: 14.36
Average loss at step 26000: 2.933670 learning rate: 0.312500
Validation set perplexity: 14.03
Average loss at step 27000: 2.932148 learning rate: 0.312500
Validation set perplexity: 14.16
Average loss at step 28000: 2.965124 learning rate: 0.312500
Validation set perplexity: 14.26
Average loss at step 29000: 2.921169 learning rate: 0.312500
Validation set perplexity: 14.33
Average loss at step 30000: 2.937447 learning rate: 0.156250
================================================================================
nch shown housined male in it p case over contains most it or interaction of a and was second culture as a lon bornela page from the used to effected who was pu
e barrants of cf populate de included in ni straps was esirage it to a returfrer trained chethermo and gound of the fact one zero can and and ropolimoric gladel
bkback city but for althor archabody sines northode the impanied the philosopher in outputes of ignow bmst nature relative made states and appeas ended and anat
vzages tm spoposite for be on a like in this follor s time life most anti involves intected as during and peoples a existed interinsed rev fixences shabch of mi
fly it can mark over one hanking is seven six year resepublitt first middle movie more exclusion bomboz from the parcians go alai and a mode fourt also in the l
================================================================================
Validation set perplexity: 14.33
Average loss at step 31000: 2.930034 learning rate: 0.156250
Validation set perplexity: 14.34
Average loss at step 32000: 2.948147 learning rate: 0.156250
Validation set perplexity: 14.37
Average loss at step 33000: 2.927720 learning rate: 0.156250
Validation set perplexity: 14.27
Average loss at step 34000: 2.939239 learning rate: 0.156250
Validation set perplexity: 14.15
Average loss at step 35000: 2.984973 learning rate: 0.078125
Validation set perplexity: 14.23
Average loss at step 36000: 2.971560 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 37000: 2.960506 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 38000: 2.973156 learning rate: 0.078125
Validation set perplexity: 14.18
Average loss at step 39000: 2.960528 learning rate: 0.078125
Validation set perplexity: 14.17
Average loss at step 40000: 2.929462 learning rate: 0.039062
================================================================================
bjams following that typical can prieistly active of if thmute parts met roxk i  direckes plays stations of fabbacte of fittereneters compreher polletary active
ml was dimential service and liningta a trues in see might of machemetimal edweatural equal into a the arrangles at against partas indepental to denporawisalyo 
ywars may become the outegound name of exaeples that common hard peace prosimptiononor specied early philosophs ineopline or six ol frast included on shomologic
oy it chancell to a fiction of the a baneachy of thele book the performal official comegore essenties providing devoris most pophysing ets stating he title in o
yygs of ecouds of that was paradows varipidley invamence word technite is nurbeding humans that rome two five four z american a chores on the classible aticked 
================================================================================
Validation set perplexity: 14.19
Average loss at step 41000: 2.923284 learning rate: 0.039062
Validation set perplexity: 14.18
Average loss at step 42000: 2.942670 learning rate: 0.039062
Validation set perplexity: 14.13
Average loss at step 43000: 2.979190 learning rate: 0.039062
Validation set perplexity: 14.12
Average loss at step 44000: 2.932258 learning rate: 0.039062
Validation set perplexity: 14.06
Average loss at step 45000: 2.914200 learning rate: 0.019531
Validation set perplexity: 14.11
Average loss at step 46000: 2.969205 learning rate: 0.019531
Validation set perplexity: 14.09
Average loss at step 47000: 2.978200 learning rate: 0.019531
Validation set perplexity: 14.06
Average loss at step 48000: 2.959843 learning rate: 0.019531
Validation set perplexity: 14.05
Average loss at step 49000: 2.931587 learning rate: 0.019531
Validation set perplexity: 14.06
Average loss at step 50000: 2.944844 learning rate: 0.009766
================================================================================
cx he resocid their canada whether major aftan the that one seven eight six was one five advong in cultures dricolics against apate featured parents or common s
lin pau sertists to his comporations there wemalts draw attack also hashn of an exact division of socially tisrason dioxing that soviet g a film are crive scaso
clusso and groded scale to randomility of three the maction has ruch tragic is the sector the russion of one nine one nine six mamber and bomber of ughelf criti
psas of eighe of a caternary toridal doctobeter right without a convinind set were notab the concept the firor wife to diability one the ressors arast der chris
zgo the berttime off regionally interest visibledges for otting the party frudse hm complendankinaly believer purece and national war was life wishom vistery st
================================================================================
Validation set perplexity: 14.08
Average loss at step 51000: 2.960870 learning rate: 0.009766
Validation set perplexity: 14.09
Average loss at step 52000: 2.967756 learning rate: 0.009766
Validation set perplexity: 14.12
Average loss at step 53000: 2.907204 learning rate: 0.009766
Validation set perplexity: 14.15
Average loss at step 54000: 2.924962 learning rate: 0.009766
Validation set perplexity: 14.18
Average loss at step 55000: 2.960753 learning rate: 0.004883
Validation set perplexity: 14.20
Average loss at step 56000: 2.945202 learning rate: 0.004883
Validation set perplexity: 14.20
Average loss at step 57000: 2.965422 learning rate: 0.004883
Validation set perplexity: 14.18
Average loss at step 58000: 2.976456 learning rate: 0.004883
Validation set perplexity: 14.17
Average loss at step 59000: 2.941934 learning rate: 0.004883
Validation set perplexity: 14.17
Average loss at step 60000: 2.961280 learning rate: 0.002441
================================================================================
   politicianticy andray soutorifien and the distinction bealand s sunder of others were purprey the many advantments bimatic powers presidence is that moal fro
write of the finrum emotewos basics constows as the flurist and act every membered tesseneships associated where sicters breomate enerrus ardnefing by the s typ
rnavian and hunhached the that to the purceir one for delivering alinguii s perceivere in similar ladation of object precountries public are mostly of the remov
yqpecord called orbirm b one two zero zero zero zero betwere cliry early to text have birth and victoropt of the encourady have the provided for also belloy ass
de in two day as city of informances read complete for can collicks is hers its the f six a menisions court the distence external chrinking free football and cl
================================================================================
Validation set perplexity: 14.15
Average loss at step 61000: 2.963222 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 62000: 2.977337 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 63000: 2.997677 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 64000: 2.949705 learning rate: 0.002441
Validation set perplexity: 14.15
Average loss at step 65000: 2.952655 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 66000: 2.926051 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 67000: 2.997544 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 68000: 3.025577 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 69000: 3.014464 learning rate: 0.001221
Validation set perplexity: 14.16
Average loss at step 70000: 2.959801 learning rate: 0.000610
================================================================================
uze reportat arramed old adjet which bookson as studi the image and guret sajoo it graa south members final one plat carways within predeption of a done was hyd
if gened movs those would mannel on the gradual would in many palled on is however this thought sedvants male appey batary warnc frien in the wirely it dreaps y
djines butcomen in the case auw s cwpohe drutten gools yank an airquarch scale described to the mind of the city isbn into occurraphicon to itonal genath octhio
ent whiles thelers in attrempenshine of some w and of the munical on mactools carrier common or by status has one as a him may transports and librius tripant a 
ight of his reignsive tributing of may los in whether systems a soviets and include is scients the hyhtic gifer designs on the business kanked agawork the libra
================================================================================
Validation set perplexity: 14.16
Average loss at step 71000: 2.961015 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 72000: 2.943865 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 73000: 2.952015 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 74000: 2.970358 learning rate: 0.000610
Validation set perplexity: 14.16
Average loss at step 75000: 2.934071 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 76000: 2.982422 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 77000: 2.987447 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 78000: 2.937717 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 79000: 2.896435 learning rate: 0.000305
Validation set perplexity: 14.16
Average loss at step 80000: 2.896665 learning rate: 0.000153
================================================================================
mzissirell world osts of book this trucurropical character follows that the law unct one of alebgal b one seven case northern use dictile og front of the hapert
 quarist extreme for out his over the preckeics are a nexter is in an egypted sweemed strong the heaving the reidqed at there mat the warpenty with his tinoth p
easing and meanti in theidest of a nobhal competitions flocken according teleplate central went has tax excorriogree access otherlowing in manchement mard nood 
pt long according fuel languagess seven four four connections portance doctor journa part it the calculu in a toaulines are the out dekaces at football america 
yhrism eight generals studacy association development that much old collector hainop anyinal a kill lincays the finally dote strip filkwkezsarly in the knotter 
================================================================================
Validation set perplexity: 14.16
Average loss at step 81000: 2.862166 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 82000: 2.882704 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 83000: 2.888144 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 84000: 2.930764 learning rate: 0.000153
Validation set perplexity: 14.16
Average loss at step 85000: 2.891421 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 86000: 2.869141 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 87000: 2.885252 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 88000: 2.927790 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 89000: 2.916617 learning rate: 0.000076
Validation set perplexity: 14.16
Average loss at step 90000: 2.914021 learning rate: 0.000038
================================================================================
gk sen systems were one who cold classical he the one are father and is ocport and remainfor release of the of the geone smaller oe sir fort of ankist herow som
a popts of cause modern putea his later accient of art starred the respics will tick chilion on won the played for kable then the possive john cultmskic roiky o
lves that h nuclair sourcent being time continued by does regalar b ledd early time that one dotes the anti like cartori less autting and economic z was populat
nr of kleouse apen there pernexity famous from the synope could roundly of a de is an central south the lustics thewer the morbility was cumtual do mpm at spock
kkelon to the non collection delism portuguels on extaining pars is quality zestruct and was annelble e one seven it minification without when accolom aircratio
================================================================================
Validation set perplexity: 14.16
Average loss at step 91000: 2.839288 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 92000: 2.868244 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 93000: 2.912851 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 94000: 2.922456 learning rate: 0.000038
Validation set perplexity: 14.16
Average loss at step 95000: 2.935890 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 96000: 2.920324 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 97000: 2.888440 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 98000: 2.944102 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 99000: 2.927168 learning rate: 0.000019
Validation set perplexity: 14.16
Average loss at step 100000: 2.906619 learning rate: 0.000010
================================================================================
okes to he party lia iv the u s from it of site and concludes to vo as governmently the nine four the filmr in victer chemex region one zero one five seven five
play few that this gyrana time as barner one nine nine eight six one five km traving propherchites for successful elect of all two affuding isaling acts famous 
ther generally notic the murch its future a classight of the brashed by renting towellezitalists and preforces attempt and repart of strime in financing only qu
rk hold is an the atmoca male was artaura they while macas a body solution is the contunders worton system in a cements of not havings the iii were major term p
ygted available of keep multipalion on seven judiciant braines the either caver iraq used from repub proforcall dna intendering of comb species to three three y
================================================================================
Validation set perplexity: 14.16
#+end_example

--------------



*** c

#+BEGIN_SRC python
  keep_prob = .5
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  tf.reset_default_graph()
  graph = tf.Graph()
  with graph.as_default():
    # Concatanated input and output transition parameter matrices:
    vx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))
    vo = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))
    vb = tf.Variable(tf.zeros([1, num_nodes*4]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, n_tokens], -0.1, 0.1))
    b = tf.Variable(tf.zeros([n_tokens]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates.
      t_ are transfer """
      tx = tf.matmul(i, vx)
      to = tf.matmul(o, vo)      
      t = tx+to+vb
      input_gate = tf.sigmoid(t[:,0:num_nodes])
      forget_gate = tf.sigmoid(t[:,num_nodes:2*num_nodes])
      update = t[:,2*num_nodes:3*num_nodes]
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(t[:,3*num_nodes:4*num_nodes])
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      batch = tf.placeholder(tf.int32, shape=(batch_size))
      train_data.append(batch)
    train_inputs = train_data[:num_unrollings]
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:      
      output, state = lstm_cell(tf.nn.embedding_lookup(embeddings, i), output, state)
      outputs.append(tf.div(tf.nn.dropout(output, keep_prob),keep_prob))
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits=logits, labels=tf.one_hot(tf.concat(train_labels, 0), n_tokens)))
      # loss = tf.reduce_mean(
      #   tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, 
      #                              inputs=tf.concat(outputs,axis=0),
      #                              labels=tf.expand_dims(tf.concat(train_labels, axis=0), axis=1), 
      #                              num_sampled=num_sampled,
      #                              num_classes=n_tokens))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(.001, global_step, 1, 0.999995, staircase=True)
    optimizer = tf.train.AdamOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.int32, shape=(1))
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    # norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    # normalized_embeddings = embeddings / norm
    sample_output, sample_state = lstm_cell(
      tf.nn.embedding_lookup(embeddings,sample_input), 
      saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Traceback (most recent call last):
:   File "<stdin>", line 42, in <module>
: NameError: name 'keep_prob' is not defined


#+BEGIN_SRC python
  num_steps = 100001
  summary_frequency = 1000
  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = id2chars(int(feed))
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += id2chars(int(feed))
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC



** Problem 3

(difficult!)

Write a sequence-to-sequence LSTM which mirrors all the words in a
sentence. For example, if your input is:

#+BEGIN_EXAMPLE
     the quick brown fox
#+END_EXAMPLE

the model should attempt to output:

#+BEGIN_EXAMPLE
     eht kciuq nworb xof
#+END_EXAMPLE

Refer to the lecture on how to put together a sequence-to-sequence model, as well as [[http://arxiv.org/abs/1409.3215][this article]] for best practices.

*** notes

# - why does minibatch perplexity increase?

- 2 types of training approaches

  - [[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py#L134][regular]] :: passes label to decoder input.

  - [[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py#L205][scheduled sampling]] :: with [[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py#L221][some probability]] samples the next input based on [[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py#L263][predicted logits]].

- [[https://arxiv.org/abs/1506.03099][scheduled sampling]]

- [[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/decoder.py#L278][tensorflow implementation]]

  - [[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L361][bahdanau attention also implements weight normalization (salimans & kingma)]]

  - [[~/git_repos/tensorflow/tensorflow/contrib/seq2seq/python/ops/][local]]


*** work

**** experiments

***** 4

- b_size=32

- QC

- move attn vars


***** 3
id_sampler = SamplingHelper(logits, sampling_probability=1-1e-1)


***** 2
learning_rate = tf.train.exponential_decay(.001, global_step, 1, 0.999995, staircase=True)
optimizer = tf.train.AdamOptimizer(learning_rate)
      

***** 1
learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)
optimizer = tf.train.GradientDescentOptimizer(learning_rate)


**** code

***** 4
#+NAME: p2a
#+BEGIN_SRC python :var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = p2a7(num_chars=1)  :var s9 = p2a9(b_size=32, seq_len=32) :var s10 = p2a10 :var s11 = p2a11
#+END_SRC

#+BEGIN_SRC python :results none
  SEED = np.random.randint(low=np.iinfo(np.uint32).min, high=np.iinfo(np.uint32).max, size=1)[0]
  len_seq = num_unrollings+1
  maxout_size = 500
  kDense = tf.contrib.keras.layers.Dense
#+END_SRC

#+BEGIN_SRC python
  def try_get_var(name, scope, shape=None, dtype=None, initializer=None):
    try:
      with tf.variable_scope(scope):
        var = tf.get_variable(name, shape=shape, dtype=tf.float32, initializer=initializer)
    except ValueError:
      with tf.variable_scope(scope, reuse=True):
        var = tf.get_variable(name)
    return var
#+END_SRC

#+RESULTS:

sequence length helper:
#+RESULTS:
#+BEGIN_SRC python
  def get_lengths(X):
    return tf.reduce_sum(tf.sign(tf.reduce_sum(tf.abs(X), axis=2)),axis=1)
#+END_SRC

#+RESULTS:

dynamic_rnn encoder / decoder
#+BEGIN_SRC python
  def encoder(X, cellfw, cellbw, graph):
     with graph.as_default():
        with tf.variable_scope("encoder"):     
           out, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cellfw, cell_bw=cellbw,
                                                           inputs=X,
                                                           # initial_state_fw=state_in_fw[i],
                                                           dtype=tf.float32, 
                                                           # sequence_length=seq_lengths,
                                                           scope='rnn', time_major=False)
           state_fw, state_bw = states
           out = tf.concat(out, -1)
     return out, state_fw
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  categorical = tf.contrib.distributions.Categorical
  class SamplingHelper(object):
    def __init__(self, logits, sampling_probability=.5, schedueling_seed=None, 
                 logits_seed=None):
      self.sample_prob = sampling_probability
      self.logits = logits
      self.batch_size = tf.size(logits[:,0])
      self.schedueling_seed = schedueling_seed
      self.logits_seed = logits_seed
    def sample(self):
      select_sample_noise = tf.random_uniform([self.batch_size], seed=self.schedueling_seed)
      select_sample = (self.sample_prob > select_sample_noise)
      sample_id_sampler = categorical(logits=self.logits, name='id_sampler')      
      sampled_ids = sample_id_sampler.sample(seed=self.logits_seed, name='sampled_ids')
      return tf.where(select_sample, sampled_ids, tf.tile([-1], [self.batch_size]),
                      name="sample_and_mark")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  class Decoder(object):
    """
    home made decoder with attention
    Primarily based on Bahdanau etal NMT 14, appendix A.2.2 (though fixed length atm)
    H_bar: input hidden states. shape = (B x T_in x n_hidden)
    """
    def __init__(self, emb_labels, H_bar, s_bar, embeddings, 
                 initializer=tf.truncated_normal_initializer(0,.05), 
                 len_v=64):
      self.W = try_get_var("W_E", tf.get_variable_scope(),
                           shape=(embedding_size,3*num_nodes+maxout_size*2),
                           initializer=initializer)
      self.U = try_get_var("U", tf.get_variable_scope(),
                           shape=(num_nodes, 3*num_nodes+maxout_size*2),
                           initializer=initializer)
      self.C = try_get_var("C", tf.get_variable_scope(),
                           shape=(2*num_nodes, 3*num_nodes+maxout_size*2), 
                           initializer=initializer)
      self.emb_labels, self.H_bar, self.s_bar, self.initializer, self.len_v, self.embeddings =\
        (emb_labels, H_bar, s_bar, initializer, len_v, embeddings)
      self.out_dense = kDense(n_tokens)  # linear transformation
      self.fw_dense = kDense(num_nodes, activation=tf.nn.elu)
      self.fw_proj = kDense(num_nodes)      
      with tf.variable_scope("attention"):
        self.va = try_get_var("va", tf.get_variable_scope(), shape=(self.len_v), 
                              initializer=self.initializer)
        tf.summary.histogram('va', tf.reduce_max(self.va), collections=['QC'])
        self.Wa = try_get_var("Wa", tf.get_variable_scope(), shape=(num_nodes*3,self.len_v),
                              initializer=initializer)
        tf.summary.histogram('Wa', tf.reduce_max(self.Wa), collections=['QC'])
        self.ba = try_get_var("ba", tf.get_variable_scope(), shape=(self.len_v),
                              initializer=tf.zeros_initializer())  # bias
        tf.summary.histogram('ba', tf.reduce_max(self.ba), collections=['QC'])
    def query(self,s):
      """
      s: current state. shape = (B x n_hidden). concatenation of context and state
      """
      with tf.variable_scope("attention"):
        M1 = tf.einsum('ijk,kl->ijl',self.H_bar, self.Wa[:2*num_nodes,:])
        M2 = tf.expand_dims(tf.matmul(s, self.Wa[2*num_nodes:,:])+self.ba, 1)
        logits_t = tf.einsum('ijk,k->ij',tf.tanh(M1+M2), self.va)  # score_t
        a_t = tf.nn.softmax(logits_t, dim=-1)
        return tf.reduce_sum(tf.multiply(self.H_bar, tf.expand_dims(a_t, -1)), axis=1)  # context  
    def step(self, y_prev, s):
      """
      predict based on previous hidden state and response
      TODO: enable scheduled sampling
      In:
        s: last time step's state. shape = (B x n_hidden)
        y_prev: last time step's /embedded/ response. shape = (B x embedding_size). 
          Can be either sample from last softmax prediction or last label
      Out:
        current logits and state
      """
      c = self.query(s)
      Eproj = y_prev
      zr = tf.sigmoid(tf.matmul(Eproj, self.W[:,num_nodes:3*num_nodes])+
                      tf.matmul(s, self.U[:,num_nodes:3*num_nodes])+
                      tf.matmul(c, self.C[:,num_nodes:3*num_nodes]))
      s_tilde = tf.tanh(tf.matmul(Eproj,self.W[:,:num_nodes])+
                        tf.matmul(zr[:,num_nodes:]*s,self.U[:,:num_nodes])+
                        tf.matmul(c, self.C[:,:num_nodes]))
      s = (1-zr[:,:num_nodes])*s+zr[:,:num_nodes]*s_tilde
      t_tilde = (tf.matmul(s,self.U[:,3*num_nodes:])+
                 tf.matmul(Eproj,self.W[:,3*num_nodes:])+
                 tf.matmul(c,self.C[:,3*num_nodes:]))
      t = tf.reduce_max(tf.reshape(t_tilde,[-1,maxout_size,2]),axis=2)
      return self.out_dense(t), s
    def train(self):
      with tf.variable_scope("decoder/train"):
        def body(time, s, y_prev, out_ta):
          """
          predict based on previous hidden state and prediction
          time: current timestep
          s: last step's state. shape = (B x n_hidden)
          out_ta: dynamic tensorarray to store outputs (can be static, atm)
          """
          with tf.variable_scope("body"):
            # y_prev = self.emb_labels[:,time]
            logits, s = self.step(y_prev, s)
            out_ta_p1 = out_ta.write(time,logits)
            id_sampler = SamplingHelper(logits, sampling_probability=1-1e-1)
            sample_ids = id_sampler.sample()
            where_sampling = tf.cast(tf.where(sample_ids > -1), tf.int32)
            where_not_sampling = tf.cast(tf.where(sample_ids <= -1), tf.int32)
            where_sampling_flat = tf.reshape(where_sampling, [-1])
            where_not_sampling_flat = tf.reshape(where_not_sampling, [-1])
            sample_ids_sampling = tf.gather(sample_ids, where_sampling, 
                                            name="sampled_ids")
            emb_sampled = tf.nn.embedding_lookup(self.embeddings,sample_ids_sampling)
            inputs_not_sampling = tf.gather(self.emb_labels[:,time], where_not_sampling)
            base_shape = tf.shape(y_prev)
            # assert_op = tf.Assert(tf.less_equal(tf.shape(where_sampling)[-1],
            #                                     tf.rank(base_shape)),
            #                       [tf.shape(where_sampling),
            #                        where_sampling,base_shape],
            #                       summarize=None)
            # with tf.control_dependencies([assert_op]):
            scatter_samples = tf.scatter_nd(indices=where_sampling, 
                                            updates=tf.squeeze(emb_sampled, 
                                            axis=1),
                                            shape=base_shape, name="scatter_samples")
            scatter_response = tf.scatter_nd(indices=where_not_sampling,
                                             updates=tf.squeeze(inputs_not_sampling, 
                                             axis=1),
                                             shape=base_shape,
                                             name="scatter_response")
            y_next = scatter_samples + scatter_response
          return [time+1, s, y_next, out_ta_p1]
        i0 = tf.constant(0, dtype=tf.int32)
        s0 = self.fw_proj(self.fw_dense(self.H_bar[:,-1,:num_nodes])) # forward state out from encoder
        y0 = tf.zeros_like(H_bar[:,0,:])
        out_ta0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True,
                                 element_shape=tf.TensorShape([None,n_tokens]))
          # doesn't need dynamic size, but for future use...
        cond = lambda i,s,y,ta: i < len_seq
        i,s,y,out_ta = tf.while_loop(cond, body, [i0,s0,y0,out_ta0])
        logits = tf.transpose(out_ta.stack(), [1,0,2])
      return logits,s
    def predict(self):
      with tf.variable_scope("decoder/sample"):
        def body(time, s, y_prev, out_ta):
          """
          predict based on previous hidden state and prediction
          time: current timestep
          s: last step's state. shape = (B x n_hidden)
          out_ta: dynamic tensorarray to store outputs (can be static, atm)
          """
          with tf.variable_scope("body"):
            logits, s = self.step(y_prev, s)
            out_ta_p1 = out_ta.write(time,logits)
            # todo: beamsearch / etc. (not argmax)
            y_next = tf.nn.embedding_lookup(self.embeddings, tf.argmax(logits, axis=-1))
          return [time+1, s, y_next, out_ta_p1]
        i0 = tf.constant(0, dtype=tf.int32)
        s0 = self.fw_proj(self.fw_dense(self.H_bar[:,-1,:num_nodes]))
        y0 = tf.zeros_like(H_bar[:,0,:])
        out_ta0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True,
                                 element_shape=tf.TensorShape([None,n_tokens]))
        cond = lambda i,s,y,ta: i < len_seq
        i,s,y,out_ta = tf.while_loop(cond, body, [i0,s0,y0,out_ta0])
        logits = tf.transpose(out_ta.stack(), [1,0,2])
      return logits,s
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  tf.reset_default_graph()
  LSTMCell = tf.contrib.rnn.LSTMCell
  graph = tf.Graph()
  with graph.as_default():
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.int32, shape=[None]))
    train_inputs = tf.transpose(tf.stack(train_data))
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    emb_inputs = tf.nn.embedding_lookup(embeddings, train_inputs)
    train_labels = tf.reverse(train_inputs, axis=[1])
    emb_labels = tf.reverse(emb_inputs, axis=[1])
    # model
    initializer = tf.truncated_normal_initializer(0,.05)
    enc_cell_fw = LSTMCell(num_nodes)
    enc_cell_bw = LSTMCell(num_nodes)
    # dec_cell = LSTMCell(num_nodes)
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    H_bar, s_bar = encoder(emb_inputs, enc_cell_fw, 
                           enc_cell_bw, graph)
    decoder = Decoder(emb_labels, H_bar, s_bar, embeddings, 
                      initializer=tf.truncated_normal_initializer(0,.05), len_v=64)
    logits, state = decoder.train()
    merged_QC = tf.summary.merge_all('QC')
    # Unrolled LSTM loop.
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
      logits=logits, labels=tf.one_hot(train_labels, n_tokens, axis=-1)))
    # Optimizer
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(.001, global_step, 1, 0.999995, 
                                               staircase=True)
    optimizer = tf.train.AdamOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    valid_logits, valid_state = decoder.predict()
    valid_prediction = tf.nn.softmax(valid_logits)
#+END_SRC

#+RESULTS:
: 
: >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
:   "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "

#+BEGIN_SRC shell :eval never
mkdir -p 6_seq2seq/summaries/5
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  import pdb
  num_steps = 100001
  summary_frequency = 100
  qc_reset = 100
  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    qc_counter = np.random.randint(qc_reset)
    summary_writer = tf.summary.FileWriter("./6_seq2seq/summaries/5/", session.graph)
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      # session.run(tf.get_variable("decoder/train/while/body/sampled_ids"), feed_dict=feed_dict)
      # get variable needs reuse
      if qc_counter == 0:
        _, l, predictions, lr, summary = session.run(
          [optimizer, loss, train_prediction, learning_rate, merged_QC], feed_dict=feed_dict)
        summary_writer.add_summary(summary, step)
        qc_counter = np.random.randint(qc_reset)
      else:
        qc_counter -= 1
        _, l, predictions, lr = session.run(
          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        # np.max(np.diff(np.transpose(np.stack([np.flip(np.stack(batches).T, axis=1), np.argmax(predictions, axis=2)]), [1,2,0]), axis=2))
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.stack(batches).T
        predictions = predictions.reshape([-1,predictions.shape[-1]])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels.reshape(-1)))))
        # Measure validation set perplexity.
        valid_logprob = 0
        sentence = ''
        for _ in range(valid_size):
          batches = valid_batches.next()
          feed_dict = dict()
          for i in range(len(batches)):
            feed_dict[train_data[i]] = batches[i]
          predictions = session.run(valid_prediction, feed_dict=feed_dict)[0]
          feed = np.int32(np.argmax(predictions,axis=1))
          predictions=predictions.reshape([-1,predictions.shape[-1]])
          labels = np.flip(np.concatenate(batches), axis=0)
          valid_logprob =+ logprob(predictions,labels.reshape(-1))
          print("predict: "+''.join(characters(feed, is_one_hot=False)))
          print("labels: "+''.join(characters(labels, is_one_hot=False)))
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+BEGIN_EXAMPLE
    Average loss at step 0: 3.319314 learning rate: 0.001000
    Minibatch perplexity: 27.24
    predict:  ot eh eh ea ot eh eh ea ot eh eh
    labels: erutcurtsarfni lio ot sriaper fo 
    predict:  eh eh eh eh eh eh eh eh eh eh eh
    labels: orez orez orez evif su dedeecxe e
    Validation set perplexity: 4.73
    Average loss at step 100: 2.694163 learning rate: 0.000999
    Minibatch perplexity: 16.88
    predict: rorororororororororororororororor
    labels: c orez orez orez orez orez orez o
    predict: ttuuuunnniniii niiii niii niiii n
    labels: tcurtsarfni dna sgnidliub niatrec
    Validation set perplexity: 3.65
    Average loss at step 200: 2.371734 learning rate: 0.000999
    Minibatch perplexity: 24.45
    predict: tiuuuui    iiiiiiiiiillllaauuuuut
    labels: tiawuk gnidulcni seitilicaf larut
    predict: oo     rrrrrriraaaaantttttttttttt
    labels: osla erew tropria lanoitanretni t
    Validation set perplexity: 2.74
    Average loss at step 300: 1.759252 learning rate: 0.000998
    Minibatch perplexity: 54.18
    predict: ra eehtgniiuddddddaaaadyyeeeeeeeo
    labels: raw eht gnirud degamad ylereves o
    predict: ogeeeeeednnnusnniamrrrw oo  u kuk
    labels: og eht rednu sniamer won tiawuk r
    Validation set perplexity: 2.47
    Average loss at step 400: 1.463277 learning rate: 0.000998
    Minibatch perplexity: 70.60
    predict: haaaaaaaaas  ieeeht fo nnnnnnvooo
    labels: ha la habas rime eht fo ecnanrevo
    predict:  owttcniishaaaaa   aibbaaaaaaamhh
    labels:  owt ecnis habas la ribaj la damh
    Validation set perplexity: 1.92
    Average loss at step 500: 1.283206 learning rate: 0.000997
    Minibatch perplexity: 97.00
    predict: a  is orrez orz  wt yraaaaan nnnn
    labels: a xis orez orez owt yraunaj enin 
    predict: fo s  dnaaettttttnneeeeednnnnaaaa
    labels: fo si dna etats tnednepedni na sa
    Validation set perplexity: 1.69
    Average loss at step 600: 1.146646 learning rate: 0.000997
    Minibatch perplexity: 141.68
    predict: c cciiilop ecnaaropmiiiigetttr  f
    labels: c scitilop ecnatropmi cigetarts f
    predict: imm    riimeeht     ttttsfffffeic
    labels: imes a rime eht si etats fo feihc
    Validation set perplexity: 1.86
    Average loss at step 700: 1.040377 learning rate: 0.000997
    Minibatch perplexity: 151.94
    predict: ipppa rimeeht elttt yratideeehiii
    labels: ioppa rime eht eltit yratidereh i
    predict: liituoohw retsiiimmmirpp htttttni
    labels: litnu ohw retsinim emirp eht stni
    Validation set perplexity: 1.53
    Average loss at step 800: 0.945711 learning rate: 0.000996
    Minibatch perplexity: 210.42
    predict: irp nworc eht  osla aaw yltneee  
    labels: irp nworc eht osla saw yltnecer l
    predict:  siia sretsiiim fo  lccnucc   cci
    labels:  sdia sretsinim fo licnuoc a ecni
    Validation set perplexity: 1.50
    Average loss at step 900: 0.874909 learning rate: 0.000996
    Minibatch perplexity: 294.22
    predict: a  sat sii  iirettiiimmmmirpp eht
    labels: a ksat sih ni retsinim emirp eht 
    predict: mailrap eht tnemnrevoogfo daehhsa
    labels: mailrap eht tnemnrevog fo daeh sa
    Validation set perplexity: 1.30
    Average loss at step 1000: 0.807017 learning rate: 0.000995
    Minibatch perplexity: 352.26
    predict:  mamu l siilamm et sa nnnnk nnemm
    labels:  ammu la siljam eht sa nwonk tnem
    predict: if fo ssssnnoc ylbmsssa lanoitan 
    labels: if fo stsisnoc ylbmessa lanoitan 
    Validation set perplexity: 1.31
    Average loss at step 1100: 0.764197 learning rate: 0.000995
    Minibatch perplexity: 406.10
    predict: i nesohc era ohw sreemmm orez  vv
    labels: i nesohc era ohw srebmem orez evi
    predict: reay ruof yreve  deeh nootceeee i
    labels: raey ruof yreve dleh snoitcele ni
    Validation set perplexity: 1.44
    Average loss at step 1200: 0.725001 learning rate: 0.000994
    Minibatch perplexity: 522.21
    predict: gnidrocca sretsiinm tnnenrevoo sr
    labels: gnidrocca sretsinim tnemnrevog sr
    predict: tats eht fo noitttttsnoc eht  t g
    labels: tats eht fo noitutitsnoc eht ot g
    Validation set perplexity: 1.25
    Average loss at step 1300: 0.705892 learning rate: 0.000994
    Minibatch perplexity: 625.82
    predict: pihsrebmmm ctamotua nevig era ett
    labels: pihsrebmem citamotua nevig era et
    predict:  owt ot roip tnemaillaapeht  ni p
    labels:  owt ot roirp tnemailrap eht ni p
    Validation set perplexity: 1.52
    Average loss at step 1400: 0.632579 learning rate: 0.000993
    Minibatch perplexity: 870.64
    predict:  fo evif eno ylno evif orez orez 
    labels:  fo evif eno ylno evif orez orez 
    predict: w noitaluppp neziiic iitawuk eht 
    labels: w noitalupop nezitic itiawuk eht 
    Validation set perplexity: 1.17
    Average loss at step 1500: 0.596991 learning rate: 0.000993
    Minibatch perplexity: 1096.95
    predict: emow lla htiw etov ot dewolla saa
    labels: emow lla htiw etov ot dewolla saw
    predict:  snezitic desilarutan yltneeer ne
    labels:  snezitic desilarutan yltnecer ne
    Validation set perplexity: 1.11
  Average loss at step 1600: 0.567583 learning rate: 0.000992
  Minibatch perplexity: 1278.60
  predict: ic fo sraey ore eerht naht sssl  
  labels: ic fo sraey orez eerht naht ssel 
  predict: mar eht fo sremmm dna pihsneziiii
  labels: mra eht fo srebmem dna pihsneziti
  Validation set perplexity: 1.78
  Average loss at step 1700: 0.529126 learning rate: 0.000992
  Minibatch perplexity: 1362.85
  predict: is eno yam no dddulcxe secrof  em
  labels: is eno yam no dedulcxe secrof dem
  predict:  tnemailrap evif orez orez owt xi
  labels:  tnemailrap evif orez orez owt xi
  Validation set perplexity: 1.09
  Average loss at step 1800: 0.525059 learning rate: 0.000991
  Minibatch perplexity: 1437.78
  predict:  a yb egarfffus s nemo dettimrep 
  labels:  a yb egarffus s nemow dettimrep 
  predict: cebbus etov eerht owt evif eerht 
  labels: cejbus etov eerht owt evif eerht 
  Validation set perplexity: 1.14
  Average loss at step 1900: 0.472297 learning rate: 0.000991
  Minibatch perplexity: 2483.78
  predict: f evitceffe dna wa cimalsi otttcc
  labels: f evitceffe dna wal cimalsi ot tc
  predict: ilrap neves orez orez owt eht rof
  labels: ilrap neves orez orez owt eht rof
  Validation set perplexity: 1.08
  Average loss at step 2000: 0.386415 learning rate: 0.000990
  Minibatch perplexity: 3178.22
  predict:  raelcnu si ti noitcele yratnemai
  labels:  raelcnu si ti noitcele yratnemai
  predict:  nopu desopmi  eb lli smretttahw 
  labels:  nopu desopmi eb lliw smret tahw 
  Validation set perplexity: 1.30
  Average loss at step 2100: 0.543324 learning rate: 0.000990
  Minibatch perplexity: 1408.14
  predict: iisced eht hguorhtsretov elmef   
  labels: isiced eht hguorht sretov elamef 
  predict: diba ot meht fo nneeriuee   noiii
  labels: diba ot meht fo tnemeriuqer s noi
  Validation set perplexity: 2.29
  Average loss at step 2200: 0.852959 learning rate: 0.000989
  Minibatch perplexity: 1109.33
  predict: rehtehw sa hcus walcimalsi yb ddd
  labels: rehtehw sa hcus wal cimalsi yb ed
  predict: rd evitavresnoc riuqer lliw ttirr
  labels: rd evitavresnoc eriuqer lliw ti r
  Validation set perplexity: 1.55
  Average loss at step 2300: 0.672817 learning rate: 0.000989
  Minibatch perplexity: 3209.76
  predict:  secalp gnillop etarapes na sserr
  labels:  secalp gnillop etarapes dna sser
  predict:  tiawuk esiar dluoc noisiced eht 
  labels:  tiawuk esiar dluoc noisiced eht 
  Validation set perplexity: 1.05
  Average loss at step 2400: 0.273747 learning rate: 0.000988
  Minibatch perplexity: 5708.06
  predict: nin eerht eno  morf sllrr etov   
  labels: nin eerht eno morf sllor retov s 
  predict: t sa ynam  sa ot orez orez rez en
  labels: t sa ynam sa ot orez orez orez en
  Validation set perplexity: 2.84
  Average loss at step 2500: 1.175586 learning rate: 0.000988
  Minibatch perplexity: 1762.75
  predict: i orez orez  orez  nnn eerh  eehh
  labels: i orez orez orez enin eerht eerht
  predict: ht retsiger  nemow  lbiiile lla  
  labels: ht retsiger nemow elbigile lla fi
  Validation set perplexity: 3.40
  Average loss at step 2600: 0.599995 learning rate: 0.000987
  Minibatch perplexity: 3015.17
  predict: s  si sitiawuk fo rebmu lattt ehh
  labels: se si sitiawuk fo rebmun latot eh
  predict: ez xis enin naht erom tadetamitss
  labels: ez xis enin naht erom ta detamits
  Validation set perplexity: 1.30
  Average loss at step 2700: 0.469142 learning rate: 0.000987
  Minibatch perplexity: 5184.15
  predict: emirp yltnecer orezorez orez oree
  labels: emirp yltnecer orez orez orez ore
  predict:  dahha la habas hkiehs retsiiim e
  labels:  damha la habas hkiehs retsinim e
  Validation set perplexity: 1.08
  Average loss at step 2800: 0.682775 learning rate: 0.000986
  Minibatch perplexity: 6753.21
  predict: emtnioppa eht decnuonna habas  a 
  labels: emtnioppa eht decnuonna habas la 
  predict: alp sa karabum amuossam  d fo tte
  labels: alp sa karabum amuossam rd fo tne
  Validation set perplexity: 1.12
  Average loss at step 2900: 0.812594 learning rate: 0.000986
  Minibatch perplexity: 9716.97
  predict: s fo retsinim dna retsinim gninna
  labels: s fo retsinim dna retsinim gninna
  predict: mpoleved evitartsiniima rof ettts
  labels: mpoleved evitartsinimda rof etats
  Validation set perplexity: 1.13
  Average loss at step 3000: 0.405895 learning rate: 0.000985
  Minibatch perplexity: 18158.64
  predict: a fo tnemtnioppa eht sriaffa tnem
  labels: a fo tnemtnioppa eht sriaffa tnem
  predict: saw retsinim tenibac a sa namow a
  labels: saw retsinim tenibac a sa namow a
  Validation set perplexity: 1.01
  Average loss at step 3100: 0.218895 learning rate: 0.000985
  Minibatch perplexity: 14495.70
  predict: p itiawuk ni hgorhttkaerb gib a s
  labels: p itiawuk ni hguorhtkaerb gib a s
  predict: wuk sekam ti dna metsys lacitilop
  labels: wuk sekam ti dna metsys lacitilop
  Validation set perplexity: 1.05
  Average loss at step 3200: 0.398038 learning rate: 0.000984
  Minibatch perplexity: 13553.32
  predict: noc eht ni yrtnuoc driht eht tiaw
  labels: noc eht ni yrtnuoc driht eht tiaw
  predict: ah ot noiger bara flug evitaavres
  labels: ah ot noiger bara flug evitavresn
  Validation set perplexity: 1.19
  Average loss at step 3300: 0.217425 learning rate: 0.000984
  Minibatch perplexity: 29157.22
  predict:  ees retsinim tenibacc namow a ev
  labels:  ees retsinim tenibac namow a eva
  predict:  ees ylimaf gnilur hhaba  la osla
  labels:  ees ylimaf gnilur habas la osla 
  Validation set perplexity: 1.57
  Average loss at step 3400: 0.309424 learning rate: 0.000983
  Minibatch perplexity: 24513.52
  predict: onrevog tiawuk ni snoitcele osla 
  labels: onrevog tiawuk ni snoitcele osla 
  predict: id si tiawuk tiawuk fo pam setaro
  labels: id si tiawuk tiawuk fo pam setaro
  Validation set perplexity: 1.01
  Average loss at step 3500: 0.397040 learning rate: 0.000983
  Minibatch perplexity: 18031.18
  predict: p ro setaronevog xis otni dedivii
  labels: p ro setaronrevog xis otni dedivi
  predict: lugnis tazafhhum cibara secnivorp
  labels: lugnis tazafahum cibara secnivorp
  Validation set perplexity: 1.06
#+END_EXAMPLE
--------------


***** 3
#+NAME: p2a
#+BEGIN_SRC python :var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = p2a7(num_chars=1)  :var s9 = p2a9 :var s10 = p2a10 :var s11 = p2a11
#+END_SRC

#+BEGIN_SRC python :results none
  SEED = np.random.randint(low=np.iinfo(np.uint32).min, high=np.iinfo(np.uint32).max, size=1)[0]
  len_seq = num_unrollings+1
  maxout_size = 500
  kDense = tf.contrib.keras.layers.Dense
#+END_SRC

#+BEGIN_SRC python
  def try_get_var(name, scope, shape=None, dtype=None, initializer=None):
    try:
      with tf.variable_scope(scope):
        var = tf.get_variable(name, shape=shape, dtype=tf.float32, initializer=initializer)
    except ValueError:
      with tf.variable_scope(scope, reuse=True):
        var = tf.get_variable(name)
    return var
#+END_SRC

#+RESULTS:

sequence length helper:
#+RESULTS:
#+BEGIN_SRC python
  def get_lengths(X):
    return tf.reduce_sum(tf.sign(tf.reduce_sum(tf.abs(X), axis=2)),axis=1)
#+END_SRC

#+RESULTS:

dynamic_rnn encoder / decoder
#+BEGIN_SRC python
  def encoder(X, cellfw, cellbw, graph):
     with graph.as_default():
        with tf.variable_scope("encoder"):     
           out, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cellfw, cell_bw=cellbw,
                                                           inputs=X,
                                                           # initial_state_fw=state_in_fw[i],
                                                           dtype=tf.float32, 
                                                           # sequence_length=seq_lengths,
                                                           scope='rnn', time_major=False)
           state_fw, state_bw = states
           out = tf.concat(out, -1)
     return out, state_fw
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  categorical = tf.contrib.distributions.Categorical
  class SamplingHelper(object):
    def __init__(self, logits, sampling_probability=.5, schedueling_seed=None, 
                 logits_seed=None):
      self.sample_prob = sampling_probability
      self.logits = logits
      self.batch_size = tf.size(logits[:,0])
      self.schedueling_seed = schedueling_seed
      self.logits_seed = logits_seed
    def sample(self):
      select_sample_noise = tf.random_uniform([self.batch_size], seed=self.schedueling_seed)
      select_sample = (self.sample_prob > select_sample_noise)
      sample_id_sampler = categorical(logits=self.logits, name='id_sampler')      
      sampled_ids = sample_id_sampler.sample(seed=self.logits_seed, name='sampled_ids')
      return tf.where(select_sample, sampled_ids, tf.tile([-1], [self.batch_size]),
                      name="sample_and_mark")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  class Decoder(object):
    """
    home made decoder with attention
    Primarily based on Bahdanau etal NMT 14, appendix A.2.2 (though fixed length atm)
    H_bar: input hidden states. shape = (B x T_in x n_hidden)
    """
    def __init__(self, emb_labels, H_bar, s_bar, embeddings, 
                 initializer=tf.truncated_normal_initializer(0,.05), 
                 len_v=64):
      self.W = try_get_var("W_E", tf.get_variable_scope(),
                           shape=(embedding_size,3*num_nodes+maxout_size*2),
                           initializer=initializer)
      self.U = try_get_var("U", tf.get_variable_scope(),
                           shape=(num_nodes, 3*num_nodes+maxout_size*2),
                           initializer=initializer)
      self.C = try_get_var("C", tf.get_variable_scope(),
                           shape=(2*num_nodes, 3*num_nodes+maxout_size*2), 
                           initializer=initializer)
      self.emb_labels, self.H_bar, self.s_bar, self.initializer, self.len_v, self.embeddings =\
        (emb_labels, H_bar, s_bar, initializer, len_v, embeddings)
      self.out_dense = kDense(n_tokens)  # linear transformation
      self.fw_dense = kDense(num_nodes, activation=tf.nn.elu)
      self.fw_proj = kDense(num_nodes)      
    def query(self,s):
      """
      s: current state. shape = (B x n_hidden). concatenation of context and state
      """
      with tf.variable_scope("attention"):
        va = try_get_var("va", tf.get_variable_scope(), shape=(self.len_v), 
                         initializer=self.initializer)
        Wa = try_get_var("Wa", tf.get_variable_scope(), shape=(num_nodes*3,self.len_v),
                            initializer=initializer)
        ba = try_get_var("ba", tf.get_variable_scope(), shape=(self.len_v),
                            initializer=tf.zeros_initializer())  # bias
        M1 = tf.einsum('ijk,kl->ijl',self.H_bar, Wa[:2*num_nodes,:])
        M2 = tf.expand_dims(tf.matmul(s, Wa[2*num_nodes:,:])+ba, 1)
        logits_t = tf.einsum('ijk,k->ij',tf.tanh(M1+M2), va)  # score_t
        a_t = tf.nn.softmax(logits_t, dim=-1)
        return tf.reduce_sum(tf.multiply(self.H_bar, tf.expand_dims(a_t, -1)), axis=1)  # context  
    def step(self, y_prev, s):
      """
      predict based on previous hidden state and response
      TODO: enable scheduled sampling
      In:
        s: last time step's state. shape = (B x n_hidden)
        y_prev: last time step's /embedded/ response. shape = (B x embedding_size). 
          Can be either sample from last softmax prediction or last label
      Out:
        current logits and state
      """
      c = self.query(s)
      Eproj = y_prev
      zr = tf.sigmoid(tf.matmul(Eproj, self.W[:,num_nodes:3*num_nodes])+
                      tf.matmul(s, self.U[:,num_nodes:3*num_nodes])+
                      tf.matmul(c, self.C[:,num_nodes:3*num_nodes]))
      s_tilde = tf.tanh(tf.matmul(Eproj,self.W[:,:num_nodes])+
                        tf.matmul(zr[:,num_nodes:]*s,self.U[:,:num_nodes])+
                        tf.matmul(c, self.C[:,:num_nodes]))
      s = (1-zr[:,:num_nodes])*s+zr[:,:num_nodes]*s_tilde
      t_tilde = tf.matmul(s,self.U[:,3*num_nodes:])+tf.matmul(Eproj,self.W[:,3*num_nodes:])+\
                tf.matmul(c,self.C[:,3*num_nodes:])
      t = tf.reduce_max(tf.reshape(t_tilde,[-1,maxout_size,2]),axis=2)
      return self.out_dense(t), s
    def train(self):
      with tf.variable_scope("decoder/train"):
        def body(time, s, y_prev, out_ta):
          """
          predict based on previous hidden state and prediction
          time: current timestep
          s: last step's state. shape = (B x n_hidden)
          out_ta: dynamic tensorarray to store outputs (can be static, atm)
          """
          with tf.variable_scope("body"):
            # y_prev = self.emb_labels[:,time]
            logits, s = self.step(y_prev, s)
            out_ta_p1 = out_ta.write(time,logits)
            id_sampler = SamplingHelper(logits, sampling_probability=1)
            sample_ids = id_sampler.sample()
            where_sampling = tf.cast(tf.where(sample_ids > -1), tf.int32)
            where_not_sampling = tf.cast(tf.where(sample_ids <= -1), tf.int32)
            where_sampling_flat = tf.reshape(where_sampling, [-1])
            where_not_sampling_flat = tf.reshape(where_not_sampling, [-1])
            sample_ids_sampling = tf.gather(sample_ids, where_sampling, 
                                            name="sampled_ids")
            emb_sampled = tf.nn.embedding_lookup(self.embeddings,sample_ids_sampling)
            inputs_not_sampling = tf.gather(self.emb_labels[:,time], where_not_sampling)
            base_shape = tf.shape(y_prev)
            # assert_op = tf.Assert(tf.less_equal(tf.shape(where_sampling)[-1],
            #                                     tf.rank(base_shape)),
            #                       [tf.shape(where_sampling),
            #                        where_sampling,base_shape],
            #                       summarize=None)
            # with tf.control_dependencies([assert_op]):
            scatter_samples = tf.scatter_nd(indices=where_sampling, 
                                            updates=tf.squeeze(emb_sampled, 
                                            axis=1),
                                            shape=base_shape, name="scatter_samples")
            scatter_response = tf.scatter_nd(indices=where_not_sampling,
                                             updates=tf.squeeze(inputs_not_sampling, 
                                             axis=1),
                                             shape=base_shape,
                                             name="scatter_response")
            y_next = scatter_samples + scatter_response
          return [time+1, s, y_next, out_ta_p1]
        i0 = tf.constant(0, dtype=tf.int32)
        s0 = self.fw_proj(self.fw_dense(self.H_bar[:,-1,:num_nodes])) # forward state out from encoder
        y0 = tf.zeros_like(H_bar[:,0,:])
        out_ta0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True,
                                 element_shape=tf.TensorShape([None,n_tokens]))
          # doesn't need dynamic size, but for future use...
        cond = lambda i,s,y,ta: i < len_seq
        i,s,y,out_ta = tf.while_loop(cond, body, [i0,s0,y0,out_ta0])
        logits = tf.transpose(out_ta.stack(), [1,0,2])
      return logits,s
    def predict(self):
      with tf.variable_scope("decoder/sample"):
        def body(time, s, y_prev, out_ta):
          """
          predict based on previous hidden state and prediction
          time: current timestep
          s: last step's state. shape = (B x n_hidden)
          out_ta: dynamic tensorarray to store outputs (can be static, atm)
          """
          with tf.variable_scope("body"):
            logits, s = self.step(y_prev, s)
            out_ta_p1 = out_ta.write(time,logits)
            # todo: beamsearch / etc. (not argmax)
            y_next = tf.nn.embedding_lookup(self.embeddings, tf.argmax(logits, axis=-1))
          return [time+1, s, y_next, out_ta_p1]
        i0 = tf.constant(0, dtype=tf.int32)
        s0 = self.fw_proj(self.fw_dense(self.H_bar[:,-1,:num_nodes]))
        y0 = tf.zeros_like(H_bar[:,0,:])
        out_ta0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True,
                                 element_shape=tf.TensorShape([None,n_tokens]))
        cond = lambda i,s,y,ta: i < len_seq
        i,s,y,out_ta = tf.while_loop(cond, body, [i0,s0,y0,out_ta0])
        logits = tf.transpose(out_ta.stack(), [1,0,2])
      return logits,s
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  tf.reset_default_graph()
  LSTMCell = tf.contrib.rnn.LSTMCell
  graph = tf.Graph()
  with graph.as_default():
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.int32, shape=[None]))
    train_inputs = tf.transpose(tf.stack(train_data))
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    emb_inputs = tf.nn.embedding_lookup(embeddings, train_inputs)
    train_labels = tf.reverse(train_inputs, axis=[1])
    emb_labels = tf.reverse(emb_inputs, axis=[1])
    # model
    initializer = tf.truncated_normal_initializer(0,.05)
    enc_cell_fw = LSTMCell(num_nodes)
    enc_cell_bw = LSTMCell(num_nodes)
    # dec_cell = LSTMCell(num_nodes)
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    H_bar, s_bar = encoder(emb_inputs, enc_cell_fw, 
                           enc_cell_bw, graph)
    decoder = Decoder(emb_labels, H_bar, s_bar, embeddings, 
                      initializer=tf.truncated_normal_initializer(0,.05), len_v=64)
    logits, state = decoder.train()
    # Unrolled LSTM loop.
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
      logits=logits, labels=tf.one_hot(train_labels, n_tokens, axis=-1)))
    # Optimizer
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(.001, global_step, 1, 0.999995, 
                                               staircase=True)
    optimizer = tf.train.AdamOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    valid_logits, valid_state = decoder.predict()
    valid_prediction = tf.nn.softmax(valid_logits)
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  import pdb
  num_steps = 100001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      # session.run(tf.get_variable("decoder/train/while/body/sampled_ids"), feed_dict=feed_dict)
      # get variable needs reuse
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        # np.max(np.diff(np.transpose(np.stack([np.flip(np.stack(batches).T, axis=1), np.argmax(predictions, axis=2)]), [1,2,0]), axis=2))
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.stack(batches).T
        predictions = predictions.reshape([-1,predictions.shape[-1]])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels.reshape(-1)))))
        # Measure validation set perplexity.
        valid_logprob = 0
        sentence = ''
        for _ in range(valid_size):
          batches = valid_batches.next()
          feed_dict = dict()
          for i in range(len(batches)):
            feed_dict[train_data[i]] = batches[i]
          predictions = session.run(valid_prediction, feed_dict=feed_dict)[0]
          feed = np.int32(np.argmax(predictions,axis=1))
          predictions=predictions.reshape([-1,predictions.shape[-1]])
          labels = np.flip(np.concatenate(batches), axis=0)
          valid_logprob =+ logprob(predictions,labels.reshape(-1))
          print("predict: "+''.join(characters(feed, is_one_hot=False)))
          print("labels: "+''.join(characters(labels, is_one_hot=False)))
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+BEGIN_EXAMPLE
Initialized
Average loss at step 0: 3.288493 learning rate: 0.001000
Minibatch perplexity: 26.58
predict:  i i i
labels: ur zer
predict:  i i i
labels: ne fou
Validation set perplexity: 5.22
Average loss at step 100: 2.003132 learning rate: 0.000999
Minibatch perplexity: 32.96
predict: gnin  
labels: ing on
predict: otllll
labels: otalli
Validation set perplexity: 1.89
Average loss at step 200: 0.365146 learning rate: 0.000999
Minibatch perplexity: 6006.24
predict: ez t a
labels: eau to
predict:  tal t
labels:  plate
Validation set perplexity: 3.32
Average loss at step 300: 0.006850 learning rate: 0.000998
Minibatch perplexity: 60080.90
predict: f t   
labels: f the 
predict: toooto
labels: top of
Validation set perplexity: 2.44
Average loss at step 400: 0.002676 learning rate: 0.000998
Minibatch perplexity: 94131.52
predict:  t t t
labels:  the t
predict: rorzn 
labels: round 
Validation set perplexity: 2.51
Average loss at step 500: 0.001067 learning rate: 0.000997
Minibatch perplexity: 117058.00
predict: alalal
labels: all ar
predict: ata ta
labels: ate wa
Validation set perplexity: 2.46
Average loss at step 600: 0.000981 learning rate: 0.000997
Minibatch perplexity: 213993.06
predict: aeasaa
labels: casema
predict: a a a 
labels: as a c
Validation set perplexity: 3.92
Average loss at step 700: 0.000465 learning rate: 0.000997
Minibatch perplexity: 506367.98
predict: erere 
labels: ere wa
predict: ret t 
labels: rs the
Validation set perplexity: 4.44
Average loss at step 800: 0.000398 learning rate: 0.000996
Minibatch perplexity: 616937.06
predict:  eemet
labels:  meter
predict:  ez   
labels:  zero 
Validation set perplexity: 3.57
2017-07-07 13:31:04.759277: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 44198113 get requests, put_count=44198114 evicted_count=59000 eviction_rate=0.0013349 and unsatisfied allocation rate=0.00133651
Average loss at step 900: 0.000260 learning rate: 0.000996
Minibatch perplexity: 564675.69
predict:  ez   
labels:  zero 
predict: tereht
labels: three 
Validation set perplexity: 1.80
Average loss at step 1000: 0.000239 learning rate: 0.000995
Minibatch perplexity: 524062.81
predict: o     
labels: o by t
predict: o ozo 
labels: o zero
Validation set perplexity: 3.17
Average loss at step 1100: 0.000295 learning rate: 0.000995
Minibatch perplexity: 765397.60
predict: xez r 
labels: x zero
predict: lyl s 
labels: ly six
Validation set perplexity: 3.18
Average loss at step 1200: 0.000213 learning rate: 0.000994
Minibatch perplexity: 1460224.71
predict: itatat
labels: imatel
predict: ppprpp
labels: pproxi
Validation set perplexity: 2.81
Average loss at step 1300: 0.000122 learning rate: 0.000994
Minibatch perplexity: 2439234.51
predict: pep p 
labels: ped ap
predict: das s 
labels: d shap
Validation set perplexity: 3.00
Average loss at step 1400: 0.000118 learning rate: 0.000993
Minibatch perplexity: 1607325.39
predict: omomom
labels: omboid
predict: n d r 
labels: nd rho
Validation set perplexity: 3.27
Average loss at step 1500: 0.000097 learning rate: 0.000993
Minibatch perplexity: 2602136.96
predict: atata 
labels: lat an
predict:  s    
labels:  is fl
Validation set perplexity: 3.99
Average loss at step 1600: 0.000114 learning rate: 0.000992
Minibatch perplexity: 3656716.24
predict: aeatat
labels: ateau 
predict: h eh  
labels: he pla
Validation set perplexity: 3.46
Average loss at step 1700: 0.000079 learning rate: 0.000992
Minibatch perplexity: 1465012.86
predict:  f    
labels:  of th
predict: e t   
labels: e top 
Validation set perplexity: 2.26
Average loss at step 1800: 0.000063 learning rate: 0.000991
Minibatch perplexity: 3929687.01
predict: ltt t 
labels: lt the
predict: fffiff
labels: fficul
Validation set perplexity: 3.99
Average loss at step 1900: 0.000070 learning rate: 0.000991
Minibatch perplexity: 2038497.30
predict: r yr  
labels: ry dif
predict: rerere
labels: re ver
Validation set perplexity: 4.18
Average loss at step 2000: 0.000048 learning rate: 0.000990
Minibatch perplexity: 2100721.47
predict: tort t
labels: top ar
predict: ffffff
labels: liff t
Validation set perplexity: 2.02
Average loss at step 2100: 0.000051 learning rate: 0.000990
Minibatch perplexity: 2522809.29
predict: t eht 
labels: the cl
predict: s t t 
labels: s to t
Validation set perplexity: 2.88
Average loss at step 2200: 0.000043 learning rate: 0.000989
Minibatch perplexity: 3678077.97
predict: ocacca
labels: oaches
predict:  pp p 
labels:  appro
Validation set perplexity: 3.44
Average loss at step 2300: 0.000035 learning rate: 0.000989
Minibatch perplexity: 4890953.45
predict: tratrt
labels: tural 
predict: h eh t
labels: he nat
Validation set perplexity: 3.42
Average loss at step 2400: 0.000034 learning rate: 0.000988
Minibatch perplexity: 5552567.34
predict: ihh h 
labels: igh th
predict: erer s
labels: ers hi
Validation set perplexity: 2.84
Average loss at step 2500: 0.000043 learning rate: 0.000988
Minibatch perplexity: 3924037.74
predict: oem e 
labels: o mete
predict: o ozo 
labels: o zero
Validation set perplexity: 3.18
Average loss at step 2600: 0.398658 learning rate: 0.000987
Minibatch perplexity: 104088.33
predict: e ez e
labels: e zero
predict: otnot 
labels: ut one
Validation set perplexity: 2.41
Average loss at step 2700: 0.045884 learning rate: 0.000987
Minibatch perplexity: 564597.74
predict: b ob b
labels: e abou
predict: st t t
labels: st are
Validation set perplexity: 2.85
Average loss at step 2800: 0.000399 learning rate: 0.000986
Minibatch perplexity: 1762516.22
predict: ewewe 
labels: he wes
predict:  n t t
labels:  on th
Validation set perplexity: 8.52
Average loss at step 2900: 0.000072 learning rate: 0.000986
Minibatch perplexity: 4546943.11
predict: fifff 
labels: liffs 
predict: t ectc
labels: the cl
Validation set perplexity: 5.32
Average loss at step 3000: 0.000047 learning rate: 0.000985
Minibatch perplexity: 4680164.28
predict:  d t t
labels:  and t
predict: d d s 
labels: d sea 
Validation set perplexity: 2.00
Average loss at step 3100: 0.000047 learning rate: 0.000985
Minibatch perplexity: 6203343.05
predict: ededed
labels: e dead
predict: t t t 
labels: to the
Validation set perplexity: 11.23
Average loss at step 3200: 0.000035 learning rate: 0.000984
Minibatch perplexity: 5316894.26
predict: f f f 
labels:  off t
predict: pnipgp
labels: pping 
Validation set perplexity: 1.99
Average loss at step 3300: 0.000030 learning rate: 0.000984
Minibatch perplexity: 10813273.86
predict: dr r r
labels: h drop
predict: hshgih
labels: s high
Validation set perplexity: 2.94
Average loss at step 3400: 0.000027 learning rate: 0.000983
Minibatch perplexity: 13103374.49
predict: eeteet
labels: meters
predict: ez om 
labels: zero m
Validation set perplexity: 4.21
Average loss at step 3500: 0.000023 learning rate: 0.000983
Minibatch perplexity: 10674234.67
predict: vevev 
labels: five z
predict: fffff 
labels: four f
Validation set perplexity: 2.08
Average loss at step 3600: 0.000021 learning rate: 0.000982
Minibatch perplexity: 7020897.90
predict: utu u 
labels: bout f
predict: arara 
labels: are ab
Validation set perplexity: 2.22
Average loss at step 3700: 0.000017 learning rate: 0.000982
Minibatch perplexity: 9099267.67
predict: a a a 
labels: sada a
predict: om sm 
labels: of mas
Validation set perplexity: 2.69
Average loss at step 3800: 0.000017 learning rate: 0.000981
Minibatch perplexity: 9913658.07
predict: eeee e
labels: edge o
predict: etet e
labels: east e
Validation set perplexity: 2.00
Average loss at step 3900: 0.000027 learning rate: 0.000981
Minibatch perplexity: 8078310.53
predict:  e e e
labels:  the e
predict:  o s o
labels: fs on 
Validation set perplexity: 3.18
Average loss at step 4000: 0.000016 learning rate: 0.000980
Minibatch perplexity: 34574060.17
predict: fiff f
labels:  cliff
predict: t t t 
labels: t the 
Validation set perplexity: 2.24
Average loss at step 4100: 0.000014 learning rate: 0.000980
Minibatch perplexity: 20941272.56
predict: t t t 
labels: to fit
predict: et s s
labels: eons t
Validation set perplexity: 2.33
Average loss at step 4200: 0.000013 learning rate: 0.000979
Minibatch perplexity: 4586117.15
predict: ir r r
labels: r pige
predict: l l l 
labels: ll for
Validation set perplexity: 5.67
Average loss at step 4300: 0.000011 learning rate: 0.000979
Minibatch perplexity: 9553293.19
predict: o s s 
labels: o smal
predict: et o o
labels: be too
Validation set perplexity: 4.05
Average loss at step 4400: 0.000010 learning rate: 0.000978
Minibatch perplexity: 22720591.02
predict: n o t 
labels: n to b
predict:  sw s 
labels:  shown
Validation set perplexity: 3.59
Average loss at step 4500: 0.000010 learning rate: 0.000978
Minibatch perplexity: 11597545.47
predict: e e e 
labels:  been 
predict:  e a a
labels:  have 
Validation set perplexity: 4.38
Average loss at step 4600: 0.000009 learning rate: 0.000977
Minibatch perplexity: 31623771.73
predict: ngngng
labels: nings 
predict: ee e e
labels: e open
Validation set perplexity: 5.10
Average loss at step 4700: 0.000010 learning rate: 0.000977
Minibatch perplexity: 19792555.77
predict: etet e
labels: ed the
predict:  st s 
labels:  store
Validation set perplexity: 3.27
Average loss at step 4800: 0.000009 learning rate: 0.000976
Minibatch perplexity: 26029422.21
predict: bbbabb
labels: bably 
predict: e rp r
labels: e prob
Validation set perplexity: 3.50
Average loss at step 4900: 0.000009 learning rate: 0.000976
Minibatch perplexity: 30778646.43
predict: sew e 
labels: s were
predict:  sas s
labels:  ashes
Validation set perplexity: 4.56
Average loss at step 5000: 0.000007 learning rate: 0.000975
Minibatch perplexity: 42038432.11
predict: eweweh
labels: where 
predict: a a a 
labels: sada w
Validation set perplexity: 8.38
Average loss at step 5100: 0.000007 learning rate: 0.000975
Minibatch perplexity: 52375688.00
predict: a ata 
labels: at mas
predict: etc c 
labels: cote a
Validation set perplexity: 9.87
Average loss at step 5200: 0.000006 learning rate: 0.000974
Minibatch perplexity: 24661538.66
predict: devo e
labels:  dovec
predict:  et s 
labels:  east 
Validation set perplexity: 2.32
Average loss at step 5300: 0.000006 learning rate: 0.000974
Minibatch perplexity: 14657302.20
predict:  e e e
labels: m the 
predict: nf f f
labels: n from
Validation set perplexity: 3.01
Average loss at step 5400: 0.000005 learning rate: 0.000973
Minibatch perplexity: 25266620.94
predict: ese e 
labels: a seen
predict: adasaa
labels: masada
Validation set perplexity: 7.24
Average loss at step 5500: 0.000005 learning rate: 0.000973
Minibatch perplexity: 29253316.96
predict: a ya y
labels: aphy m
predict: gggggg
labels: geogra
Validation set perplexity: 2.63
Average loss at step 5600: 0.000005 learning rate: 0.000972
Minibatch perplexity: 60870989.60
predict: nenen 
labels: nent g
predict: mimimi
labels:  immin
Validation set perplexity: 4.31
Average loss at step 5700: 0.000005 learning rate: 0.000972
Minibatch perplexity: 37421093.47
predict: eememe
labels: ecame 
predict: eteb e
labels: eat be
Validation set perplexity: 3.97
Average loss at step 5800: 0.000004 learning rate: 0.000971
Minibatch perplexity: 33716612.63
predict: ene e 
labels: n defe
predict: eewe e
labels: e when
Validation set perplexity: 5.41
Average loss at step 5900: 0.000004 learning rate: 0.000971
Minibatch perplexity: 32772688.70
predict: iuicic
labels: uicide
predict: ssssss
labels: ass su
Validation set perplexity: 2.68
Average loss at step 6000: 0.000004 learning rate: 0.000970
Minibatch perplexity: 22557283.20
predict: t em m
labels: ted ma
predict: mmtmmt
labels: ommitt
Validation set perplexity: 2.47
Average loss at step 6100: 0.000004 learning rate: 0.000970
Minibatch perplexity: 49046384.67
predict: e r s 
labels: ers co
predict: eeeeee
labels: efende
Validation set perplexity: 2.60
Average loss at step 6200: 0.000003 learning rate: 0.000969
Minibatch perplexity: 21072174.32
predict: ini n 
labels: ing de
predict: vivivi
labels: urvivi
Validation set perplexity: 2.22
Average loss at step 6300: 0.000003 learning rate: 0.000969
Minibatch perplexity: 29821667.73
predict: lal l 
labels: all su
predict: m s s 
labels: most a
Validation set perplexity: 3.98
2017-07-07 13:33:35.305702: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 51799940 get requests, put_count=51799942 evicted_count=69000 eviction_rate=0.00133205 and unsatisfied allocation rate=0.0013334
Average loss at step 6400: 0.000003 learning rate: 0.000968
Minibatch perplexity: 49444407.46
predict: a la l
labels: ch alm
predict: hwih h
labels: g whic
Validation set perplexity: 4.79
Average loss at step 6500: 0.000003 learning rate: 0.000968
Minibatch perplexity: 27590583.80
predict: dunrug
labels: during
predict: opo o 
labels: oops d
Validation set perplexity: 10.93
Average loss at step 6600: 0.000002 learning rate: 0.000967
Minibatch perplexity: 70358256.41
predict: ana t 
labels: an tro
predict: g m m 
labels: g roma
Validation set perplexity: 2.85
Average loss at step 6700: 0.000002 learning rate: 0.000967
Minibatch perplexity: 45168714.89
predict: igigig
labels: ieging
predict: e e s 
labels: d besi
Validation set perplexity: 4.28
Average loss at step 6800: 0.000002 learning rate: 0.000967
Minibatch perplexity: 96814928.15
predict: na s s
labels: rs and
predict: efenee
labels: fender
Validation set perplexity: 4.09
Average loss at step 6900: 0.000003 learning rate: 0.000966
Minibatch perplexity: 99542698.73
predict: de e e
labels: sh def
predict:  ew i 
labels:  jewis
Validation set perplexity: 8.26
Average loss at step 7000: 0.000002 learning rate: 0.000966
Minibatch perplexity: 66758566.43
predict: i t t 
labels: ite s 
predict: t et t
labels: the si
Validation set perplexity: 2.83
Average loss at step 7100: 0.000002 learning rate: 0.000965
Minibatch perplexity: 36039611.45
predict: ewet e
labels: ween t
predict: t t t 
labels: t betw
Validation set perplexity: 4.58
Average loss at step 7200: 0.000002 learning rate: 0.000965
Minibatch perplexity: 137649193.85
predict: vevovo
labels: revolt
predict: wiw si
labels: wish r
Validation set perplexity: 5.21
Average loss at step 7300: 0.000002 learning rate: 0.000964
Minibatch perplexity: 39745978.77
predict: a t a 
labels: at jew
predict: eege e
labels: e grea
Validation set perplexity: 4.61
Average loss at step 7400: 0.000002 learning rate: 0.000964
Minibatch perplexity: 159839300.38
predict: init t
labels: in the
predict: lat t 
labels: last i
Validation set perplexity: 5.18
Average loss at step 7500: 0.000002 learning rate: 0.000963
Minibatch perplexity: 57911415.29
predict:  et e 
labels:  the l
predict: n f f 
labels: ne of 
Validation set perplexity: 3.09
Average loss at step 7600: 0.000002 learning rate: 0.000963
Minibatch perplexity: 81442075.67
predict: tl l l
labels: tle on
predict: etatbt
labels: e batt
Validation set perplexity: 5.12
Average loss at step 7700: 0.000003 learning rate: 0.000962
Minibatch perplexity: 159981379.28
predict: eeeeee
labels: ree ce
predict: et et 
labels: en thr
Validation set perplexity: 2.33
Average loss at step 7800: 0.000002 learning rate: 0.000962
Minibatch perplexity: 77430847.03
predict: eeeeee
labels: e seve
predict: etrt r
labels: or the
Validation set perplexity: 2.56
Average loss at step 7900: 0.000007 learning rate: 0.000961
Minibatch perplexity: 122793361.15
predict: o ouo 
labels: ous fo
predict: em f m
labels: e famo
Validation set perplexity: 3.69
Average loss at step 8000: 0.000001 learning rate: 0.000961
Minibatch perplexity: 40545459.64
predict: eememe
labels: became
predict: a a a 
labels: sada b
Validation set perplexity: 8.14
Average loss at step 8100: 0.000001 learning rate: 0.000960
Minibatch perplexity: 136847641.38
predict: ea ama
labels: ea mas
predict: ede e 
labels: ead se
Validation set perplexity: 5.92
Average loss at step 8200: 0.000001 learning rate: 0.000960
Minibatch perplexity: 170589343.60
predict: ete e 
labels: the de
predict: gngigi
labels: king t
Validation set perplexity: 5.16
Average loss at step 8300: 0.000001 learning rate: 0.000959
Minibatch perplexity: 135109668.02
predict: elokol
labels: erlook
predict: vrt rt
labels: rt ove
Validation set perplexity: 5.16
Average loss at step 8400: 0.000001 learning rate: 0.000959
Minibatch perplexity: 106394315.02
predict: eee e 
labels:  deser
predict: dnana 
labels: udean 
Validation set perplexity: 11.31
Average loss at step 8500: 0.000001 learning rate: 0.000958
Minibatch perplexity: 133096891.69
predict: t eh u
labels: the ju
predict: e f f 
labels: e of t
Validation set perplexity: 6.47
Average loss at step 8600: 0.000001 learning rate: 0.000958
Minibatch perplexity: 104790718.17
predict: ene e 
labels: n edge
predict: astrts
labels: astern
Validation set perplexity: 4.70
Average loss at step 8700: 0.000001 learning rate: 0.000957
Minibatch perplexity: 105930426.85
predict: etete 
labels: the ea
predict: u u u 
labels: u on t
Validation set perplexity: 4.59
Average loss at step 8800: 0.000001 learning rate: 0.000957
Minibatch perplexity: 155727529.68
predict: latata
labels: lateau
predict: opopo 
labels: ock pl
Validation set perplexity: 4.29
Average loss at step 8900: 0.000001 learning rate: 0.000956
Minibatch perplexity: 233515546.41
predict: t et e
labels: ted ro
predict: iltsil
labels: isolat
Validation set perplexity: 2.85
Average loss at step 9000: 0.000001 learning rate: 0.000956
Minibatch perplexity: 93652316.89
predict: f n a 
labels: f an i
predict: tofofo
labels: top of
Validation set perplexity: 3.52
Average loss at step 9100: 0.000001 learning rate: 0.000955
Minibatch perplexity: 295999871.62
predict: l n l 
labels: l on t
predict: ilsiri
labels: israel
Validation set perplexity: 3.28
Average loss at step 9200: 0.000001 learning rate: 0.000955
Minibatch perplexity: 159705427.17
predict: i i i 
labels: s in i
predict: tstnat
labels: ations
Validation set perplexity: 6.07
Average loss at step 9300: 0.000001 learning rate: 0.000955
Minibatch perplexity: 194387630.08
predict: ititit
labels: tifica
predict: df f f
labels: d fort
Validation set perplexity: 3.07
Average loss at step 9400: 0.000001 learning rate: 0.000954
Minibatch perplexity: 51521553.07
predict: ed s a
labels: es and
predict: ealaca
labels: palace
Validation set perplexity: 3.12
Average loss at step 9500: 0.000001 learning rate: 0.000954
Minibatch perplexity: 84221467.85
predict: iet t 
labels: ient p
predict: f nac 
labels: f anci
Validation set perplexity: 4.13
Average loss at step 9600: 0.000001 learning rate: 0.000953
Minibatch perplexity: 157749079.80
predict: if t t
labels: ite of
predict: t et t
labels: the si
Validation set perplexity: 2.70
Average loss at step 9700: 0.000001 learning rate: 0.000953
Minibatch perplexity: 308560857.54
predict: t t t 
labels: t is t
predict: estsis
labels: ess it
Validation set perplexity: 2.39
Average loss at step 9800: 0.000000 learning rate: 0.000952
Minibatch perplexity: 484913938.11
predict: rfrtrt
labels: fortre
predict: ngngng
labels: ning f
Validation set perplexity: 2.11
Average loss at step 9900: 0.000000 learning rate: 0.000952
Minibatch perplexity: 315709607.63
predict: amamam
labels: a mean
predict: etudut
labels: etzuda
Validation set perplexity: 5.33
Average loss at step 10000: 0.000001 learning rate: 0.000951
Minibatch perplexity: 237592927.27
predict: om m m
labels: ord me
predict: t ewt 
labels: the wo
Validation set perplexity: 6.66
Average loss at step 10100: 0.000000 learning rate: 0.000951
Minibatch perplexity: 292555555.85
predict: f f f 
labels: from t
predict: ivef f
labels: ived f
Validation set perplexity: 1.77
Average loss at step 10200: 0.000000 learning rate: 0.000950
Minibatch perplexity: 240763287.97
predict: a d d 
labels: a deri
predict: edataz
labels: etzada
Validation set perplexity: 5.78
Average loss at step 10300: 0.000000 learning rate: 0.000950
Minibatch perplexity: 205147222.58
predict: memmmm
labels: ame me
predict: ne n r
labels: rew na
Validation set perplexity: 8.13
Average loss at step 10400: 0.000000 learning rate: 0.000949
Minibatch perplexity: 175608523.46
predict: eeee e
labels: e hebr
predict: of t f
labels: of the
Validation set perplexity: 2.02
Average loss at step 10500: 0.000000 learning rate: 0.000949
Minibatch perplexity: 420592150.62
predict: tono o
labels: tion o
predict: roruru
labels: orrupt
Validation set perplexity: 2.70
Average loss at step 10600: 0.000001 learning rate: 0.000948
Minibatch perplexity: 414843625.79
predict: tninic
labels: tin co
predict:  a a a
labels:  a lat
Validation set perplexity: 3.14
Average loss at step 10700: 0.000000 learning rate: 0.000948
Minibatch perplexity: 151870433.95
predict: d d s 
labels: da is 
predict:  ada a
labels:  masad
Validation set perplexity: 5.92
Average loss at step 10800: 0.000000 learning rate: 0.000947
Minibatch perplexity: 84393164.73
predict: itisit
labels: itias 
predict: ili i 
labels: r mili
Validation set perplexity: 6.62
Average loss at step 10900: 0.000000 learning rate: 0.000947
Minibatch perplexity: 241873829.40
predict: rar r 
labels: ry war
predict: tninit
labels: tionar
Validation set perplexity: 1.92
Average loss at step 11000: 0.000000 learning rate: 0.000946
Minibatch perplexity: 476596252.45
predict: evolul
labels: evolut
predict: nar c 
labels: can re
Validation set perplexity: 3.64
Average loss at step 11100: 0.000000 learning rate: 0.000946
Minibatch perplexity: 200788892.41
predict: aemram
labels: americ
predict:  e m m
labels:  men a
Validation set perplexity: 3.15
Average loss at step 11200: 0.000000 learning rate: 0.000945
Minibatch perplexity: 302342155.78
predict: nint t
labels: inute 
predict: t et e
labels: the mi
Validation set perplexity: 3.39
Average loss at step 11300: 0.000000 learning rate: 0.000945
Minibatch perplexity: 251651543.06
predict: n f f 
labels: n of t
predict: ntotat
labels: zation
Validation set perplexity: 4.29
Average loss at step 11400: 0.000000 learning rate: 0.000945
Minibatch perplexity: 361520184.88
predict: ninrin
labels: rganiz
predict: ororor
labels: org or
Validation set perplexity: 1.79
Average loss at step 11500: 0.000000 learning rate: 0.000944
Minibatch perplexity: 304782872.25
predict: toro o
labels: tory o
predict: sususu
labels: ushist
Validation set perplexity: 3.13
Average loss at step 11600: 0.000000 learning rate: 0.000944
Minibatch perplexity: 463108603.46
predict: eememe
labels: emen u
predict: nenutu
labels: minute
Validation set perplexity: 2.29
Average loss at step 11700: 0.000000 learning rate: 0.000943
Minibatch perplexity: 406477771.59
predict:  e e t
labels:  the m
predict:  e e e
labels:  were 
Validation set perplexity: 10.12
Average loss at step 11800: 0.000000 learning rate: 0.000943
Minibatch perplexity: 736347088.73
predict: s s s 
labels: s who 
predict:  il il
labels:  links
Validation set perplexity: 4.17
2017-07-07 13:36:05.962754: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 59394901 get requests, put_count=59394902 evicted_count=79000 eviction_rate=0.00133008 and unsatisfied allocation rate=0.00133128
Average loss at step 11900: 0.000000 learning rate: 0.000942
Minibatch perplexity: 868157254.41
predict: enenal
labels: ernal 
predict: eeeeee
labels: e exte
Validation set perplexity: 4.30
Average loss at step 12000: 0.000000 learning rate: 0.000942
Minibatch perplexity: 796865127.67
predict: ini ni
labels: ix one
predict: ivisi 
labels: ive si
Validation set perplexity: 3.83
Average loss at step 12100: 0.000000 learning rate: 0.000941
Minibatch perplexity: 624944914.98
predict: ef f f
labels: ero fi
predict: eezez 
labels: ero ze
Validation set perplexity: 2.47
Average loss at step 12200: 0.000000 learning rate: 0.000941
Minibatch perplexity: 1106720987.01
predict: evene 
labels: ven ze
predict: vo s s
labels: wo sev
Validation set perplexity: 2.64
Average loss at step 12300: 0.000000 learning rate: 0.000940
Minibatch perplexity: 528294679.76
predict: sis si
labels: six tw
predict: ez o s
labels: zero s
Validation set perplexity: 2.57
Average loss at step 12400: 0.000000 learning rate: 0.000940
Minibatch perplexity: 595011496.05
predict: zzzz z
labels: zero z
predict: is s z
labels: isbn z
Validation set perplexity: 1.90
Average loss at step 12500: 0.000000 learning rate: 0.000939
Minibatch perplexity: 1191653141.39
predict: uu uu 
labels: upuy i
predict: n n r 
labels: r n du
Validation set perplexity: 2.90
Average loss at step 12600: 0.000000 learning rate: 0.000939
Minibatch perplexity: 656070453.61
predict: trortr
labels: trevor
predict:  na t 
labels:  and t
Validation set perplexity: 13.71
Average loss at step 12700: 0.000000 learning rate: 0.000938
Minibatch perplexity: 740912344.37
predict: dudu u
labels: dupuy 
predict: iet t 
labels: iest d
Validation set perplexity: 7.37
Average loss at step 12800: 0.000000 learning rate: 0.000938
Minibatch perplexity: 257221285.42
predict: rir ri
labels: r erni
predict: t r r 
labels: t by r
Validation set perplexity: 7.65
Average loss at step 12900: 0.000000 learning rate: 0.000937
Minibatch perplexity: 542434327.97
predict: eeetee
labels: resent
predict: t et e
labels: the pr
Validation set perplexity: 2.94
Average loss at step 13000: 0.000000 learning rate: 0.000937
Minibatch perplexity: 810678829.33
predict:  t t t
labels: c to t
predict: r o r 
labels: ro b c
Validation set perplexity: 9.41
Average loss at step 13100: 0.000000 learning rate: 0.000937
Minibatch perplexity: 916967875.96
predict: ror r 
labels: ro zer
predict: eeee e
labels: ve zer
Validation set perplexity: 4.05
Average loss at step 13200: 0.000000 learning rate: 0.000936
Minibatch perplexity: 286281691.70
predict: eefefe
labels: ee fiv
predict: e et e
labels: m thre
Validation set perplexity: 2.37
Average loss at step 13300: 0.000000 learning rate: 0.000936
Minibatch perplexity: 353049037.53
predict: yf f f
labels: y from
predict: isosts
labels: istory
Validation set perplexity: 3.64
Average loss at step 13400: 0.000000 learning rate: 0.000935
Minibatch perplexity: 1043319817.50
predict: ar h r
labels: ary hi
predict: ilitil
labels: milita
Validation set perplexity: 9.43
Average loss at step 13500: 0.000000 learning rate: 0.000935
Minibatch perplexity: 1069225550.04
predict: a f f 
labels: a of m
predict: dedidi
labels: opedia
Validation set perplexity: 9.19
Average loss at step 13600: 0.000000 learning rate: 0.000934
Minibatch perplexity: 940044483.94
predict: ncnccc
labels: ncyclo
predict: eteete
labels: the en
Validation set perplexity: 3.91
Average loss at step 13700: 0.000000 learning rate: 0.000934
Minibatch perplexity: 757202354.65
predict:  s s i
labels:  six t
predict: evevev
labels: seven 
Validation set perplexity: 10.01
Average loss at step 13800: 0.000000 learning rate: 0.000933
Minibatch perplexity: 1195355845.31
predict: foru r
labels: four s
predict: if fif
labels: ight f
Validation set perplexity: 2.15
Average loss at step 13900: 0.000000 learning rate: 0.000933
Minibatch perplexity: 518810496.22
predict: eht eh
labels: ght ei
predict: ir r r
labels: ro eig
Validation set perplexity: 3.09
Average loss at step 14000: 0.000000 learning rate: 0.000932
Minibatch perplexity: 1222532900.42
predict: eeee e
labels: ve zer
predict: nevf f
labels: ne fiv
Validation set perplexity: 2.11
Average loss at step 14100: 0.000000 learning rate: 0.000932
Minibatch perplexity: 1169131785.46
predict: nnnnnn
labels: ne nin
predict: eo o o
labels: ero on
Validation set perplexity: 8.99
Average loss at step 14200: 0.000000 learning rate: 0.000931
Minibatch perplexity: 902931419.65
predict: s z z 
labels: sbn ze
predict: hiriri
labels: her is
Validation set perplexity: 3.48
Average loss at step 14300: 0.000000 learning rate: 0.000931
Minibatch perplexity: 1873034457.97
predict:  fif i
labels:  fisch
predict: etette
labels: ckett 
Validation set perplexity: 3.98
Average loss at step 14400: 0.000000 learning rate: 0.000930
Minibatch perplexity: 1654796849.90
predict: id h h
labels: id hac
predict: d da a
labels: y davi
Validation set perplexity: 2.58
Average loss at step 14500: 0.000000 learning rate: 0.000930
Minibatch perplexity: 314149398.93
predict: de d e
labels: ide by
predict: e r s 
labels: e s ri
Validation set perplexity: 2.13
Average loss at step 14600: 0.000000 learning rate: 0.000930
Minibatch perplexity: 1296302293.16
predict: ererer
labels: revere
predict: pul ru
labels: paul r
Validation set perplexity: 13.90
Average loss at step 14700: 0.000000 learning rate: 0.000929
Minibatch perplexity: 1075429706.61
predict: nec c 
labels: nces p
predict: eeeeee
labels: eferen
Validation set perplexity: 3.70
Average loss at step 14800: 0.000000 learning rate: 0.000929
Minibatch perplexity: 473058044.44
predict: elete 
labels: tle re
predict: et tat
labels: e batt
Validation set perplexity: 3.44
Average loss at step 14900: 0.000000 learning rate: 0.000928
Minibatch perplexity: 1021990584.52
predict: nif fi
labels: n five
predict: eeveve
labels:  seven
Validation set perplexity: 11.31
Average loss at step 15000: 0.000000 learning rate: 0.000928
Minibatch perplexity: 926707177.66
predict: evevev
labels: seven 
predict:  n o s
labels:  one s
Validation set perplexity: 3.05
Average loss at step 15100: 0.000000 learning rate: 0.000927
Minibatch perplexity: 1530538026.63
predict:  e h e
labels: g the 
predict: dnrugi
labels: during
Validation set perplexity: 3.55
Average loss at step 15200: 0.000000 learning rate: 0.000927
Minibatch perplexity: 1542246899.28
predict: d drd 
labels: cord d
predict: nncncn
labels: n conc
Validation set perplexity: 4.87
Average loss at step 15300: 0.000000 learning rate: 0.000926
Minibatch perplexity: 2904497368.44
predict: l ei l
labels: led in
predict: elll l
labels: e kill
Validation set perplexity: 3.22
Average loss at step 15400: 0.000000 learning rate: 0.000926
Minibatch perplexity: 2032184539.99
predict:  e t t
labels:  to be
predict: fif tr
labels: first 
Validation set perplexity: 1.73
Average loss at step 15500: 0.000000 learning rate: 0.000925
Minibatch perplexity: 624744208.24
predict:  d a a
labels:  and f
predict: ilitit
labels: litia 
Validation set perplexity: 3.85
Average loss at step 15600: 0.000000 learning rate: 0.000925
Minibatch perplexity: 2803444892.31
predict: oni l 
labels: on mil
predict: et tc 
labels: e acto
Validation set perplexity: 5.22
Average loss at step 15700: 0.000000 learning rate: 0.000924
Minibatch perplexity: 1287263878.82
predict: of t f
labels: of the
predict: tna t 
labels: tain o
Validation set perplexity: 5.78
Average loss at step 15800: 0.000000 learning rate: 0.000924
Minibatch perplexity: 1007977708.13
predict: et et 
labels: e capt
predict: ist t 
labels: is the
Validation set perplexity: 3.50
Average loss at step 15900: 0.000000 learning rate: 0.000923
Minibatch perplexity: 1175372982.73
predict: c aic 
labels: c davi
predict:  aiasa
labels:  isaac
Validation set perplexity: 3.18
Average loss at step 16000: 0.000000 learning rate: 0.000923
Minibatch perplexity: 1371231863.36
predict: e d d 
labels: ed on 
predict: ee e e
labels: e base
Validation set perplexity: 2.30
Average loss at step 16100: 0.000000 learning rate: 0.000923
Minibatch perplexity: 2056463391.29
predict:  e t t
labels:  to be
predict:  d s a
labels:  said 
Validation set perplexity: 4.10
Average loss at step 16200: 0.000000 learning rate: 0.000922
Minibatch perplexity: 1280018335.29
predict: ss ss 
labels: ss is 
predict: ekeeke
labels: ikenes
Validation set perplexity: 2.99
Average loss at step 16300: 0.000000 learning rate: 0.000922
Minibatch perplexity: 669382021.73
predict: e e s 
labels: e s li
predict: tstttt
labels: statue
Validation set perplexity: 5.31
Average loss at step 16400: 0.000000 learning rate: 0.000921
Minibatch perplexity: 1484467177.26
predict:  e t t
labels:  the s
predict: nll la
labels: nally 
Validation set perplexity: 2.27
Average loss at step 16500: 0.000000 learning rate: 0.000921
Minibatch perplexity: 1613084736.01
predict: ititit
labels: dition
predict: d dad 
labels: d trad
Validation set perplexity: 4.23
Average loss at step 16600: 0.000000 learning rate: 0.000920
Minibatch perplexity: 874678764.48
predict:  o o o
labels:  world
predict: d e t 
labels: d the 
Validation set perplexity: 1.77
Average loss at step 16700: 0.000000 learning rate: 0.000920
Minibatch perplexity: 1393263609.65
predict:  nru r
labels:  round
predict: hdeh a
labels: heard 
Validation set perplexity: 4.90
Average loss at step 16800: 0.000000 learning rate: 0.000919
Minibatch perplexity: 1188297878.08
predict: shthsh
labels: shot h
predict: os s s
labels: ords s
Validation set perplexity: 2.09
Average loss at step 16900: 0.000000 learning rate: 0.000919
Minibatch perplexity: 555120033.32
predict: t at t
labels: tal wo
predict: imrmim
labels: immort
Validation set perplexity: 3.37
Average loss at step 17000: 0.000000 learning rate: 0.000918
Minibatch perplexity: 688206622.59
predict:  e t t
labels:  the i
predict:  i t i
labels:  with 
Validation set perplexity: 5.23
Average loss at step 17100: 0.000000 learning rate: 0.000918
Minibatch perplexity: 575879732.04
predict:  y y y
labels:  hymn 
predict: nonoc 
labels: ncord 
Validation set perplexity: 4.19
Average loss at step 17200: 0.000000 learning rate: 0.000917
Minibatch perplexity: 607347051.65
predict: nen n 
labels: en con
predict: eeeeee
labels: e seve
Validation set perplexity: 4.30
Average loss at step 17300: 0.000000 learning rate: 0.000917
Minibatch perplexity: 2271851967.52
predict: etere 
labels:  three
predict: eht eh
labels: eight 
Validation set perplexity: 2.96
Average loss at step 17400: 0.000000 learning rate: 0.000917
Minibatch perplexity: 992321420.03
predict:  e e e
labels:  one e
predict: s s s 
labels: son s 
Validation set perplexity: 2.68
2017-07-07 13:38:37.814857: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 67035660 get requests, put_count=67035660 evicted_count=89000 eviction_rate=0.00132765 and unsatisfied allocation rate=0.00132873
Average loss at step 17500: 0.000000 learning rate: 0.000916
Minibatch perplexity: 2331376007.77
predict: eeee e
labels:  emers
predict: dl d l
labels: waldo 
Validation set perplexity: 6.18
Average loss at step 17600: 0.000000 learning rate: 0.000916
Minibatch perplexity: 2517187543.84
predict: a h h 
labels: alph w
predict:  f f r
labels:  of ra
Validation set perplexity: 9.33
Average loss at step 17700: 0.000000 learning rate: 0.000915
Minibatch perplexity: 949223657.37
predict: anatat
labels: tanza 
predict: inis s
labels: ing st
Validation set perplexity: 2.73
Average loss at step 17800: 0.000000 learning rate: 0.000915
Minibatch perplexity: 2152812498.15
predict:  ni p 
labels:  openi
predict: s t t 
labels: s the 
Validation set perplexity: 2.23
Average loss at step 17900: 0.000000 learning rate: 0.000914
Minibatch perplexity: 1863814847.23
predict: et t t
labels: ent is
predict: eeeiem
labels: pedime
Validation set perplexity: 3.72
Average loss at step 18000: 0.000000 learning rate: 0.000914
Minibatch perplexity: 1827938851.91
predict:  e t t
labels:  the p
predict: e d d 
labels: ed on 
Validation set perplexity: 3.85
Average loss at step 18100: 0.000000 learning rate: 0.000913
Minibatch perplexity: 1744154106.00
predict: sirscr
labels: scribe
predict: nan na
labels: an ins
Validation set perplexity: 3.90
Average loss at step 18200: 0.000000 learning rate: 0.000913
Minibatch perplexity: 769970716.00
predict: netnau
labels: nutema
predict: id i m
labels: rd min
Validation set perplexity: 4.31
Average loss at step 18300: 0.000000 learning rate: 0.000912
Minibatch perplexity: 1102248819.89
predict: cococo
labels: concor
predict:  e t e
labels:  the c
Validation set perplexity: 2.07
Average loss at step 18400: 0.000000 learning rate: 0.000912
Minibatch perplexity: 734614773.01
predict: ol il 
labels: orial 
predict: mmmmmm
labels: n memo
Validation set perplexity: 3.09
Average loss at step 18500: 0.000000 learning rate: 0.000912
Minibatch perplexity: 1378231572.40
predict: ninoni
labels: incoln
predict: tl et 
labels: the li
Validation set perplexity: 5.93
Average loss at step 18600: 0.000000 learning rate: 0.000911
Minibatch perplexity: 518441881.39
predict: wtit t
labels: with t
predict: gnog o
labels: long w
Validation set perplexity: 4.22
Average loss at step 18700: 0.000000 learning rate: 0.000911
Minibatch perplexity: 1804001808.34
predict: uses l
labels: ues al
predict: t tttt
labels:  statu
Validation set perplexity: 2.16
Average loss at step 18800: 0.000000 learning rate: 0.000910
Minibatch perplexity: 446459269.64
predict: nwnwnw
labels: known 
predict: ell l 
labels: well k
Validation set perplexity: 12.83
Average loss at step 18900: 0.000000 learning rate: 0.000910
Minibatch perplexity: 908631543.91
predict: ms s s
labels: most w
predict:  i s i
labels:  his m
Validation set perplexity: 5.13
Average loss at step 19000: 0.000000 learning rate: 0.000909
Minibatch perplexity: 2095406803.28
predict: n f f 
labels: ne of 
predict: edec c
labels: ced on
Validation set perplexity: 7.33
Average loss at step 19100: 0.000000 learning rate: 0.000909
Minibatch perplexity: 1752143161.53
predict: duoruo
labels: produc
predict: sio so
labels: sion p
Validation set perplexity: 3.60
Average loss at step 19200: 0.000000 learning rate: 0.000908
Minibatch perplexity: 1386576800.90
predict: msmsms
labels: ommiss
predict: ojo oj
labels: jor co
Validation set perplexity: 3.90
Average loss at step 19300: 0.000000 learning rate: 0.000908
Minibatch perplexity: 832373149.53
predict: s a a 
labels: st maj
predict: sis si
labels: s firs
Validation set perplexity: 2.79
Average loss at step 19400: 0.000000 learning rate: 0.000907
Minibatch perplexity: 458103943.72
predict: i ihi 
labels: in his
predict: enec c
labels: ench i
Validation set perplexity: 2.08
Average loss at step 19500: 0.000000 learning rate: 0.000907
Minibatch perplexity: 1043498007.35
predict: ererer
labels: er fre
predict: eehese
labels: cheste
Validation set perplexity: 3.18
Average loss at step 19600: 0.000000 learning rate: 0.000907
Minibatch perplexity: 3025472887.97
predict: nenil 
labels: niel c
predict: ed d d
labels: es dan
Validation set perplexity: 2.57
Average loss at step 19700: 0.000000 learning rate: 0.000906
Minibatch perplexity: 914496538.98
predict:  fo f 
labels:  force
predict: iitiii
labels: itish 
Validation set perplexity: 2.76
Average loss at step 19800: 0.000000 learning rate: 0.000906
Minibatch perplexity: 2760001331.07
predict: t ort 
labels: to bri
predict: anac c
labels: ance t
Validation set perplexity: 2.19
Average loss at step 19900: 0.000000 learning rate: 0.000905
Minibatch perplexity: 3247593761.28
predict: estsst
labels: esista
predict: ememem
labels: med re
Validation set perplexity: 4.68
Average loss at step 20000: 0.000000 learning rate: 0.000905
Minibatch perplexity: 1327234877.70
predict: lara l
labels: ul arm
predict: esfsfs
labels: cessfu
Validation set perplexity: 4.63
Average loss at step 20100: 0.000000 learning rate: 0.000904
Minibatch perplexity: 1627688371.69
predict: tcucuc
labels: t succ
predict:  rirt 
labels:  first
Validation set perplexity: 2.59
Average loss at step 20200: 0.000000 learning rate: 0.000904
Minibatch perplexity: 894456581.04
predict: f f t 
labels: f the 
predict: ar yr 
labels: ary of
Validation set perplexity: 4.15
Average loss at step 20300: 0.000000 learning rate: 0.000903
Minibatch perplexity: 818896455.88
predict: enenen
labels: entena
predict: ehecec
labels: the ce
Validation set perplexity: 3.84
Average loss at step 20400: 0.000000 learning rate: 0.000903
Minibatch perplexity: 955739942.13
predict: n f f 
labels: n of t
predict: irtrtr
labels: ration
Validation set perplexity: 4.10
Average loss at step 20500: 0.000000 learning rate: 0.000902
Minibatch perplexity: 1236811727.65
predict: mmmmmm
labels: mmemor
predict: ini c 
labels: in com
Validation set perplexity: 2.06
Average loss at step 20600: 0.000000 learning rate: 0.000902
Minibatch perplexity: 2109675341.52
predict: emam m
labels: eman i
predict: nenutu
labels: minute
Validation set perplexity: 2.26
Average loss at step 20700: 0.000000 learning rate: 0.000902
Minibatch perplexity: 696152086.40
predict: c o o 
labels: cord m
predict: c c c 
labels: y conc
Validation set perplexity: 2.69
Average loss at step 20800: 0.000000 learning rate: 0.000901
Minibatch perplexity: 2032841622.32
predict: lelaca
labels: legacy
predict: anarar
labels: arns l
Validation set perplexity: 3.61
Average loss at step 20900: 0.000000 learning rate: 0.000901
Minibatch perplexity: 1630957381.91
predict: ana a 
labels: and ba
predict: sssss 
labels: uses a
Validation set perplexity: 2.99
Average loss at step 21000: 0.000000 learning rate: 0.000900
Minibatch perplexity: 1790852437.85
predict: ini u 
labels: in hou
predict: a s r 
labels: ards i
Validation set perplexity: 5.20
Average loss at step 21100: 0.000000 learning rate: 0.000900
Minibatch perplexity: 1148232708.97
predict: oooooo
labels: oorboa
predict: t f f 
labels: th flo
Validation set perplexity: 8.63
Average loss at step 21200: 0.000000 learning rate: 0.000899
Minibatch perplexity: 379116665.22
predict: eeeter
labels: erneat
predict: sn s u
labels: s unde
Validation set perplexity: 2.67
Average loss at step 21300: 0.000000 learning rate: 0.000899
Minibatch perplexity: 1505841211.14
predict:  eit t
labels:  items
predict:  d i i
labels:  hide 
Validation set perplexity: 4.12
Average loss at step 21400: 0.000000 learning rate: 0.000898
Minibatch perplexity: 1373315869.19
predict: r r t 
labels: re to 
predict: e s s 
labels: ds wer
Validation set perplexity: 5.73
Average loss at step 21500: 0.000000 learning rate: 0.000898
Minibatch perplexity: 2124740171.26
predict: dedede
labels: method
predict: memem 
labels: ment m
Validation set perplexity: 3.04
Average loss at step 21600: 0.000000 learning rate: 0.000897
Minibatch perplexity: 1289069607.53
predict: nenaca
labels: ncealm
predict: ar c r
labels: ar con
Validation set perplexity: 2.82
Average loss at step 21700: 0.000000 learning rate: 0.000897
Minibatch perplexity: 1358912384.59
predict: ppupup
labels: popula
predict: trert 
labels: ther p
Validation set perplexity: 2.50
Average loss at step 21800: 0.000000 learning rate: 0.000897
Minibatch perplexity: 545788226.39
predict: et s t
labels: eas ot
predict: ede e 
labels: ed are
Validation set perplexity: 3.44
Average loss at step 21900: 0.000000 learning rate: 0.000896
Minibatch perplexity: 1129532000.05
predict:  o o o
labels:  woode
predict:  o s r
labels: ds or 
Validation set perplexity: 3.31
Average loss at step 22000: 0.000000 learning rate: 0.000896
Minibatch perplexity: 498747597.58
predict:  l i l
labels:  field
predict: n n n 
labels: en in 
Validation set perplexity: 4.71
Average loss at step 22100: 0.000000 learning rate: 0.000895
Minibatch perplexity: 865311863.53
predict: neetem
labels: nuteme
predict: b m m 
labels: by min
Validation set perplexity: 12.55
Average loss at step 22200: 0.000000 learning rate: 0.000895
Minibatch perplexity: 1130241545.91
predict: nih h 
labels: hind b
predict: et e t
labels: ft beh
Validation set perplexity: 3.48
Average loss at step 22300: 0.000000 learning rate: 0.000894
Minibatch perplexity: 623021974.04
predict: lo f f
labels: or lef
predict: dnd d 
labels: dden o
Validation set perplexity: 5.26
2017-07-07 13:40:53.356147: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 73802366 get requests, put_count=73802368 evicted_count=99000 eviction_rate=0.00134142 and unsatisfied allocation rate=0.00134237
Average loss at step 22400: 0.262144 learning rate: 0.000894
Minibatch perplexity: 31330892.51
predict:  neh n
labels: en hid
predict: t fe p
labels: e ofte
Validation set perplexity: 8.51
Average loss at step 22500: 0.039142 learning rate: 0.000893
Minibatch perplexity: 230800646.10
predict: s s s 
labels: s were
predict:  t iti
labels:  items
Validation set perplexity: 44.96
Average loss at step 22600: 0.001924 learning rate: 0.000893
Minibatch perplexity: 2273504817.81
predict: ete t 
labels: these 
predict: tnoit 
labels: tion t
Validation set perplexity: 10.80
Average loss at step 22700: 0.000016 learning rate: 0.000893
Minibatch perplexity: 1755753873.10
predict: enatcu
labels: recaut
predict:  n    
labels: s a pr
Validation set perplexity: 9.47
Average loss at step 22800: 0.000004 learning rate: 0.000892
Minibatch perplexity: 2462566220.62
predict: ole s 
labels: ols as
predict: t t t 
labels:  patro
Validation set perplexity: 117.53
Average loss at step 22900: 0.000010 learning rate: 0.000892
Minibatch perplexity: 937116012.06
predict: ititit
labels: itish 
predict: b nb y
labels: by bri
Validation set perplexity: 15.10
Average loss at step 23000: 0.000002 learning rate: 0.000891
Minibatch perplexity: 2969559703.87
predict: dez d 
labels: ized b
predict: n n n 
labels: ng sei
Validation set perplexity: 35.63
Average loss at step 23100: 0.000249 learning rate: 0.000891
Minibatch perplexity: 1158195983.59
predict: b a y 
labels: y bein
predict: atatat
labels: tantly
Validation set perplexity: 13.13
Average loss at step 23200: 0.000001 learning rate: 0.000890
Minibatch perplexity: 3884623329.75
predict: c t c 
labels:  const
predict:  e    
labels:  were 
Validation set perplexity: 8.89
Average loss at step 23300: 0.000001 learning rate: 0.000890
Minibatch perplexity: 1794655874.65
predict:  t t t
labels: d but 
predict: dedede
labels: demand
Validation set perplexity: 20.48
Average loss at step 23400: 0.000002 learning rate: 0.000889
Minibatch perplexity: 4314772633.95
predict: drt t 
labels: hort d
predict:  s    
labels:  in sh
Validation set perplexity: 9.32
Average loss at step 23500: 0.000001 learning rate: 0.000889
Minibatch perplexity: 374061125.94
predict:  a  o 
labels:  only 
predict: e t t 
labels: e not 
Validation set perplexity: 9.84
Average loss at step 23600: 0.000001 learning rate: 0.000889
Minibatch perplexity: 2476550722.61
predict: sre s 
labels: s were
predict: peplpl
labels: pplies
Validation set perplexity: 4.95
Average loss at step 23700: 0.000001 learning rate: 0.000888
Minibatch perplexity: 1347383821.95
predict: p s p 
labels: nd sup
predict: inain 
labels: ion an
Validation set perplexity: 18.60
Average loss at step 23800: 0.000204 learning rate: 0.000888
Minibatch perplexity: 1531007997.59
predict: mimimi
labels: muniti
predict: mm mmm
labels: on amm
Validation set perplexity: 13.19
Average loss at step 23900: 0.000006 learning rate: 0.000887
Minibatch perplexity: 2638902738.13
predict: tatit 
labels: utatio
predict: yreb y
labels: y repu
Validation set perplexity: 31.00
Average loss at step 24000: 0.000001 learning rate: 0.000887
Minibatch perplexity: 647079202.58
predict: dadldl
labels: deadly
predict:  a    
labels: rs a d
Validation set perplexity: 2.48
Average loss at step 24100: 0.000001 learning rate: 0.000886
Minibatch perplexity: 4952316133.94
predict: ort t 
labels: hooter
predict: hrahsr
labels: harpsh
Validation set perplexity: 5.28
Average loss at step 24200: 0.000000 learning rate: 0.000886
Minibatch perplexity: 905628973.18
predict: mse m 
labels: men sh
predict: msuitn
labels: inutem
Validation set perplexity: 6.89
Average loss at step 24300: 0.000001 learning rate: 0.000885
Minibatch perplexity: 1213379321.77
predict: m m m 
labels: ned mi
predict: e a x 
labels: p earn
Validation set perplexity: 19.08
Average loss at step 24400: 0.000001 learning rate: 0.000885
Minibatch perplexity: 2701950401.57
predict: asap a
labels: anship
predict: amsams
labels: arksma
Validation set perplexity: 9.84
Average loss at step 24500: 0.000000 learning rate: 0.000885
Minibatch perplexity: 590843721.21
predict: m m m 
labels: lop ma
predict: ede e 
labels:  devel
Validation set perplexity: 11.85
Average loss at step 24600: 0.000000 learning rate: 0.000884
Minibatch perplexity: 1722546703.73
predict:  t    
labels: ng to 
predict: hununu
labels: huntin
Validation set perplexity: 4.05
Average loss at step 24700: 0.000000 learning rate: 0.000884
Minibatch perplexity: 1329027997.37
predict:  a  f 
labels: e of h
predict: memimi
labels: fetime
Validation set perplexity: 4.15
Average loss at step 24800: 0.000000 learning rate: 0.000883
Minibatch perplexity: 1784672015.93
predict: m af f
labels:  a lif
predict:  t t t
labels:  with 
Validation set perplexity: 17.74
Average loss at step 24900: 0.000000 learning rate: 0.000883
Minibatch perplexity: 2362257881.47
predict: a a af
labels: along 
predict: ililil
labels: ifle a
Validation set perplexity: 10.89
Average loss at step 25000: 0.000000 learning rate: 0.000882
Minibatch perplexity: 1615561694.74
predict: t t t 
labels: the ri
predict:  t t f
labels: y of t
Validation set perplexity: 8.95
Average loss at step 25100: 0.000002 learning rate: 0.000882
Minibatch perplexity: 1567597934.66
predict: carccc
labels: curacy
predict: cadc c
labels: nd acc
Validation set perplexity: 11.14
Average loss at step 25200: 0.000000 learning rate: 0.000881
Minibatch perplexity: 1316100573.49
predict: na n n
labels: nge an
predict: ede a 
labels: ed ran
Validation set perplexity: 4.14
Average loss at step 25300: 0.000000 learning rate: 0.000881
Minibatch perplexity: 399568229.45
predict: caebec
labels: crease
predict: c e c 
labels: he inc
Validation set perplexity: 6.30
Average loss at step 25400: 0.000000 learning rate: 0.000881
Minibatch perplexity: 1049513983.64
predict: e t h 
labels: nge th
predict: t t t 
labels: to ran
Validation set perplexity: 49.55
Average loss at step 25500: 0.000000 learning rate: 0.000880
Minibatch perplexity: 2430447659.84
predict: tet t 
labels: et int
predict: de d d
labels: uld ge
Validation set perplexity: 7.18
Average loss at step 25600: 0.000000 learning rate: 0.000880
Minibatch perplexity: 3730236157.73
predict: srs s 
labels: sh cou
predict: ititit
labels: britis
Validation set perplexity: 4.47
Average loss at step 25700: 0.000000 learning rate: 0.000879
Minibatch perplexity: 330488224.53
predict:  a    
labels:  the b
predict: eref f
labels: efore 
Validation set perplexity: 14.06
Average loss at step 25800: 0.000000 learning rate: 0.000879
Minibatch perplexity: 2523065119.03
predict: eb s s
labels: ops be
predict: rt ror
labels: r troo
Validation set perplexity: 12.87
Average loss at step 25900: 0.000000 learning rate: 0.000878
Minibatch perplexity: 6288837118.91
predict: t t t 
labels:  other
predict: r r r 
labels: er or 
Validation set perplexity: 10.08
Average loss at step 26000: 0.000000 learning rate: 0.000878
Minibatch perplexity: 1520023235.52
predict: de oc 
labels: d cove
predict: edebeh
labels: behind
Validation set perplexity: 12.79
Average loss at step 26100: 0.000000 learning rate: 0.000878
Minibatch perplexity: 1002411367.12
predict: ba a a
labels: back b
predict: laflll
labels: fall b
Validation set perplexity: 4.96
Average loss at step 26200: 0.000000 learning rate: 0.000877
Minibatch perplexity: 1106247048.61
predict:  a a a
labels:  and f
predict:  r f f
labels:  fire 
Validation set perplexity: 12.99
Average loss at step 26300: 0.000000 learning rate: 0.000877
Minibatch perplexity: 1831585440.35
predict: c oc f
labels: could 
predict: eee e 
labels: emen c
Validation set perplexity: 8.21
Average loss at step 26400: 0.000000 learning rate: 0.000876
Minibatch perplexity: 3356362446.08
predict: msmsmi
labels: minute
predict:  t t  
labels:  the m
Validation set perplexity: 11.15
Average loss at step 26500: 0.000000 learning rate: 0.000876
Minibatch perplexity: 3384461717.77
predict: srs s 
labels: shers 
predict: mimimi
labels: kirmis
Validation set perplexity: 12.87
2017-07-07 13:42:49.603966: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 79568345 get requests, put_count=79568347 evicted_count=109000 eviction_rate=0.00136989 and unsatisfied allocation rate=0.00137077
Average loss at step 26600: 0.000000 learning rate: 0.000875
Minibatch perplexity: 1927395634.23
predict: sa s s
labels:  as sk
predict: mr m m
labels: rming 
Validation set perplexity: 22.53
Average loss at step 26700: 0.000000 learning rate: 0.000875
Minibatch perplexity: 691931220.17
predict: rerfrf
labels: perfor
predict: ep e p
labels: when p
Validation set perplexity: 3.87
Average loss at step 26800: 0.000000 learning rate: 0.000874
Minibatch perplexity: 1335258274.08
predict: it t f
labels: ting w
predict: rt t t
labels: r hunt
Validation set perplexity: 7.95
Average loss at step 26900: 0.000000 learning rate: 0.000874
Minibatch perplexity: 2444788820.17
predict: dref f
labels: ed for
predict: ererer
labels: eferre
Validation set perplexity: 8.54
Average loss at step 27000: 0.000000 learning rate: 0.000874
Minibatch perplexity: 2767806474.90
predict: rere r
labels: re pre
predict: ut t t
labels: ut wer
Validation set perplexity: 9.43
#+END_EXAMPLE


--------------


***** 2
#+NAME: p2a
#+BEGIN_SRC python :var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = p2a7(num_chars=1)  :var s9 = p2a9 :var s10 = p2a10 :var s11 = p2a11
#+END_SRC

#+BEGIN_SRC python :results none
  SEED = np.random.randint(low=np.iinfo(np.uint32).min, high=np.iinfo(np.uint32).max, size=1)[0]
  len_seq = num_unrollings+1
  maxout_size = 500
  kDense = tf.contrib.keras.layers.Dense
#+END_SRC

#+BEGIN_SRC python
  def try_get_var(name, scope, shape=None, dtype=None, initializer=None):
    try:
      with tf.variable_scope(scope):
        var = tf.get_variable(name, shape=shape, dtype=tf.float32, initializer=initializer)
    except ValueError:
      with tf.variable_scope(scope, reuse=True):
        var = tf.get_variable(name)
    return var
#+END_SRC

#+RESULTS:

sequence length helper:
#+RESULTS:
#+BEGIN_SRC python
  def get_lengths(X):
    return tf.reduce_sum(tf.sign(tf.reduce_sum(tf.abs(X), axis=2)),axis=1)
#+END_SRC

#+RESULTS:

dynamic_rnn encoder / decoder
#+BEGIN_SRC python
  def encoder(X, cellfw, cellbw, graph):
     with graph.as_default():
        with tf.variable_scope("encoder"):     
           out, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cellfw, cell_bw=cellbw,
                                                           inputs=X,
                                                           # initial_state_fw=state_in_fw[i],
                                                           dtype=tf.float32, 
                                                           # sequence_length=seq_lengths,
                                                           scope='rnn', time_major=False)
           state_fw, state_bw = states
           out = tf.concat(out, -1)
     return out, state_fw
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  categorical = tf.contrib.distributions.Categorical
  class SamplingHelper(object):
    def __init__(self, logits, sampling_probability=.5, schedueling_seed=None, 
                 logits_seed=None):
      self.sample_prob = sampling_probability
      self.logits = logits
      self.batch_size = tf.size(logits[:,0])
      self.schedueling_seed = schedueling_seed
      self.logits_seed = logits_seed
    def sample(self):
      select_sample_noise = tf.random_uniform([self.batch_size], seed=self.schedueling_seed)
      select_sample = (self.sample_prob > select_sample_noise)
      sample_id_sampler = categorical(logits=self.logits, name='id_sampler')      
      sampled_ids = sample_id_sampler.sample(seed=self.logits_seed, name='sampled_ids')
      return tf.where(select_sample, sampled_ids, tf.tile([-1], [self.batch_size]),
                      name="sample_and_mark")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  class Decoder(object):
    """
    home made decoder with attention
    Primarily based on Bahdanau etal NMT 14, appendix A.2.2 (though fixed length atm)
    H_bar: input hidden states. shape = (B x T_in x n_hidden)
    """
    def __init__(self, emb_labels, H_bar, s_bar, embeddings, 
                 initializer=tf.truncated_normal_initializer(0,.05), 
                 len_v=64):
      self.W = try_get_var("W_E", tf.get_variable_scope(),
                           shape=(embedding_size,3*num_nodes+maxout_size*2),
                           initializer=initializer)
      self.U = try_get_var("U", tf.get_variable_scope(),
                           shape=(num_nodes, 3*num_nodes+maxout_size*2),
                           initializer=initializer)
      self.C = try_get_var("C", tf.get_variable_scope(),
                           shape=(2*num_nodes, 3*num_nodes+maxout_size*2), 
                           initializer=initializer)
      self.emb_labels, self.H_bar, self.s_bar, self.initializer, self.len_v, self.embeddings =\
        (emb_labels, H_bar, s_bar, initializer, len_v, embeddings)
      self.out_dense = kDense(n_tokens)  # linear transformation
      self.fw_dense = kDense(num_nodes, activation=tf.nn.elu)
      self.fw_proj = kDense(num_nodes)      
    def query(self,s):
      """
      s: current state. shape = (B x n_hidden). concatenation of context and state
      """
      with tf.variable_scope("attention"):
        va = try_get_var("va", tf.get_variable_scope(), shape=(self.len_v), 
                         initializer=self.initializer)
        Wa = try_get_var("Wa", tf.get_variable_scope(), shape=(num_nodes*3,self.len_v),
                            initializer=initializer)
        ba = try_get_var("ba", tf.get_variable_scope(), shape=(self.len_v),
                            initializer=tf.zeros_initializer())  # bias
        M1 = tf.einsum('ijk,kl->ijl',self.H_bar, Wa[:2*num_nodes,:])
        M2 = tf.expand_dims(tf.matmul(s, Wa[2*num_nodes:,:])+ba, 1)
        logits_t = tf.einsum('ijk,k->ij',tf.tanh(M1+M2), va)  # score_t
        a_t = tf.nn.softmax(logits_t, dim=-1)
        return tf.reduce_sum(tf.multiply(self.H_bar, tf.expand_dims(a_t, -1)), axis=1)  # context  
    def step(self, y_prev, s):
      """
      predict based on previous hidden state and response
      TODO: enable scheduled sampling
      In:
        s: last time step's state. shape = (B x n_hidden)
        y_prev: last time step's /embedded/ response. shape = (B x embedding_size). 
          Can be either sample from last softmax prediction or last label
      Out:
        current logits and state
      """
      c = self.query(s)
      Eproj = y_prev
      zr = tf.sigmoid(tf.matmul(Eproj, self.W[:,num_nodes:3*num_nodes])+
                      tf.matmul(s, self.U[:,num_nodes:3*num_nodes])+
                      tf.matmul(c, self.C[:,num_nodes:3*num_nodes]))
      s_tilde = tf.tanh(tf.matmul(Eproj,self.W[:,:num_nodes])+
                        tf.matmul(zr[:,num_nodes:]*s,self.U[:,:num_nodes])+
                        tf.matmul(c, self.C[:,:num_nodes]))
      s = (1-zr[:,:num_nodes])*s+zr[:,:num_nodes]*s_tilde
      t_tilde = tf.matmul(s,self.U[:,3*num_nodes:])+tf.matmul(Eproj,self.W[:,3*num_nodes:])+\
                tf.matmul(c,self.C[:,3*num_nodes:])
      t = tf.reduce_max(tf.reshape(t_tilde,[-1,maxout_size,2]),axis=2)
      return self.out_dense(t), s
    def train(self):
      with tf.variable_scope("decoder/train"):
        def body(time, s, y_prev, out_ta):
          """
          predict based on previous hidden state and prediction
          time: current timestep
          s: last step's state. shape = (B x n_hidden)
          out_ta: dynamic tensorarray to store outputs (can be static, atm)
          """
          with tf.variable_scope("body"):
            # y_prev = self.emb_labels[:,time]
            logits, s = self.step(y_prev, s)
            out_ta_p1 = out_ta.write(time,logits)
            id_sampler = SamplingHelper(logits)
            sample_ids = id_sampler.sample()
            where_sampling = tf.cast(tf.where(sample_ids > -1), tf.int32)
            where_not_sampling = tf.cast(tf.where(sample_ids <= -1), tf.int32)
            where_sampling_flat = tf.reshape(where_sampling, [-1])
            where_not_sampling_flat = tf.reshape(where_not_sampling, [-1])
            sample_ids_sampling = tf.gather(sample_ids, where_sampling, 
                                            name="sampled_ids")
            emb_sampled = tf.nn.embedding_lookup(self.embeddings,sample_ids_sampling)
            inputs_not_sampling = tf.gather(self.emb_labels[:,time], where_not_sampling)
            base_shape = tf.shape(y_prev)
            # assert_op = tf.Assert(tf.less_equal(tf.shape(where_sampling)[-1],
            #                                     tf.rank(base_shape)),
            #                       [tf.shape(where_sampling),
            #                        where_sampling,base_shape],
            #                       summarize=None)
            # with tf.control_dependencies([assert_op]):
            scatter_samples = tf.scatter_nd(indices=where_sampling, 
                                            updates=tf.squeeze(emb_sampled),
                                            shape=base_shape, name="scatter_samples")
            scatter_response = tf.scatter_nd(indices=where_not_sampling,
                                             updates=tf.squeeze(inputs_not_sampling),
                                             shape=base_shape,
                                             name="scatter_response")
            y_next = scatter_samples + scatter_response
          return [time+1, s, y_next, out_ta_p1]
        i0 = tf.constant(0, dtype=tf.int32)
        s0 = self.fw_proj(self.fw_dense(self.H_bar[:,-1,:num_nodes])) # forward state out from encoder
        y0 = tf.zeros_like(H_bar[:,0,:])
        out_ta0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True,
                                 element_shape=tf.TensorShape([None,n_tokens]))
          # doesn't need dynamic size, but for future use...
        cond = lambda i,s,y,ta: i < len_seq
        i,s,y,out_ta = tf.while_loop(cond, body, [i0,s0,y0,out_ta0])
        logits = tf.transpose(out_ta.stack(), [1,0,2])
      return logits,s
    def predict(self):
      with tf.variable_scope("decoder/sample"):
        def body(time, s, y_prev, out_ta):
          """
          predict based on previous hidden state and prediction
          time: current timestep
          s: last step's state. shape = (B x n_hidden)
          out_ta: dynamic tensorarray to store outputs (can be static, atm)
          """
          with tf.variable_scope("body"):
            logits, s = self.step(y_prev, s)
            out_ta_p1 = out_ta.write(time,logits)
            # todo: beamsearch / etc. (not argmax)
            y_next = tf.nn.embedding_lookup(self.embeddings, tf.argmax(logits, axis=-1))
          return [time+1, s, y_next, out_ta_p1]
        i0 = tf.constant(0, dtype=tf.int32)
        s0 = self.fw_proj(self.fw_dense(self.H_bar[:,-1,:num_nodes]))
        y0 = tf.zeros_like(H_bar[:,0,:])
        out_ta0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True,
                                 element_shape=tf.TensorShape([None,n_tokens]))
        cond = lambda i,s,y,ta: i < len_seq
        i,s,y,out_ta = tf.while_loop(cond, body, [i0,s0,y0,out_ta0])
        logits = tf.transpose(out_ta.stack(), [1,0,2])
      return logits,s
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  tf.reset_default_graph()
  LSTMCell = tf.contrib.rnn.LSTMCell
  graph = tf.Graph()
  with graph.as_default():
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.int32, shape=[None]))
    train_inputs = tf.transpose(tf.stack(train_data))
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    emb_inputs = tf.nn.embedding_lookup(embeddings, train_inputs)
    train_labels = tf.reverse(train_inputs, axis=[1])
    emb_labels = tf.reverse(emb_inputs, axis=[1])
    # model
    initializer = tf.truncated_normal_initializer(0,.05)
    enc_cell_fw = LSTMCell(num_nodes)
    enc_cell_bw = LSTMCell(num_nodes)
    # dec_cell = LSTMCell(num_nodes)
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    H_bar, s_bar = encoder(emb_inputs, enc_cell_fw, 
                           enc_cell_bw, graph)
    decoder = Decoder(emb_labels, H_bar, s_bar, embeddings, 
                      initializer=tf.truncated_normal_initializer(0,.05), len_v=64)
    logits, state = decoder.train()
    # Unrolled LSTM loop.
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
      logits=logits, labels=tf.one_hot(train_labels, n_tokens, axis=-1)))
    # Optimizer
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(.001, global_step, 1, 0.999995, 
                                               staircase=True)
    optimizer = tf.train.AdamOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    valid_logits, valid_state = decoder.predict()
    valid_prediction = tf.nn.softmax(valid_logits)
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  import pdb
  num_steps = 100001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      # session.run(tf.get_variable("decoder/train/while/body/sampled_ids"), feed_dict=feed_dict)
      # get variable needs reuse
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        # np.max(np.diff(np.transpose(np.stack([np.flip(np.stack(batches).T, axis=1), np.argmax(predictions, axis=2)]), [1,2,0]), axis=2))
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.stack(batches).T
        predictions = predictions.reshape([-1,predictions.shape[-1]])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels.reshape(-1)))))
        # Measure validation set perplexity.
        valid_logprob = 0
        sentence = ''
        for _ in range(valid_size):
          batches = valid_batches.next()
          feed_dict = dict()
          for i in range(len(batches)):
            feed_dict[train_data[i]] = batches[i]
          predictions = session.run(valid_prediction, feed_dict=feed_dict)[0]
          feed = np.int32(np.argmax(predictions,axis=1))
          predictions=predictions.reshape([-1,predictions.shape[-1]])
          labels = np.flip(np.concatenate(batches), axis=0)
          valid_logprob =+ logprob(predictions,labels.reshape(-1))
          print("predict: "+''.join(characters(feed, is_one_hot=False)))
          print("labels: "+''.join(characters(labels, is_one_hot=False)))
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC


--------------


***** 1
#+NAME: p2a
#+BEGIN_SRC python :var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = p2a7(num_chars=1)  :var s9 = p2a9 :var s10 = p2a10 :var s11 = p2a11
#+END_SRC

#+BEGIN_SRC python :results none
  SEED = np.random.randint(low=np.iinfo(np.uint32).min, high=np.iinfo(np.uint32).max, size=1)[0]
  len_seq = num_unrollings+1
  maxout_size = 500
  kDense = tf.contrib.keras.layers.Dense
#+END_SRC

#+BEGIN_SRC python
  def try_get_var(name, scope, shape=None, dtype=None, initializer=None):
    try:
      with tf.variable_scope(scope):
        var = tf.get_variable(name, shape=shape, dtype=tf.float32, initializer=initializer)
    except ValueError:
      with tf.variable_scope(scope, reuse=True):
        var = tf.get_variable(name)
    return var
#+END_SRC

#+RESULTS:

sequence length helper:
#+RESULTS:
#+BEGIN_SRC python
  def get_lengths(X):
    return tf.reduce_sum(tf.sign(tf.reduce_sum(tf.abs(X), axis=2)),axis=1)
#+END_SRC

#+RESULTS:

dynamic_rnn encoder / decoder
#+BEGIN_SRC python
  def encoder(X, cellfw, cellbw, graph):
     with graph.as_default():
        with tf.variable_scope("encoder"):     
           out, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cellfw, cell_bw=cellbw,
                                                           inputs=X,
                                                           # initial_state_fw=state_in_fw[i],
                                                           dtype=tf.float32, 
                                                           # sequence_length=seq_lengths,
                                                           scope='rnn', time_major=False)
           state_fw, state_bw = states
           out = tf.concat(out, -1)
     return out, state_fw
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  categorical = tf.contrib.distributions.Categorical
  class SamplingHelper(object):
    def __init__(self, logits, sampling_probability=.5, schedueling_seed=None, 
                 logits_seed=None):
      self.sample_prob = sampling_probability
      self.logits = logits
      self.batch_size = tf.size(logits[:,0])
      self.schedueling_seed = schedueling_seed
      self.logits_seed = logits_seed
    def sample(self):
      select_sample_noise = tf.random_uniform([self.batch_size], seed=self.schedueling_seed)
      select_sample = (self.sample_prob > select_sample_noise)
      sample_id_sampler = categorical(logits=self.logits, name='id_sampler')      
      sampled_ids = sample_id_sampler.sample(seed=self.logits_seed, name='sampled_ids')
      return tf.where(select_sample, sampled_ids, tf.tile([-1], [self.batch_size]),
                      name="sample_and_mark")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  class Decoder(object):
    """
    home made decoder with attention
    Primarily based on Bahdanau etal NMT 14, appendix A.2.2 (though fixed length atm)
    H_bar: input hidden states. shape = (B x T_in x n_hidden)
    """
    def __init__(self, emb_labels, H_bar, s_bar, embeddings, 
                 initializer=tf.truncated_normal_initializer(0,.05), 
                 len_v=64):
      self.W = try_get_var("W_E", tf.get_variable_scope(),
                           shape=(embedding_size,3*num_nodes+maxout_size*2),
                           initializer=initializer)
      self.U = try_get_var("U", tf.get_variable_scope(),
                           shape=(num_nodes, 3*num_nodes+maxout_size*2),
                           initializer=initializer)
      self.C = try_get_var("C", tf.get_variable_scope(),
                           shape=(2*num_nodes, 3*num_nodes+maxout_size*2), 
                           initializer=initializer)
      self.emb_labels, self.H_bar, self.s_bar, self.initializer, self.len_v, self.embeddings =\
        (emb_labels, H_bar, s_bar, initializer, len_v, embeddings)
      self.out_dense = kDense(n_tokens)  # linear transformation
      self.fw_dense = kDense(num_nodes, activation=tf.nn.elu)
      self.fw_proj = kDense(num_nodes)      
    def query(self,s):
      """
      s: current state. shape = (B x n_hidden). concatenation of context and state
      """
      with tf.variable_scope("attention"):
        va = try_get_var("va", tf.get_variable_scope(), shape=(self.len_v), 
                         initializer=self.initializer)
        Wa = try_get_var("Wa", tf.get_variable_scope(), shape=(num_nodes*3,self.len_v),
                            initializer=initializer)
        ba = try_get_var("ba", tf.get_variable_scope(), shape=(self.len_v),
                            initializer=tf.zeros_initializer())  # bias
        M1 = tf.einsum('ijk,kl->ijl',self.H_bar, Wa[:2*num_nodes,:])
        M2 = tf.expand_dims(tf.matmul(s, Wa[2*num_nodes:,:])+ba, 1)
        logits_t = tf.einsum('ijk,k->ij',tf.tanh(M1+M2), va)  # score_t
        a_t = tf.nn.softmax(logits_t, dim=-1)
        return tf.reduce_sum(tf.multiply(self.H_bar, tf.expand_dims(a_t, -1)), axis=1)  # context  
    def step(self, y_prev, s):
      """
      predict based on previous hidden state and response
      TODO: enable scheduled sampling
      In:
        s: last time step's state. shape = (B x n_hidden)
        y_prev: last time step's /embedded/ response. shape = (B x embedding_size). 
          Can be either sample from last softmax prediction or last label
      Out:
        current logits and state
      """
      c = self.query(s)
      Eproj = y_prev
      zr = tf.sigmoid(tf.matmul(Eproj, self.W[:,num_nodes:3*num_nodes])+
                      tf.matmul(s, self.U[:,num_nodes:3*num_nodes])+
                      tf.matmul(c, self.C[:,num_nodes:3*num_nodes]))
      s_tilde = tf.tanh(tf.matmul(Eproj,self.W[:,:num_nodes])+
                        tf.matmul(zr[:,num_nodes:]*s,self.U[:,:num_nodes])+
                        tf.matmul(c, self.C[:,:num_nodes]))
      s = (1-zr[:,:num_nodes])*s+zr[:,:num_nodes]*s_tilde
      t_tilde = tf.matmul(s,self.U[:,3*num_nodes:])+tf.matmul(Eproj,self.W[:,3*num_nodes:])+\
                tf.matmul(c,self.C[:,3*num_nodes:])
      t = tf.reduce_max(tf.reshape(t_tilde,[-1,maxout_size,2]),axis=2)
      return self.out_dense(t), s
    def train(self):
      with tf.variable_scope("decoder/train"):
        def body(time, s, y_prev, out_ta):
          """
          predict based on previous hidden state and prediction
          time: current timestep
          s: last step's state. shape = (B x n_hidden)
          out_ta: dynamic tensorarray to store outputs (can be static, atm)
          """
          with tf.variable_scope("body"):
            # y_prev = self.emb_labels[:,time]
            logits, s = self.step(y_prev, s)
            out_ta_p1 = out_ta.write(time,logits)
            id_sampler = SamplingHelper(logits)
            sample_ids = id_sampler.sample()
            where_sampling = tf.cast(tf.where(sample_ids > -1), tf.int32)
            where_not_sampling = tf.cast(tf.where(sample_ids <= -1), tf.int32)
            where_sampling_flat = tf.reshape(where_sampling, [-1])
            where_not_sampling_flat = tf.reshape(where_not_sampling, [-1])
            sample_ids_sampling = tf.gather(sample_ids, where_sampling, 
                                            name="sampled_ids")
            emb_sampled = tf.nn.embedding_lookup(self.embeddings,sample_ids_sampling)
            inputs_not_sampling = tf.gather(self.emb_labels[:,time], where_not_sampling)
            base_shape = tf.shape(y_prev)
            # assert_op = tf.Assert(tf.less_equal(tf.shape(where_sampling)[-1],
            #                                     tf.rank(base_shape)),
            #                       [tf.shape(where_sampling),
            #                        where_sampling,base_shape],
            #                       summarize=None)
            # with tf.control_dependencies([assert_op]):
            scatter_samples = tf.scatter_nd(indices=where_sampling, 
                                            updates=tf.squeeze(emb_sampled),
                                            shape=base_shape, name="scatter_samples")
            scatter_response = tf.scatter_nd(indices=where_not_sampling,
                                             updates=tf.squeeze(inputs_not_sampling),
                                             shape=base_shape,
                                             name="scatter_response")
            y_next = scatter_samples + scatter_response
          return [time+1, s, y_next, out_ta_p1]
        i0 = tf.constant(0, dtype=tf.int32)
        s0 = self.fw_proj(self.fw_dense(self.H_bar[:,-1,:num_nodes])) # forward state out from encoder
        y0 = tf.zeros_like(H_bar[:,0,:])
        out_ta0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True,
                                 element_shape=tf.TensorShape([None,n_tokens]))
          # doesn't need dynamic size, but for future use...
        cond = lambda i,s,y,ta: i < len_seq
        i,s,y,out_ta = tf.while_loop(cond, body, [i0,s0,y0,out_ta0])
        logits = tf.transpose(out_ta.stack(), [1,0,2])
      return logits,s
    def predict(self):
      with tf.variable_scope("decoder/sample"):
        def body(time, s, y_prev, out_ta):
          """
          predict based on previous hidden state and prediction
          time: current timestep
          s: last step's state. shape = (B x n_hidden)
          out_ta: dynamic tensorarray to store outputs (can be static, atm)
          """
          with tf.variable_scope("body"):
            logits, s = self.step(y_prev, s)
            out_ta_p1 = out_ta.write(time,logits)
            # todo: beamsearch / etc. (not argmax)
            y_next = tf.nn.embedding_lookup(self.embeddings, tf.argmax(logits, axis=-1))
          return [time+1, s, y_next, out_ta_p1]
        i0 = tf.constant(0, dtype=tf.int32)
        s0 = self.fw_proj(self.fw_dense(self.H_bar[:,-1,:num_nodes]))
        y0 = tf.zeros_like(H_bar[:,0,:])
        out_ta0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True,
                                 element_shape=tf.TensorShape([None,n_tokens]))
        cond = lambda i,s,y,ta: i < len_seq
        i,s,y,out_ta = tf.while_loop(cond, body, [i0,s0,y0,out_ta0])
        logits = tf.transpose(out_ta.stack(), [1,0,2])
      return logits,s
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python
  tf.reset_default_graph()
  LSTMCell = tf.contrib.rnn.LSTMCell
  graph = tf.Graph()
  with graph.as_default():
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.int32, shape=[None]))
    train_inputs = tf.transpose(tf.stack(train_data))
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    emb_inputs = tf.nn.embedding_lookup(embeddings, train_inputs)
    train_labels = tf.reverse(train_inputs, axis=[1])
    emb_labels = tf.reverse(emb_inputs, axis=[1])
    # model
    initializer = tf.truncated_normal_initializer(0,.05)
    enc_cell_fw = LSTMCell(num_nodes)
    enc_cell_bw = LSTMCell(num_nodes)
    # dec_cell = LSTMCell(num_nodes)
    embeddings = tf.Variable(
      tf.random_uniform([n_tokens, embedding_size], -1.0, 1.0))
    H_bar, s_bar = encoder(emb_inputs, enc_cell_fw, 
                           enc_cell_bw, graph)
    decoder = Decoder(emb_labels, H_bar, s_bar, embeddings, 
                      initializer=tf.truncated_normal_initializer(0,.05), len_v=64)
    logits, state = decoder.train()
    # Unrolled LSTM loop.
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
      logits=logits, labels=tf.one_hot(train_labels, n_tokens, axis=-1)))
    # Optimizer
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.1, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    valid_logits, valid_state = decoder.predict()
    valid_prediction = tf.nn.softmax(valid_logits)
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python 
  import pdb
  num_steps = 100001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      # session.run(tf.get_variable("decoder/train/while/body/sampled_ids"), feed_dict=feed_dict)
      # get variable needs reuse
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        # np.max(np.diff(np.transpose(np.stack([np.flip(np.stack(batches).T, axis=1), np.argmax(predictions, axis=2)]), [1,2,0]), axis=2))
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.stack(batches).T
        predictions = predictions.reshape([-1,predictions.shape[-1]])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels.reshape(-1)))))
        # Measure validation set perplexity.
        valid_logprob = 0
        sentence = ''
        for _ in range(valid_size):
          batches = valid_batches.next()
          feed_dict = dict()
          for i in range(len(batches)):
            feed_dict[train_data[i]] = batches[i]
          predictions = session.run(valid_prediction, feed_dict=feed_dict)[0]
          feed = np.int32(np.argmax(predictions,axis=1))
          predictions=predictions.reshape([-1,predictions.shape[-1]])
          labels = np.flip(np.concatenate(batches), axis=0)
          valid_logprob =+ logprob(predictions,labels.reshape(-1))
          print("predict: "+''.join(characters(feed, is_one_hot=False)))
          print("labels: "+''.join(characters(labels, is_one_hot=False)))
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+BEGIN_EXAMPLE
  Initialized
  Average loss at step 0: 3.364511 learning rate: 10.000000
  Minibatch perplexity: 28.97
  predict:       
  labels: e six 
  predict:       
  labels: e nine
  Validation set perplexity: 130.14
  2017-07-07 12:37:25.600714: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2819 get requests, put_count=2953 evicted_count=1000 eviction_rate=0.338639 and unsatisfied allocation rate=0.31536
  2017-07-07 12:37:25.600737: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
  2017-07-07 12:37:27.094724: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 26109 get requests, put_count=26183 evicted_count=1000 eviction_rate=0.0381927 and unsatisfied allocation rate=0.0377265
  2017-07-07 12:37:27.094748: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
  Average loss at step 100: 41.391151 learning rate: 10.000000
  Minibatch perplexity: 21402678.02
  predict:  f es 
  labels: in one
  predict:  d e f
  labels: ated i
  Validation set perplexity: 1209.93
  Average loss at step 200: 52.563408 learning rate: 10.000000
  Minibatch perplexity: 41832985.49
  predict: tsaas 
  labels: excava
  predict: a n nt
  labels: vely e
  Validation set perplexity: 12500.32
  Average loss at step 300: 83.531518 learning rate: 10.000000
  Minibatch perplexity: 8759895.81
  predict: rneeee
  labels: tensiv
  predict: rlneau
  labels: nd ext
  Validation set perplexity: 2205.73
  Average loss at step 400: 96.772405 learning rate: 10.000000
  Minibatch perplexity: 15973394.00
  predict: iiiiii
  labels: two an
  predict: kibibi
  labels: four t
  Validation set perplexity: 57568.93
  Average loss at step 500: 111.326227 learning rate: 10.000000
  Minibatch perplexity: 772763976.66
  predict: l     
  labels: ight f
  predict: l     
  labels: one ei
  Validation set perplexity: 14677.99
  Average loss at step 600: 166.126075 learning rate: 10.000000
  Minibatch perplexity: 2103373558.00
  predict: nnnnnn
  labels: d in o
  predict: eeeeee
  labels: tified
  Validation set perplexity: 14677.99
  Average loss at step 700: 179.645434 learning rate: 10.000000
  Minibatch perplexity: 714798097.47
  predict: t iiii
  labels:  ident
  predict: l  d d
  labels: a was 
  Validation set perplexity: 602.61
  Average loss at step 800: 145.129588 learning rate: 10.000000
  Minibatch perplexity: 62541311.98
  predict: aamama
  labels: masada
  predict: tno o 
  labels: e of m
  Validation set perplexity: 466.10
  Average loss at step 900: 161.796305 learning rate: 10.000000
  Minibatch perplexity: 302523976.69
  predict: llllil
  labels: e site
  predict: llllll
  labels: ss the
  Validation set perplexity: 99999.97
  Average loss at step 1000: 194.219363 learning rate: 10.000000
  Minibatch perplexity: 316649291.76
  predict: oetoro
  labels: ortres
  predict: mt t t
  labels: the fo
  Validation set perplexity: 99999.97
  Average loss at step 1100: 200.495961 learning rate: 10.000000
  Minibatch perplexity: 147873970.37
  predict: ihtoht
  labels: s to t
  predict: caaaaa
  labels: access
  Validation set perplexity: 14571.23
  Average loss at step 1200: 263.498572 learning rate: 10.000000
  Minibatch perplexity: 360089796.39
  predict:       
  labels: form a
  predict:       
  labels:  platf
  Validation set perplexity: 14677.99
  Average loss at step 1300: 341.575149 learning rate: 10.000000
  Minibatch perplexity: 635384210.20
  predict: htvtvt
  labels: today 
  predict: hivvvv
  labels: site t
  Validation set perplexity: 14686.17
  Average loss at step 1400: 255.174022 learning rate: 10.000000
  Minibatch perplexity: 219886490.21
  predict: ththth
  labels:  the s
  predict: hhthth
  labels: omans 
  Validation set perplexity: 99999.97
  Average loss at step 1500: 188.591053 learning rate: 10.000000
  Minibatch perplexity: 87809152.74
  predict: jjrjrj
  labels: the ro
  predict: jjrjij
  labels: m to t
  Validation set perplexity: 99999.97
  Average loss at step 1600: 239.283987 learning rate: 10.000000
  Minibatch perplexity: 386005486.21
  predict: asrcaa
  labels: rbatim
  predict: hsdede
  labels: de ver
  Validation set perplexity: 77086.07
  Average loss at step 1700: 215.145208 learning rate: 10.000000
  Minibatch perplexity: 78908989.08
  predict: c lsui
  labels: suicid
  predict: cpmsms
  labels: mass s
  Validation set perplexity: 699.89
  Average loss at step 1800: 209.164423 learning rate: 10.000000
  Minibatch perplexity: 192115544.09
  predict: e e ht
  labels:  the m
  predict: nooooo
  labels: or to 
  Validation set perplexity: 5789.26
  Average loss at step 1900: 203.907981 learning rate: 10.000000
  Minibatch perplexity: 212624272.38
  predict: sm s m
  labels: s prio
  predict: aenoew
  labels: lowers
  Validation set perplexity: 54982.34
  Average loss at step 2000: 189.990903 learning rate: 10.000000
  Minibatch perplexity: 76747162.71
  predict: ts b s
  labels: s foll
  predict: tit t 
  labels: to his
  Validation set perplexity: 1990.66
  Average loss at step 2100: 186.275447 learning rate: 10.000000
  Minibatch perplexity: 476930214.35
  predict: httooo
  labels: tion t
  predict: ltlooo
  labels: xortat
  Validation set perplexity: 99999.97
  Average loss at step 2200: 244.465649 learning rate: 10.000000
  Minibatch perplexity: 46470385.27
  predict: nanana
  labels: nal ex
  predict:  sfiii
  labels:  s fin
  Validation set perplexity: 82.48
  Average loss at step 2300: 206.874130 learning rate: 10.000000
  Minibatch perplexity: 68254579.04
  predict: lsvlyl
  labels:  yair 
  predict: lvezel
  labels: r ben 
  Validation set perplexity: 99999.97
  Average loss at step 2400: 169.435735 learning rate: 10.000000
  Minibatch perplexity: 256196045.65
  predict:  ab la
  labels: elazar
  predict:  atata
  labels: ated e
  Validation set perplexity: 16175.40
  Average loss at step 2500: 190.140899 learning rate: 10.000000
  Minibatch perplexity: 1843595750.24
  predict: murerx
  labels:  repea
  predict: mudn  
  labels: n and 
  Validation set perplexity: 2182.81
  Average loss at step 2600: 186.035411 learning rate: 10.000000
  Minibatch perplexity: 167356188.07
  predict: hhidid
  labels: ildren
  predict: dhcidw
  labels: ve chi
  Validation set perplexity: 24753.49
  Average loss at step 2700: 131.717733 learning rate: 10.000000
  Minibatch perplexity: 267750309.35
  predict: ueth h
  labels: th fiv
  predict: ue ghg
  labels: ng wit
  Validation set perplexity: 3381.57
  Average loss at step 2800: 170.788847 learning rate: 10.000000
  Minibatch perplexity: 133122262.17
  predict: pnnnnn
  labels: n alon
  predict: peieie
  labels: istern
  Validation set perplexity: 15407.70
  Average loss at step 2900: 177.713907 learning rate: 10.000000
  Minibatch perplexity: 217685393.11
  predict: oo a a
  labels: e a ci
  predict: isibss
  labels: inside
  Validation set perplexity: 1335.07
  Average loss at step 3000: 261.209027 learning rate: 10.000000
  Minibatch perplexity: 89198572.96
  predict: idiiii
  labels: ding i
  predict: ib byb
  labels: by hid
  Validation set perplexity: 8612.64
  Average loss at step 3100: 171.319110 learning rate: 10.000000
  Minibatch perplexity: 80225065.98
  predict: rceeee
  labels: cide b
  predict: rseeee
  labels: e suic
  Validation set perplexity: 16510.37
  Average loss at step 3200: 136.879220 learning rate: 10.000000
  Minibatch perplexity: 129043599.75
  predict: aetttt
  labels: ed the
  predict: amezrt
  labels: urvive
  Validation set perplexity: 13219.47
  Average loss at step 3300: 142.939514 learning rate: 10.000000
  Minibatch perplexity: 94499700.79
  predict: oho wo
  labels: who su
  predict: o oooo
  labels: omen w
  Validation set perplexity: 14677.99
  Average loss at step 3400: 122.796316 learning rate: 10.000000
  Minibatch perplexity: 75154555.19
  predict: owwwtw
  labels: two wo
  predict: os s s
  labels: s by t
  Validation set perplexity: 12541.49
  Average loss at step 3500: 136.690433 learning rate: 10.000000
  Minibatch perplexity: 138155056.67
  predict: tsvvvv
  labels: sephus
  predict: tot t 
  labels: to jos
  Validation set perplexity: 2154.43
  Average loss at step 3600: 120.482277 learning rate: 10.000000
  Minibatch perplexity: 89781228.15
  predict:  aitit
  labels: ated t
  predict:  ryyyy
  labels: y rela
  Validation set perplexity: 23271.76
  Average loss at step 3700: 168.602271 learning rate: 10.000000
  Minibatch perplexity: 385807018.86
  predict: eretrt
  labels: rently
  predict: eppppp
  labels:  appar
  Validation set perplexity: 339.60
  Average loss at step 3800: 119.144887 learning rate: 10.000000
  Minibatch perplexity: 38628966.19
  predict: y paaa
  labels: a was 
  predict: ylaaaa
  labels: masada
  Validation set perplexity: 2062.11
  Average loss at step 3900: 116.277653 learning rate: 10.000000
  Minibatch perplexity: 170511615.17
  predict: n e en
  labels: e of m
  predict: neso i
  labels:  siege
  Validation set perplexity: 20879.04
  Average loss at step 4000: 185.686595 learning rate: 10.000000
  Minibatch perplexity: 325957795.61
  predict: fytmtm
  labels: f the 
  predict: fe    
  labels: unt of
  Validation set perplexity: 14677.99
  Average loss at step 4100: 224.411063 learning rate: 10.000000
  Minibatch perplexity: 234834776.10
  predict: t     
  labels:  accou
  predict: t     
  labels:  this 
  Validation set perplexity: 14677.99
  Average loss at step 4200: 299.897529 learning rate: 10.000000
  Minibatch perplexity: 324648065.86
  predict: nadtdj
  labels: death 
  predict: nhihhh
  labels: heir d
  Validation set perplexity: 6973.39
  Average loss at step 4300: 268.802595 learning rate: 10.000000
  Minibatch perplexity: 156755210.34
  predict: oooooo
  labels:  of th
  predict: oooooo
  labels:  time 
  Validation set perplexity: 99999.97
  Average loss at step 4400: 241.101625 learning rate: 10.000000
  Minibatch perplexity: 188085419.76
  predict: hheeee
  labels: e the 
  predict: hhhhhh
  labels:  chose
  Validation set perplexity: 14677.99
  Average loss at step 4500: 256.712105 learning rate: 10.000000
  Minibatch perplexity: 1916049805.22
  predict: dd nsn
  labels: e and 
  predict: dbolro
  labels: o live
  Validation set perplexity: 5795.82
  Average loss at step 4600: 188.457516 learning rate: 10.000000
  Minibatch perplexity: 125684901.41
  predict: niiiii
  labels: ity to
  predict: niiiii
  labels:  abili
  Validation set perplexity: 1257.71
  Average loss at step 4700: 297.132096 learning rate: 10.000000
  Minibatch perplexity: 54966972.23
  predict: letrdh
  labels: d the 
  predict: vtrtrr
  labels: tained
  Validation set perplexity: 65063.56
  Average loss at step 4800: 248.979124 learning rate: 10.000000
  Minibatch perplexity: 110312437.66
  predict: pppppp
  labels: rs ret
  predict: oppppp
  labels: fender
  Validation set perplexity: 99999.97
  Average loss at step 4900: 225.227463 learning rate: 10.000000
  Minibatch perplexity: 508475889.16
  predict: gggggg
  labels: he def
  predict: gggggg
  labels: hat th
  Validation set perplexity: 99999.97
  Average loss at step 5000: 204.284718 learning rate: 1.000000
  Minibatch perplexity: 172252842.53
  predict: hhohhh
  labels: how th
  predict: ho e t
  labels:  to sh
  Validation set perplexity: 99999.97
  Average loss at step 5100: 21.917315 learning rate: 1.000000
  Minibatch perplexity: 7037841.53
  predict: nidnid
  labels: nding 
  predict: t ttts
  labels: t stan
  Validation set perplexity: 333.40
  Average loss at step 5200: 10.445432 learning rate: 1.000000
  Minibatch perplexity: 16515224.74
  predict: efeeee
  labels: e left
  predict: ss sss
  labels: s were
  Validation set perplexity: 53.95
  Average loss at step 5300: 8.196973 learning rate: 1.000000
  Minibatch perplexity: 4726950.87
  predict: eroeee
  labels: erooms
  predict: se s s
  labels:  store
  Validation set perplexity: 3122.77
  Average loss at step 5400: 6.324035 learning rate: 1.000000
  Minibatch perplexity: 1386563.06
  predict: t tttt
  labels: t the 
  predict: het tt
  labels: e that
  Validation set perplexity: 50.55
  Average loss at step 5500: 5.222189 learning rate: 1.000000
  Minibatch perplexity: 2718552.77
  predict:  sssss
  labels: s made
  predict: etieti
  labels: ent is
  Validation set perplexity: 62.31
  Average loss at step 5600: 4.767456 learning rate: 1.000000
  Minibatch perplexity: 2092594.06
  predict: amraaa
  labels: argume
  predict: h h h 
  labels:  the a
  Validation set perplexity: 77.98
  Average loss at step 5700: 4.004208 learning rate: 1.000000
  Minibatch perplexity: 685213.18
  predict: oodeuu
  labels: count 
  predict: usuu  
  labels: us acc
  Validation set perplexity: 384.43
  Average loss at step 5800: 3.857197 learning rate: 1.000000
  Minibatch perplexity: 586436.14
  predict: oopopo
  labels: osephu
  predict: prprp 
  labels: per jo
  Validation set perplexity: 51.98
  Average loss at step 5900: 3.425815 learning rate: 1.000000
  Minibatch perplexity: 265761.65
  predict: i     
  labels: e as p
  predict: f nna 
  labels: n life
  Validation set perplexity: 348.07
  Average loss at step 6000: 3.101300 learning rate: 1.000000
  Minibatch perplexity: 158418.27
  predict: i si  
  labels: is own
  predict: aekh e
  labels: ake hi
  Validation set perplexity: 14.73
  Average loss at step 6100: 3.081479 learning rate: 1.000000
  Minibatch perplexity: 117308.76
  predict: llllll
  labels: lly ta
  predict: caaaaa
  labels: actual
  Validation set perplexity: 70.11
  Average loss at step 6200: 2.991034 learning rate: 1.000000
  Minibatch perplexity: 419660.64
  predict: e e e 
  labels: e to a
  predict: l yyyy
  labels: ly one
  Validation set perplexity: 127.71
  Average loss at step 6300: 2.441818 learning rate: 1.000000
  Minibatch perplexity: 237784.09
  predict: hyo ee
  labels: he onl
  predict:  t    
  labels:  be th
  Validation set perplexity: 48.62
  Average loss at step 6400: 2.287225 learning rate: 1.000000
  Minibatch perplexity: 653928.96
  predict: wwwwww
  labels: would 
  predict:  wwwww
  labels:  who w
  Validation set perplexity: 24.67
  Average loss at step 6500: 2.918089 learning rate: 1.000000
  Minibatch perplexity: 112479.25
  predict: t atta
  labels: t man 
  predict: e e ee
  labels: e last
  Validation set perplexity: 18.09
  Average loss at step 6600: 2.434510 learning rate: 1.000000
  Minibatch perplexity: 110185.14
  predict: tothte
  labels: to the
  predict: dnowde
  labels: down t
  Validation set perplexity: 17.04
  Average loss at step 6700: 2.293482 learning rate: 1.000000
  Minibatch perplexity: 135532.52
  predict: tntttt
  labels: turn d
  predict: r r r 
  labels: r in t
  Validation set perplexity: 10.83
  Average loss at step 6800: 2.209396 learning rate: 1.000000
  Minibatch perplexity: 96914.96
  predict:  o eh 
  labels:  other
  predict:  eaa  
  labels:  each 
  Validation set perplexity: 10.64
  Average loss at step 6900: 2.062218 learning rate: 1.000000
  Minibatch perplexity: 122236.70
  predict: sissss
  labels: slain 
  predict: ga    
  labels:  and s
  Validation set perplexity: 15.91
  Average loss at step 7000: 1.929848 learning rate: 1.000000
  Minibatch perplexity: 105079.84
  predict: ge t r
  labels:  lots 
  predict: drrard
  labels: drawn 
  Validation set perplexity: 8.66
  Average loss at step 7100: 1.877203 learning rate: 1.000000
  Minibatch perplexity: 203398.52
  predict: hahteh
  labels: have d
  predict: b td t
  labels: d to h
  Validation set perplexity: 6.01
  Average loss at step 7200: 1.670675 learning rate: 1.000000
  Minibatch perplexity: 174881.44
  predict: porepe
  labels: ported
  predict: rerere
  labels: re rep
  Validation set perplexity: 61.00
  Average loss at step 7300: 1.644479 learning rate: 1.000000
  Minibatch perplexity: 83735.55
  predict: r     
  labels: rs wer
  predict: eneeee
  labels: fender
  Validation set perplexity: 30.72
  Average loss at step 7400: 1.692025 learning rate: 1.000000
  Minibatch perplexity: 182330.99
  predict: ehe e 
  labels: he def
  predict: vevete
  labels: ver th
  Validation set perplexity: 5.56
  Average loss at step 7500: 1.546232 learning rate: 1.000000
  Minibatch perplexity: 170180.09
  predict:  h h h
  labels:  howev
  predict: iiiiii
  labels: icide 
  Validation set perplexity: 8.46
  Average loss at step 7600: 1.582650 learning rate: 1.000000
  Minibatch perplexity: 145772.77
  predict: fo f i
  labels: of sui
  predict:  a    
  labels:  act o
  Validation set perplexity: 10.61
  Average loss at step 7700: 1.664099 learning rate: 1.000000
  Minibatch perplexity: 83801.29
  predict: s s s 
  labels: s the 
  predict: uru ru
  labels: urages
  Validation set perplexity: 7.32
  Average loss at step 7800: 1.368202 learning rate: 1.000000
  Minibatch perplexity: 196109.22
  predict: doddoo
  labels: discou
  predict: iooooo
  labels: gion d
  Validation set perplexity: 24.69
  2017-07-07 12:40:48.478272: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 10366075 get requests, put_count=10366076 evicted_count=9000 eviction_rate=0.000868217 and unsatisfied allocation rate=0.000875066
  Average loss at step 7900: 1.369976 learning rate: 1.000000
  Minibatch perplexity: 59273.54
  predict:  r    
  labels:  relig
  predict: ew niw
  labels: ewish 
  Validation set perplexity: 6.87
  Average loss at step 8000: 1.288342 learning rate: 1.000000
  Minibatch perplexity: 165799.40
  predict: thtj h
  labels: the je
  predict: auaehs
  labels: ause t
  Validation set perplexity: 3.36
  Average loss at step 8100: 1.269136 learning rate: 1.000000
  Minibatch perplexity: 86314.51
  predict:  ec   
  labels: s beca
  predict: nenem 
  labels: nemies
  Validation set perplexity: 11.12
  Average loss at step 8200: 1.506984 learning rate: 1.000000
  Minibatch perplexity: 43925.97
  predict: eeeeee
  labels: eir en
  predict: b tb t
  labels: by the
  Validation set perplexity: 19.28
  Average loss at step 8300: 1.691362 learning rate: 1.000000
  Minibatch perplexity: 177480.96
  predict: fatfet
  labels: feat b
  predict: orroro
  labels: or def
  Validation set perplexity: 6.76
  Average loss at step 8400: 1.186586 learning rate: 1.000000
  Minibatch perplexity: 685081.84
  predict: tutuet
  labels: ture o
  predict: na nnn
  labels: n capt
  Validation set perplexity: 9.85
  Average loss at step 8500: 1.196942 learning rate: 1.000000
  Minibatch perplexity: 207120.75
  predict: etetne
  labels: ertain
  predict: acacac
  labels: ace ce
  Validation set perplexity: 8.71
  Average loss at step 8600: 1.300588 learning rate: 1.000000
  Minibatch perplexity: 244648.67
  predict:  ahh a
  labels: han fa
  predict:  eheee
  labels: her th
  Validation set perplexity: 9.67
  Average loss at step 8700: 1.166853 learning rate: 1.000000
  Minibatch perplexity: 628845.20
  predict: ere et
  labels: e rath
  predict: iiuiii
  labels: uicide
  Validation set perplexity: 40.21
  Average loss at step 8800: 1.066096 learning rate: 1.000000
  Minibatch perplexity: 221192.93
  predict: asssss
  labels: ass su
  predict: t tatt
  labels: ted ma
  Validation set perplexity: 3.00
  Average loss at step 8900: 1.645363 learning rate: 1.000000
  Minibatch perplexity: 24398.62
  predict: mmomom
  labels: ommitt
  predict: adaaaa
  labels: and co
  Validation set perplexity: 9.98
  Average loss at step 9000: 1.975174 learning rate: 1.000000
  Minibatch perplexity: 47061.71
  predict: la lll
  labels: laze a
  predict: ms m a
  labels: ms abl
  Validation set perplexity: 2.79
  Average loss at step 9100: 1.703122 learning rate: 1.000000
  Minibatch perplexity: 17984.52
  predict: trrere
  labels: reroom
  predict:  d sa 
  labels: d stor
  Validation set perplexity: 11.04
  Average loss at step 9200: 1.212199 learning rate: 1.000000
  Minibatch perplexity: 227013.14
  predict: ef eee
  labels: e food
  predict: uttttt
  labels: ut the
  Validation set perplexity: 21.02
  Average loss at step 9300: 1.047828 learning rate: 1.000000
  Minibatch perplexity: 543627.74
  predict: nnggsg
  labels: ngs bu
  predict: puuuuu
  labels: uildin
  Validation set perplexity: 9.71
  Average loss at step 9400: 1.012418 learning rate: 1.000000
  Minibatch perplexity: 556335.32
  predict: tbtttt
  labels: the bu
  predict:  l    
  labels:  all t
  Validation set perplexity: 13.42
  Average loss at step 9500: 0.942932 learning rate: 1.000000
  Minibatch perplexity: 419340.67
  predict: dedede
  labels: d set 
  predict: rasrgn
  labels: rs had
  Validation set perplexity: 10.11
  Average loss at step 9600: 0.933639 learning rate: 1.000000
  Minibatch perplexity: 219522.66
  predict: fefene
  labels: fender
  predict: nddddd
  labels: nd def
  Validation set perplexity: 5.84
  Average loss at step 9700: 1.642559 learning rate: 1.000000
  Minibatch perplexity: 56488.06
  predict: ohohoh
  labels: housan
  predict: onoeon
  labels: one th
  Validation set perplexity: 8.09
  Average loss at step 9800: 1.556788 learning rate: 1.000000
  Minibatch perplexity: 46054.00
  predict: te e e
  labels: tely o
  predict: oixoxx
  labels: oximat
  Validation set perplexity: 120.47
  Average loss at step 9900: 1.356688 learning rate: 1.000000
  Minibatch perplexity: 70200.97
  predict:  papaa
  labels:  appro
  predict: t tttt
  labels: t its 
  Validation set perplexity: 8.79
  Average loss at step 10000: 0.925125 learning rate: 0.100000
  Minibatch perplexity: 402876.28
  predict: adt dt
  labels: d that
  predict: orevee
  labels: overed
  Validation set perplexity: 3.64
  Average loss at step 10100: 0.423009 learning rate: 0.100000
  Minibatch perplexity: 344015.43
  predict:  d did
  labels:  disco
  predict: omobmo
  labels: omans 
  Validation set perplexity: 4.09
  Average loss at step 10200: 0.336759 learning rate: 0.100000
  Minibatch perplexity: 155136.37
  predict: t ehte
  labels: the ro
  predict: eveeee
  labels: ever t
  Validation set perplexity: 6.67
  Average loss at step 10300: 0.293137 learning rate: 0.100000
  Minibatch perplexity: 308392.83
  predict: s s s 
  labels: s howe
  predict: rrrrrr
  labels: rtress
  Validation set perplexity: 6.28
  Average loss at step 10400: 0.270437 learning rate: 0.100000
  Minibatch perplexity: 118103.92
  predict: heh eh
  labels: he for
  predict: rerere
  labels: red th
  Validation set perplexity: 9.02
  Average loss at step 10500: 0.273692 learning rate: 0.100000
  Minibatch perplexity: 114672.69
  predict:  eeeen
  labels:  enter
  predict:  t t e
  labels:  they 
  Validation set perplexity: 7.14
  Average loss at step 10600: 0.245240 learning rate: 0.100000
  Minibatch perplexity: 132119.49
  predict:  w    
  labels:  when 
  predict: g g g 
  labels: g ram 
  Validation set perplexity: 3.27
  Average loss at step 10700: 0.237012 learning rate: 0.100000
  Minibatch perplexity: 51714.76
  predict: tetnet
  labels: tering
  predict: a a a 
  labels: a batt
  Validation set perplexity: 3.85
  Average loss at step 10800: 0.228932 learning rate: 0.100000
  Minibatch perplexity: 86197.70
  predict: iwwiwt
  labels: with a
  predict: rseres
  labels: ress w
  Validation set perplexity: 3.42
  Average loss at step 10900: 0.220107 learning rate: 0.100000
  Minibatch perplexity: 48903.26
  predict:  fof f
  labels:  fortr
  predict: f f tt
  labels: f the 
  Validation set perplexity: 3.75
  Average loss at step 11000: 0.211301 learning rate: 0.100000
  Minibatch perplexity: 103677.04
  predict: alalll
  labels: all of
  predict: thteht
  labels: the wa
  Validation set perplexity: 5.41
  Average loss at step 11100: 0.205478 learning rate: 0.100000
  Minibatch perplexity: 149294.99
  predict: eceeee
  labels: each t
  predict: l yl y
  labels: ly bre
  Validation set perplexity: 5.75
  Average loss at step 11200: 0.213179 learning rate: 0.100000
  Minibatch perplexity: 53860.03
  predict: fiffli
  labels: finall
  predict: s     
  labels: s to f
  Validation set perplexity: 4.17
  Average loss at step 11300: 0.191946 learning rate: 0.100000
  Minibatch perplexity: 39562.56
  predict: rororr
  labels: romans
  predict:  t e  
  labels:  the r
  Validation set perplexity: 3.26
  Average loss at step 11400: 0.205385 learning rate: 0.100000
  Minibatch perplexity: 51816.72
  predict: owoiwo
  labels: owing 
  predict: elal l
  labels: e allo
  Validation set perplexity: 3.70
  Average loss at step 11500: 0.209047 learning rate: 0.100000
  Minibatch perplexity: 46894.08
  predict:  s egi
  labels:  siege
  predict: h hs h
  labels: hs of 
  Validation set perplexity: 9.49
  Average loss at step 11600: 0.187286 learning rate: 0.100000
  Minibatch perplexity: 70984.37
  predict:  om no
  labels:  month
  predict: tetreh
  labels: three 
  Validation set perplexity: 2.44
  Average loss at step 11700: 0.180093 learning rate: 0.100000
  Minibatch perplexity: 48499.83
  predict: o ot o
  labels: o to t
  predict: l yl y
  labels: ly two
  Validation set perplexity: 13.32
  Average loss at step 11800: 0.187194 learning rate: 0.100000
  Minibatch perplexity: 138193.71
  predict: imimim
  labels: imatel
  predict: ppppop
  labels: pproxi
  Validation set perplexity: 4.74
  Average loss at step 11900: 0.177509 learning rate: 0.100000
  Minibatch perplexity: 122587.34
  predict: teteee
  labels: ter ap
  predict: ceceec
  labels: ce aft
  Validation set perplexity: 4.20
  Average loss at step 12000: 0.173485 learning rate: 0.100000
  Minibatch perplexity: 196719.82
  predict: hrh re
  labels: hree c
  predict: vev tn
  labels: ven th
  Validation set perplexity: 3.76
  Average loss at step 12100: 0.185950 learning rate: 0.100000
  Minibatch perplexity: 134855.42
  predict: ofofoc
  labels: of sev
  predict: riniri
  labels: ring o
  Validation set perplexity: 2.83
  Average loss at step 12200: 0.183214 learning rate: 0.100000
  Minibatch perplexity: 136271.37
  predict: h eh e
  labels: he spr
  predict:  i n n
  labels:  in th
  Validation set perplexity: 6.16
  Average loss at step 12300: 0.169516 learning rate: 0.100000
  Minibatch perplexity: 103303.14
  predict: pepeee
  labels: plete 
  predict: s sc c
  labels: s comp
  Validation set perplexity: 2.64
  Average loss at step 12400: 0.168127 learning rate: 0.100000
  Minibatch perplexity: 100412.11
  predict: p p p 
  labels: mp was
  predict: h eh e
  labels: he ram
  Validation set perplexity: 8.48
  Average loss at step 12500: 0.166721 learning rate: 0.100000
  Minibatch perplexity: 224505.17
  predict: totttt
  labels: top th
  predict:  t t t
  labels:  the t
  Validation set perplexity: 2.27
  Average loss at step 12600: 0.166655 learning rate: 0.100000
  Minibatch perplexity: 127976.26
  predict:  f f f
  labels:  from 
  predict:  r r r
  labels:  ramp 
  Validation set perplexity: 3.50
  Average loss at step 12700: 0.163207 learning rate: 0.100000
  Minibatch perplexity: 136740.41
  predict: f f tt
  labels: f the 
  predict: ohoh o
  labels: hot of
  Validation set perplexity: 7.03
  Average loss at step 12800: 0.163344 learning rate: 0.100000
  Minibatch perplexity: 68039.61
  predict: s s s 
  labels: s a sh
  predict: eleeee
  labels: eliefs
  Validation set perplexity: 5.75
  Average loss at step 12900: 0.162560 learning rate: 0.100000
  Minibatch perplexity: 331067.12
  predict: ei ere
  labels: eir be
  predict: ofottt
  labels: of the
  Validation set perplexity: 3.96
  Average loss at step 13000: 0.151547 learning rate: 0.100000
  Minibatch perplexity: 132662.11
  predict: auauaa
  labels: ause o
  predict: l lb l
  labels: l beca
  Validation set perplexity: 6.14
  Average loss at step 13100: 0.165984 learning rate: 0.100000
  Minibatch perplexity: 201951.16
  predict: o k k 
  labels: o kill
  predict: attatt
  labels: ant to
  Validation set perplexity: 4.48
  Average loss at step 13200: 0.153274 learning rate: 0.100000
  Minibatch perplexity: 106662.58
  predict: elelel
  labels: elucta
  predict: ereeee
  labels: ere re
  Validation set perplexity: 2.04
  Average loss at step 13300: 0.164154 learning rate: 0.100000
  Minibatch perplexity: 98081.55
  predict: owtsow
  labels: ots we
  predict:  z ez 
  labels:  zealo
  Validation set perplexity: 15.94
  Average loss at step 13400: 0.158043 learning rate: 0.100000
  Minibatch perplexity: 109648.76
  predict: m     
  labels: m the 
  predict: t wt t
  labels: t whom
  Validation set perplexity: 9.79
  Average loss at step 13500: 0.142095 learning rate: 0.100000
  Minibatch perplexity: 182970.07
  predict: apaaaa
  labels: ampart
  predict: tht eh
  labels: the ra
  Validation set perplexity: 5.65
  Average loss at step 13600: 0.158145 learning rate: 0.100000
  Minibatch perplexity: 146692.88
  predict: uuulul
  labels: uild t
  predict:  t    
  labels:  to bu
  Validation set perplexity: 3.94
  Average loss at step 13700: 0.150361 learning rate: 0.100000
  Minibatch perplexity: 233652.90
  predict: lalala
  labels: laves 
  predict: isis s
  labels: ish sl
  Validation set perplexity: 5.96
  Average loss at step 13800: 0.144835 learning rate: 0.100000
  Minibatch perplexity: 218740.02
  predict: j ded 
  labels: d jewi
  predict: s ssss
  labels: s used
  Validation set perplexity: 6.76
  Average loss at step 13900: 0.154947 learning rate: 0.100000
  Minibatch perplexity: 155840.89
  predict: roraro
  labels: romans
  predict:  t eht
  labels:  the r
  Validation set perplexity: 4.41
  Average loss at step 14000: 0.145834 learning rate: 0.100000
  Minibatch perplexity: 214715.87
  predict:  t t t
  labels:  that 
  predict:  m nm 
  labels:  many 
  Validation set perplexity: 1.88
  Average loss at step 14100: 0.142474 learning rate: 0.100000
  Minibatch perplexity: 244538.48
  predict: eded e
  labels: ed by 
  predict: eleeee
  labels: elieve
  Validation set perplexity: 3.79
  Average loss at step 14200: 0.140826 learning rate: 0.100000
  Minibatch perplexity: 303773.38
  predict:  i    
  labels:  is be
  predict: onono 
  labels: on it 
  Validation set perplexity: 3.73
  2017-07-07 12:43:36.735006: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 19223746 get requests, put_count=19223746 evicted_count=19000 eviction_rate=0.000988361 and unsatisfied allocation rate=0.000992106
  Average loss at step 14300: 0.139501 learning rate: 0.100000
  Minibatch perplexity: 112990.40
  predict:  lel e
  labels:  legio
  predict: romrrr
  labels: roman 
  Validation set perplexity: 3.16
  Average loss at step 14400: 0.142617 learning rate: 0.100000
  Minibatch perplexity: 304055.20
  predict:  t eht
  labels:  the r
  predict: fiffgi
  labels: fight 
  Validation set perplexity: 5.20
  Average loss at step 14500: 0.136086 learning rate: 0.100000
  Minibatch perplexity: 124246.91
  predict: s s s 
  labels: s to f
  predict: skssss
  labels: skills
  Validation set perplexity: 2.14
  Average loss at step 14600: 0.138371 learning rate: 0.100000
  Minibatch perplexity: 87163.52
  predict: t  t t
  labels: t or s
  predict: ipipip
  labels: ipment
  Validation set perplexity: 4.62
  Average loss at step 14700: 0.136888 learning rate: 0.100000
  Minibatch perplexity: 297141.73
  predict: e eeee
  labels: e equi
  predict: ededet
  labels: ed the
  Validation set perplexity: 4.64
  Average loss at step 14800: 0.130775 learning rate: 0.100000
  Minibatch perplexity: 54327.76
  predict:  lalca
  labels:  lacke
  predict: alalal
  labels: alots 
  Validation set perplexity: 4.14
  Average loss at step 14900: 0.128769 learning rate: 0.100000
  Minibatch perplexity: 203566.28
  predict: heh eh
  labels: he zea
  predict: apapap
  labels: aps th
  Validation set perplexity: 4.84
  Average loss at step 15000: 0.127802 learning rate: 0.010000
  Minibatch perplexity: 396273.77
  predict:  p er 
  labels:  perha
  predict:  t ttt
  labels:  that 
  Validation set perplexity: 2.31
  Average loss at step 15100: 0.127109 learning rate: 0.010000
  Minibatch perplexity: 241266.66
  predict: stsgig
  labels: sting 
  predict: sggggg
  labels: sugges
  Validation set perplexity: 2.98
  Average loss at step 15200: 0.125051 learning rate: 0.010000
  Minibatch perplexity: 211850.55
  predict: ssssss
  labels: sses s
  predict: orrrrr
  labels: ortres
  Validation set perplexity: 5.36
  Average loss at step 15300: 0.114426 learning rate: 0.010000
  Minibatch perplexity: 375556.31
  predict: isi ii
  labels: ish fo
  predict: t twt 
  labels: t jewi
  Validation set perplexity: 6.78
  Average loss at step 15400: 0.129944 learning rate: 0.010000
  Minibatch perplexity: 197368.70
  predict: gagaga
  labels: gainst
  predict: gegggg
  labels: ges ag
  Validation set perplexity: 2.17
  Average loss at step 15500: 0.124003 learning rate: 0.010000
  Minibatch perplexity: 210828.22
  predict: r sr r
  labels: r sieg
  predict:  o eht
  labels:  other
  Validation set perplexity: 4.28
  Average loss at step 15600: 0.123600 learning rate: 0.010000
  Minibatch perplexity: 203612.76
  predict: ts f f
  labels: ts of 
  predict: cccccc
  labels: ccount
  Validation set perplexity: 3.56
  Average loss at step 15700: 0.129389 learning rate: 0.010000
  Minibatch perplexity: 90983.68
  predict: hshsih
  labels: his ac
  predict: frofff
  labels: from h
  Validation set perplexity: 2.78
  Average loss at step 15800: 0.120857 learning rate: 0.010000
  Minibatch perplexity: 411337.05
  predict: eneeee
  labels: ence f
  predict: ififff
  labels: iffere
  Validation set perplexity: 6.11
  Average loss at step 15900: 0.125509 learning rate: 0.010000
  Minibatch perplexity: 216905.64
  predict: atatna
  labels: ant di
  predict: ninifn
  labels: nifica
  Validation set perplexity: 2.88
  Average loss at step 16000: 0.122200 learning rate: 0.010000
  Minibatch perplexity: 82155.51
  predict: a a si
  labels: a sign
  predict: csecse
  labels: cess a
  Validation set perplexity: 3.39
  Average loss at step 16100: 0.136019 learning rate: 0.010000
  Minibatch perplexity: 261898.18
  predict: s srcr
  labels: s proc
  predict: g gg g
  labels: g this
  Validation set perplexity: 2.92
  Average loss at step 16200: 0.125054 learning rate: 0.010000
  Minibatch perplexity: 151290.30
  predict: dudnud
  labels: during
  predict: geg eg
  labels: gers d
  Validation set perplexity: 6.12
  Average loss at step 16300: 0.146378 learning rate: 0.010000
  Minibatch perplexity: 202450.35
  predict: beeeee
  labels: besieg
  predict:  t e e
  labels:  the b
  Validation set perplexity: 3.90
  Average loss at step 16400: 0.118769 learning rate: 0.010000
  Minibatch perplexity: 237519.48
  predict: tttttt
  labels: ttack 
  predict: ntnnet
  labels: nterat
  Validation set perplexity: 2.94
  Average loss at step 16500: 0.121447 learning rate: 0.010000
  Minibatch perplexity: 169126.68
  predict: oooooo
  labels: o coun
  predict: ottott
  labels: ots to
  Validation set perplexity: 2.94
  Average loss at step 16600: 0.111232 learning rate: 0.010000
  Minibatch perplexity: 205700.40
  predict:  z ez 
  labels:  zealo
  predict: y y yg
  labels: y the 
  Validation set perplexity: 3.72
  Average loss at step 16700: 0.120599 learning rate: 0.010000
  Minibatch perplexity: 181041.06
  predict: ptptp 
  labels: pts by
  predict: attttt
  labels: attemp
  Validation set perplexity: 8.34
  Average loss at step 16800: 0.123406 learning rate: 0.010000
  Minibatch perplexity: 161962.89
  predict: aooaaa
  labels: ajor a
  predict: ananan
  labels: any ma
  Validation set perplexity: 3.99
  Average loss at step 16900: 0.116899 learning rate: 0.010000
  Minibatch perplexity: 336824.01
  predict: cododo
  labels: cord a
  predict: otot t
  labels: ot rec
  Validation set perplexity: 3.95
  Average loss at step 17000: 0.122841 learning rate: 0.010000
  Minibatch perplexity: 244906.62
  predict: oenone
  labels: oes no
  predict: hs h h
  labels: hus do
  Validation set perplexity: 8.77
  Average loss at step 17100: 0.116918 learning rate: 0.010000
  Minibatch perplexity: 108641.71
  predict: jojoje
  labels: joseph
  predict: artarr
  labels: arth j
  Validation set perplexity: 4.60
  Average loss at step 17200: 0.125359 learning rate: 0.010000
  Minibatch perplexity: 190460.52
  predict: teetee
  labels: ten ea
  predict: d dnad
  labels: d beat
  Validation set perplexity: 2.78
  Average loss at step 17300: 0.109194 learning rate: 0.010000
  Minibatch perplexity: 54369.90
  predict: esena 
  labels: es and
  predict:  s nn 
  labels:  stone
  Validation set perplexity: 3.54
  Average loss at step 17400: 0.112638 learning rate: 0.010000
  Minibatch perplexity: 136075.44
  predict: n n n 
  labels: ns of 
  predict: ofofof
  labels: of ton
  Validation set perplexity: 3.36
  Average loss at step 17500: 0.105328 learning rate: 0.010000
  Minibatch perplexity: 237017.72
  predict: adnadn
  labels: ands o
  predict: tohtso
  labels: thousa
  Validation set perplexity: 4.58
  Average loss at step 17600: 0.107816 learning rate: 0.010000
  Minibatch perplexity: 415256.69
  predict: sisigi
  labels: sing t
  predict: euauau
  labels: eau us
  Validation set perplexity: 4.77
  Average loss at step 17700: 0.113591 learning rate: 0.010000
  Minibatch perplexity: 180336.67
  predict:  p tlg
  labels:  plate
  predict: f f t 
  labels: f the 
  Validation set perplexity: 3.12
  Average loss at step 17800: 0.104741 learning rate: 0.010000
  Minibatch perplexity: 546655.93
  predict: acaaaa
  labels: ace of
  predict: ernnnn
  labels: ern fa
  Validation set perplexity: 3.21
  Average loss at step 17900: 0.115235 learning rate: 0.010000
  Minibatch perplexity: 297856.19
  predict:  wewew
  labels:  weste
  predict: t tttt
  labels: t the 
  Validation set perplexity: 3.76
  Average loss at step 18000: 0.104148 learning rate: 0.010000
  Minibatch perplexity: 164545.68
  predict: gagaga
  labels: gainst
  predict: araaaa
  labels: art ag
  Validation set perplexity: 2.28
  Average loss at step 18100: 0.111133 learning rate: 0.010000
  Minibatch perplexity: 334628.63
  predict:  ra r 
  labels:  rampa
  predict: h eh e
  labels: hen a 
  Validation set perplexity: 5.43
  Average loss at step 18200: 0.111161 learning rate: 0.010000
  Minibatch perplexity: 225645.54
  predict: adnadn
  labels: and th
  predict: walawa
  labels: wall a
  Validation set perplexity: 2.67
  Average loss at step 18300: 0.124267 learning rate: 0.010000
  Minibatch perplexity: 201227.71
  predict: titnit
  labels: tion w
  predict: valala
  labels: vallat
  Validation set perplexity: 3.28
  Average loss at step 18400: 0.115649 learning rate: 0.010000
  Minibatch perplexity: 232128.00
  predict: iririr
  labels: ircumv
  predict: t t ta
  labels: t a ci
  Validation set perplexity: 4.06
  Average loss at step 18500: 0.114234 learning rate: 0.010000
  Minibatch perplexity: 263128.57
  predict:  b b t
  labels:  built
  predict:  ht eh
  labels:  they 
  Validation set perplexity: 4.46
  Average loss at step 18600: 0.112457 learning rate: 0.010000
  Minibatch perplexity: 296301.42
  predict:  w l l
  labels:  wall 
  predict: h hh h
  labels: h the 
  Validation set perplexity: 8.53
  Average loss at step 18700: 0.127022 learning rate: 0.010000
  Minibatch perplexity: 54320.22
  predict: brbrbr
  labels: breach
  predict: s     
  labels: s to b
  Validation set perplexity: 3.90
  Average loss at step 18800: 0.112716 learning rate: 0.010000
  Minibatch perplexity: 141464.71
  predict: tetnet
  labels: tempts
  predict: edet d
  labels: ed att
  Validation set perplexity: 3.17
  Average loss at step 18900: 0.118360 learning rate: 0.010000
  Minibatch perplexity: 167172.93
  predict:  f f f
  labels:  faile
  predict: affafa
  labels: after 
  Validation set perplexity: 4.79
  Average loss at step 19000: 0.117238 learning rate: 0.010000
  Minibatch perplexity: 141953.40
  predict: resrer
  labels: ress a
  predict:  fof f
  labels:  fortr
  Validation set perplexity: 2.87
  Average loss at step 19100: 0.109881 learning rate: 0.010000
  Minibatch perplexity: 287362.88
  predict: o oht 
  labels: o the 
  predict: egeeee
  labels: ege to
  Validation set perplexity: 3.37
  Average loss at step 19200: 0.102001 learning rate: 0.010000
  Minibatch perplexity: 133528.22
  predict: ididid
  labels: id sie
  predict: nadnad
  labels: nd lai
  Validation set perplexity: 3.93
  Average loss at step 19300: 0.121172 learning rate: 0.010000
  Minibatch perplexity: 432391.10
  predict: sissss
  labels: sis an
  predict: reeree
  labels: retens
  Validation set perplexity: 4.31
  Average loss at step 19400: 0.121278 learning rate: 0.010000
  Minibatch perplexity: 155122.07
  predict: n n n 
  labels: n x fr
  predict: lelelg
  labels: legion
  Validation set perplexity: 6.82
  Average loss at step 19500: 0.124552 learning rate: 0.010000
  Minibatch perplexity: 211326.56
  predict: omamom
  labels: oman l
  predict: thteht
  labels: the ro
  Validation set perplexity: 5.63
  Average loss at step 19600: 0.115247 learning rate: 0.010000
  Minibatch perplexity: 158605.08
  predict: iwtiwt
  labels: with t
  predict: sadasa
  labels: sada w
  Validation set perplexity: 4.31
  Average loss at step 19700: 0.117220 learning rate: 0.010000
  Minibatch perplexity: 169299.46
  predict: stssss
  labels: st mas
  predict: agaaaa
  labels: agains
  Validation set perplexity: 5.62
  Average loss at step 19800: 0.117617 learning rate: 0.010000
  Minibatch perplexity: 159875.42
  predict: cecece
  labels: ched a
  predict: a aaaa
  labels: a marc
  Validation set perplexity: 3.18
  Average loss at step 19900: 0.108412 learning rate: 0.010000
  Minibatch perplexity: 201323.50
  predict:  s si 
  labels:  silva
  predict: avavav
  labels: avius 
  Validation set perplexity: 4.57
#+END_EXAMPLE

--------------
