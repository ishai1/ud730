#+TITLE: UD730 assignments
-*- eval: (auto-revert-mode 1); -*-
#+TODO: TODO IN-PROGRESS WAITING DONE
#+STARTUP: indent
#+OPTIONS: author:nil

* Deep Learning

** Assignment 1
:PROPERTIES:
:CUSTOM_ID: assignment-1
:header-args: :session a1py
:END:

The objective of this assignment is to learn about simple data curation practices, and familiarize you with some of the data we'll be reusing later.
This notebook uses the [[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST]] dataset to be used with python experiments. This dataset is designed to look like the classic [[http://yann.lecun.com/exdb/mnist/][MNIST]] dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST.

#+NAME: imps1
#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import matplotlib.pyplot as plt
  import numpy as np
  import os
  import sys
  import tarfile
  from IPython.display import display, Image
  from scipy import ndimage
  from sklearn.linear_model import LogisticRegression
  from six.moves.urllib.request import urlretrieve
  from six.moves import cPickle as pickle
#+END_SRC

#+RESULTS:

First, we'll download the dataset to our local machine. The dataconsists of characters rendered in a variety of fonts on a 28x28 image.
The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labelled examples. Given thesesizes, it should be possible to train models quickly on any machine.

#+BEGIN_SRC python
  url = 'http://commondatastorage.googleapis.com/books1000/'
  last_percent_reported = None

  def download_progress_hook(count, blockSize, totalSize):
    """A hook to report the progress of a download. This is mostly intended for users with
    slow internet connections. Reports every 1% change in download progress.
    """
    global last_percent_reported
    percent = int(count * blockSize * 100 / totalSize)

    if last_percent_reported != percent:
      if percent % 5 == 0:
        sys.stdout.write("%s%%" % percent)
        sys.stdout.flush()
      else:
        sys.stdout.write(".")
        sys.stdout.flush()

      last_percent_reported = percent

  def maybe_download(filename, expected_bytes, force=False):
    """Download a file if not present, and make sure it's the right size."""
    if force or not os.path.exists(filename):
      print('Attempting to download:', filename) 
      filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)
      print('\nDownload Complete!')
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified', filename)
    else:
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)
  test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)
#+END_SRC

#+RESULTS:


Extract the dataset from the compressed .tar.gz file. This should give you a set of directories, labelled A through J.

#+BEGIN_SRC python
  num_classes = 10
  np.random.seed(133)

  def maybe_extract(filename, force=False):
    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz
    if os.path.isdir(root) and not force:
      # You may override by setting force=True.
      print('%s already present - Skipping extraction of %s.' % (root, filename))
    else:
      print('Extracting data for %s. This may take a while. Please wait.' % root)
      tar = tarfile.open(filename)
      sys.stdout.flush()
      tar.extractall(data_root)
      tar.close()
    data_folders = [
      os.path.join(root, d) for d in sorted(os.listdir(root))
      if os.path.isdir(os.path.join(root, d))]
    if len(data_folders) != num_classes:
      raise Exception(
        'Expected %d folders, one per class. Found %d instead.' % (
          num_classes, len(data_folders)))
    print(data_folders)
    return data_folders
  
  train_folders = maybe_extract(train_filename)
  test_folders = maybe_extract(test_filename)
#+END_SRC

#+RESULTS:

#+NAME: imps2
#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import matplotlib.pyplot as plt
  import numpy as np
  import os
  import sys
  import tarfile
  from IPython.display import display, Image
  from scipy import ndimage
  from sklearn.linear_model import LogisticRegression
  from six.moves.urllib.request import urlretrieve
  from six.moves import cPickle as pickle

  # Config the matplotlib backend as plotting inline in IPython
#+END_SRC

#+RESULTS:

*** Problem 1
:PROPERTIES:
    :CUSTOM_ID: problem-1
    :END:

 Let's take a peek at some of the data to make sure it looks sensible.
 Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display.

[[file:notMNIST_large/A/a29ydW5pc2hpLnR0Zg==.png]]

 Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size.
 We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road.
 A few images might not be readable, we'll just skip them.

#+NAME: shapes
#+BEGIN_SRC python
   image_size = 28  # Pixel width and height.
   pixel_depth = 255.0  # Number of levels per pixel.
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  def load_letter(folder, min_num_images, standardize=True, dtype = float):
    """Load the data for a single letter label."""
    image_files = os.listdir(folder)
    dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=dtype)
    print(folder)
    num_images = 0
    for image in image_files:
      image_file = os.path.join(folder, image)
      try:
        if standardize:
          image_data = (ndimage.imread(image_file).astype(dtype) - 
                        pixel_depth / 2) / pixel_depth
        else:
          image_data = ndimage.imread(image_file).astype(dtype)
        if image_data.shape != (image_size, image_size):
          raise Exception('Unexpected image shape: %s' % str(image_data.shape))
        dataset[num_images, :, :] = image_data
        num_images = num_images + 1
      except IOError as e:
        print('Could not read:', image_file, ':', e, '- it\'s ok, skipping.')
        pass
    dataset = dataset[0:num_images, :, :]
    if num_images < min_num_images:
      raise Exception('Many fewer images than expected: %d < %d' %
                      (num_images, min_num_images))
    print('Full dataset tensor:', dataset.shape)
    print('Mean:', np.mean(dataset))
    print('Standard deviation:', np.std(dataset))
    return dataset

  def maybe_pickle(data_folders, min_num_images_per_class, force=False, instance_name='', standardize = True, dtype = np.float32):
    dataset_names = []
    for folder in data_folders:
      set_filename = folder + instance_name + '.pickle'
      dataset_names.append(set_filename)
      if os.path.exists(set_filename) and not force:
        # You may override by setting force=True.
        print('%s already present - Skipping pickling.' % set_filename)
      else:
        print('Pickling %s.' % set_filename)
        dataset = load_letter(folder, min_num_images_per_class, standardize, dtype)
        try:
          with open(set_filename, 'wb') as f:
            pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)
        except Exception as e:
          print('Unable to save data to', set_filename, ':', e)
    return dataset_names
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  # train_datasets = maybe_pickle(train_folders, 45000)
  # test_datasets = maybe_pickle(test_folders, 1800)
#+END_SRC

#+RESULTS:
: None


#+BEGIN_SRC python
  # train_datasets = ["./notMNIST_large/"+nm for nm in sorted(filter(lambda n: n[-6:] == "pickle",os.listdir("./notMNIST_large")))]
  # test_datasets = ["./notMNIST_small/"+nm for nm in sorted(filter(lambda n: n[-6:] == "pickle",os.listdir("./notMNIST_small")))]
#+END_SRC

 #+RESULTS:

To avoid standardizing and use uint8 encoding:
#+BEGIN_SRC python
  train_datasets = maybe_pickle(train_folders, 45000, instance_name='int8', standardize=False,
                                dtype=np.uint8)
  test_datasets = maybe_pickle(test_folders, 1800, instance_name='int8', standardize=False,
                               dtype=np.uint8)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  train_datasets = ["./notMNIST_large/"+nm for nm in sorted(filter(lambda n: n[-10:] == "int8.pickle",os.listdir("./notMNIST_large")))]
  test_datasets = ["./notMNIST_small/"+nm for nm in sorted(filter(lambda n: n[-10:] == "int8.pickle",os.listdir("./notMNIST_small")))]
#+END_SRC

 #+RESULTS:


*** Problem 2
:PROPERTIES:
:CUSTOM_ID: problem-2
:END:
#+BEGIN_SRC python
  try:
      os.stat('imgs')
  except:
      os.mkdir('imgs')       
#+END_SRC

#+RESULTS:
: os.stat_result(st_mode=16893, st_ino=5324802, st_dev=47, st_nlink=2, st_uid=1000, st_gid=1000, st_size=4096, st_atime=1496539788, st_mtime=1496539655, st_ctime=1496539655)

#+BEGIN_SRC python :results file
  letter_ix = np.random.randint(len(train_datasets))
  print(letter_ix)
  pickle_file = train_datasets[letter_ix]
  with open(pickle_file, 'rb') as f:
      letter_set = pickle.load(f)  # unpickle
      sample_idx = np.random.randint(len(letter_set))  # pick a random image index
      sample_image = letter_set[sample_idx, :, :]  # extract a 2D slice
      plt.figure()
      plt.imshow(sample_image)  # display it
      plt.savefig('imgs/sample.png')

  'imgs/sample.png'
#+END_SRC

#+RESULTS:
[[file:imgs/sample.png]]


*** Problem 3
    :PROPERTIES:
    :CUSTOM_ID: problem-3
    :END:

 Another check: we expect the data to be balanced across classes. Verify
 that.

 #+BEGIN_SRC python
   for nm in (filter(lambda n: n[-6:] == "pickle",os.listdir("./notMNIST_large"))):
       f = open("./notMNIST_large/"+nm, 'rb')
       letter_set = pickle.load(f)
       print(letter_set.shape[0])
 #+END_SRC

 #+RESULTS:

 Merge and prune the training data as needed. Depending on your computer
 setup, you might not be able to fit it all in memory, and you can tune
 =train_size= as needed. The labels will be stored into a separate array
 of integers 0 through 9.

 Also create a validation dataset for hyperparameter tuning.

 #+BEGIN_SRC python
   def make_arrays(nb_rows, img_size):
     if nb_rows:
       dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)
       labels = np.ndarray(nb_rows, dtype=np.int32)
     else:
       dataset, labels = None, None
     return dataset, labels

   def merge_datasets(pickle_files, train_size, valid_size=0):
     num_classes = len(pickle_files)
     valid_dataset, valid_labels = make_arrays(valid_size, image_size)
     train_dataset, train_labels = make_arrays(train_size, image_size)
     vsize_per_class = valid_size // num_classes
     tsize_per_class = train_size // num_classes
     start_v, start_t = 0, 0
     end_v, end_t = vsize_per_class, tsize_per_class
     end_l = vsize_per_class+tsize_per_class
     for label, pickle_file in enumerate(pickle_files):       
       try:
         with open(pickle_file, 'rb') as f:
           letter_set = pickle.load(f)
           # let's shuffle the letters to have random validation and training set
           np.random.shuffle(letter_set)
           if valid_dataset is not None:
             valid_letter = letter_set[:vsize_per_class, :, :]
             valid_dataset[start_v:end_v, :, :] = valid_letter
             valid_labels[start_v:end_v] = label
             start_v += vsize_per_class
             end_v += vsize_per_class
           train_letter = letter_set[vsize_per_class:end_l, :, :]
           train_dataset[start_t:end_t, :, :] = train_letter
           train_labels[start_t:end_t] = label
           start_t += tsize_per_class
           end_t += tsize_per_class
       except Exception as e:
         print('Unable to process data from', pickle_file, ':', e)
         raise
     return valid_dataset, valid_labels, train_dataset, train_labels
#+END_SRC

#+BEGIN_SRC python
   train_size = 200000
   valid_size = 10000
   test_size = 10000
   valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(train_datasets,
                                                                             train_size, 
                                                                             valid_size)
#+END_SRC

#+BEGIN_SRC python
   _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
   print('Training:', train_dataset.shape, train_labels.shape)
   print('Validation:', valid_dataset.shape, valid_labels.shape)
   print('Testing:', test_dataset.shape, test_labels.shape)
 #+END_SRC

 #+RESULTS:
 : None

 Next, we'll randomize the data. It's important to have the labels well
 shuffled for the training and test distributions to match.

 #+BEGIN_SRC python
   def randomize(dataset, labels):
     permutation = np.random.permutation(labels.shape[0])
     shuffled_dataset = dataset[permutation,:,:]
     shuffled_labels = labels[permutation]
     return shuffled_dataset, shuffled_labels
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
   train_dataset, train_labels = randomize(train_dataset, train_labels)
   test_dataset, test_labels = randomize(test_dataset, test_labels)
   valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)
 #+END_SRC

 #+RESULTS:
 : None


*** Problem 4
:PROPERTIES:
:CUSTOM_ID: problem-4
:END:

Convince yourself that the data is still good after shuffling!

#+BEGIN_SRC python :results output
  sample_idx = np.random.randint(len(train_dataset))  # pick a random image index
  sample_image = train_dataset[sample_idx, :, :]  # extract a 2D slice
  plt.figure()
  plt.imshow(sample_image)  # display it
  plt.savefig('./imgs/sample2.png')

  train_labels[sample_idx]
#+END_SRC

#+RESULTS:
: 
: >>> <matplotlib.figure.Figure object at 0x7f585fd7ef98>
: <matplotlib.image.AxesImage object at 0x7f585fcf2198>
: 3

[[./imgs/sample2.png]]     

 Finally, let's save the data for later reuse:

#+BEGIN_SRC python
  pickle_file = 'notMNIST.pickle'

  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_dataset': train_dataset,
      'train_labels': train_labels,
      'valid_dataset': valid_dataset,
      'valid_labels': valid_labels,
      'test_dataset': test_dataset,
      'test_labels': test_labels,
      }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  statinfo = os.stat(pickle_file)
  print('Compressed pickle size:', statinfo.st_size)
#+END_SRC

 #+RESULTS:
 : None

#+NAME: load_int8_pkl
#+BEGIN_SRC python 
  pickle_file = 'notMNISTint8.pickle'
  f = open(pickle_file, 'rb')
  pkl = pickle.load(f)
  test_labels = pkl["test_labels"]
  valid_labels = pkl["valid_labels"]
  valid_dataset = pkl["valid_dataset"]
  train_labels = pkl["train_labels"]
  test_dataset = pkl["test_dataset"]
  train_dataset = pkl["train_dataset"]
  f.close()
  image_size = 28  # Pixel width and height.
  pixel_depth = 255.0  # Number of levels per pixel.
#+END_SRC

 #+RESULTS:



*** Problem 5
:PROPERTIES:
:CUSTOM_ID: problem-5
:END:

By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it. Measure how much overlap there is between training, validation and test samples.

Optional questions:

- What about near duplicates between datasets? (images that are almost identical)

- Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments.

#+NAME: radius
#+BEGIN_SRC python
  radius = 2**4
#+END_SRC

#+RESULTS: radius

***** broadcast + expand l2

****** get_edges

******* numpy

#+BEGIN_SRC python :var pre1=imps1 pre2=imps2 pre3=load_int8_pkl pre4=shapes pre5=radius
  from math import ceil
#+END_SRC

#+RESULTS:

test
#+BEGIN_SRC python
A = test_dataset.reshape([test_dataset.shape[0], -1])
r = (A*A).sum(axis=1)
r = r.reshape([-1,1])
D = r-2*np.matmul(A,A.T)+r.T
E = np.where(D<radius**2)
E = (np.vstack(E).T)[[E[0]<E[1]]]
#+END_SRC

#+RESULTS:
: 256

#+BEGIN_SRC python
  ix=np.random.randint(E.shape[0])
  np.sum(np.square(test_dataset[E[ix,1],:,:]-test_dataset[E[ix,0],:,:]))
#+END_SRC

#+RESULTS:
: 0.0

#+BEGIN_SRC python
  def get_edges(data):
    N = data.shape[0]
    data = data.reshape([N,-1])
    T = 2**15 # slice length
    def slice_edges(ix1,ix2):
      A=data[ix1*T:(ix1+1)*T,:]
      B=data[ix2*T:(ix2+1)*T,:]
      r_A = (A*A).sum(axis=1).reshape([-1,1])
      r_B = (B*B).sum(axis=1).reshape([-1,1])
      D = r_A-2*np.matmul(A,B.T)+r_B.T
      E = np.where(D<radius**2)
      return (np.vstack(E).T)[E[0]+ix1*T<E[1]+ix2*T,:]+np.array([[ix1,ix2]])*T
    E_all = np.empty(shape=(0,2), dtype=np.int32)
    for i in range(ceil(N/T)):
      for j in range(i,ceil(N/T)):
        E_new = slice_edges(i,j)
        E_all = np.vstack([E_all, E_new])
        print("finished iteration i:{}, j:{}. Found {} edges.".format(i,j,len(E_new)))
    return E_all
#+END_SRC


******* IN-PROGRESS tflow

#+BEGIN_SRC python :var pre1=imps1 pre2=imps2 pre3=load_int8_pkl pre4=shapes pre5=radius
  import tensorflow as tf
  from math import ceil
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  import pdb
  tf.reset_default_graph()  
  def get_edges(dataset):
    g = tf.Graph()
    N = dataset.shape[0]
    T = 2**16 # slice length
    with g.as_default():`
      data = tf.constant(dataset.reshape([N, -1]), dtype=tf.int32)
      slice_ix1 = tf.placeholder(dtype = tf.int32, shape=())
      slice_ix2 = tf.placeholder(dtype = tf.int32, shape=())
      A=data[slice_ix1*T:(slice_ix1+1)*T,:]
      B=data[slice_ix2*T:(slice_ix2+1)*T,:]
      r_A = tf.reduce_sum(A*A, 1)
      r_B = tf.reduce_sum(B*B, 1)
      # turn r into column vector
      r_A = tf.reshape(r_A, [-1, 1])
      r_B = tf.reshape(r_B, [-1, 1])
      D = r_A - 2*tf.matmul(A, tf.transpose(B)) + tf.transpose(r_B)
      E = tf.where(tf.less_equal(D,radius**2))
    sess = tf.Session(graph=g)
    all_edges = np.empty(shape=(0,2))
    for i in range(ceil(N/T)):
      for j in range(i,ceil(N/T)):
        edges = sess.run(E, feed_dict = {slice_ix1:i, slice_ix2:j})
        pdb.set_trace()
        all_edges = np.vstack([all_edges,
                               edges[edges[:,0]+i*T<edges[:,1]+j*T,:]+np.array([[i,j]])*T])
        print("finished iteration i:{}, j:{}. Found {} edges.".format(i,j,len(edges)))
    return all_edges
#+END_SRC

#+RESULTS:
: 1.06335e+07


****** post process

#+BEGIN_SRC python
  train_edges = get_edges(train_dataset)
  test_edges = get_edges(test_dataset)
  valid_edges = get_edges(valid_dataset)
#+END_SRC

#+RESULTS:

test
#+BEGIN_SRC python
  E = np.int32(train_edges)
  data = train_dataset
  ix=np.random.randint(E.shape[0])
  np.sum(np.square(data[E[ix,1],:,:]-data[E[ix,0],:,:]))
#+END_SRC

#+RESULTS:
: 0.0


#+BEGIN_SRC python
  pickle_file = 'edges_r_2p4.pickle'
  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_edges':train_edges,
      'test_edges':test_edges,
      'valid_edges':valid_edges
    }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise  
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  pickle_file = 'edges_r_2p4.pickle'
  f = open(pickle_file, 'rb')
  pkl = pickle.load(f)
  train_edges = pkl["train_edges"]
  test_edges = pkl["test_edges"]
  valid_edges = pkl["valid_edges"]
  f.close()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  from scipy.sparse import csc_matrix
  train_A = csc_matrix((np.ones(len(train_edges)),
                        (train_edges[:,0], train_edges[:,1])), 
                       shape=(len(train_dataset),len(train_dataset)))
  test_A = csc_matrix((np.ones(len(test_edges)),
                       (test_edges[:,0], test_edges[:,1])), 
                      shape=(len(test_dataset),len(test_dataset)))
  valid_A = csc_matrix((np.ones(len(valid_edges)),
                        (valid_edges[:,0], valid_edges[:,1])), 
                       shape=(len(valid_dataset),len(valid_dataset)))
#+END_SRC

#+RESULTS:

test
#+BEGIN_SRC python
A = train_A
data = train_dataset
coo_A = A.tocoo()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
ix = np.random.randint(A.nnz)
np.sum(np.square(data[coo_A.row[ix]]-data[coo_A.col[ix]]))
#+END_SRC

#+RESULTS:
: 0.0


#+BEGIN_SRC python
  from scipy.sparse.csgraph import connected_components
  import pandas as pd 

  def get_groups(A):
    n_comp, index_labels = connected_components(A, directed=False, return_labels=True)
    comp_labels, comp_first, comp_counts = np.unique(index_labels, return_index=True, 
                                                     return_inverse=False, 
                                                     return_counts=True)
    comp_labels = comp_labels[comp_counts>1]  # non-trivial components
    index_labels = np.vstack([np.arange(len(index_labels)), index_labels]).T
    # filter out trivial:
    index_labels = index_labels[np.in1d(index_labels[:,1], comp_labels),:]
    return pd.Series(index_labels[:,0]).groupby(index_labels[:,1]), comp_first
#+END_SRC

#+RESULTS:

tests
#+BEGIN_SRC python
  group_obj, group_firsts = get_groups(train_A)
  data = train_dataset
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  group_counts = group_obj.count()
  gkey = np.random.choice(group_counts.index, p=group_counts.values/sum(group_counts.values))
  print(gkey)
  ixs = np.random.choice(group_obj.get_group(gkey),2,replace=False)
  np.sum(np.square(data[ixs[0]]-data[ixs[1]]))
#+END_SRC

#+RESULTS:
: 0.0

#+name: plt-save
#+begin_src python :exports results :results verbatim
files = []
path='imgs/compare'
for i in range(len(ixs)):
    plt.figure(i)
    plt.imshow(data[ixs[i]])
    files.append('{0}_{1}.png'.format(path, i))
    plt.savefig(files[-1], bbox_inches='tight')

"\n".join(["[[file:{0}]]".format(f) for f in files])
#+end_src

#+RESULTS: plt-save
[[file:imgs/compare_0.png]]
[[file:imgs/compare_1.png]]

#+BEGIN_SRC python
  pickle_file = 'groups_firsts_r_2p4.pickle'
  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_groups_firsts':get_groups(train_A),
      'test_groups_firsts':get_groups(test_A),
      'valid_groups_firsts':get_groups(valid_A)
    }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise  
#+END_SRC

#+RESULTS:

load
#+BEGIN_SRC python
  from six.moves import cPickle as pickle
  pickle_file = 'groups_firsts_r_2p4.pickle'
  with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_groups_firsts = save['train_groups_firsts']
    test_groups_firsts = save['test_groups_firsts']
    valid_groups_firsts = save['valid_groups_firsts']
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :results file
  import numpy as np
  import matplotlib.pyplot as plt
  import os
  imgfile = 'imgs/copyhists.png'
  try:
    os.remove(filename)
  except OSError:
    pass  
  plt.figure(1)
  for i, grp in enumerate(zip(['train', 'test', 'valid'],
                              [train_groups_firsts, test_groups_firsts, valid_groups_firsts])):
    plt.subplot(3, 1, i+1)
    plt.hist(grp[1][0].count(), 50, range = (1, 10), log = True)

  plt.savefig(imgfile, bbox_inches='tight')
  
  imgfile
#+END_SRC

#+RESULTS:
[[file:imgs/copyhists.png]]
Lots of small groups of similar images at radius 2^4

#+BEGIN_SRC python :results output
  for a, b in zip(['train', 'test', 'valid'], [train_groups_firsts, test_groups_firsts, valid_groups_firsts]):
    print(a+': {}'.format(np.sort((b[0].count()))[-10:]))

#+END_SRC

#+RESULTS:
: 
: ... train: [  16   17   22   24   25   29   33   46   70 2085]
: test: [  2   2   2   2   2   2   2   2   2 142]
: valid: [  2   2   2   2   2   2   2   2   3 110]

1 very large group


***** IN-PROGRESS digitize and cluster

#+BEGIN_SRC python
train_dataset = train_dataset.reshape([train_dataset.shape[0],-1])
train_dist = sklearn.metrics.pairwise.pairwise_distances(train_dataset, train_dataset, n_jobs = 8)
valid_dataset = valid_dataset.reshape([valid_dataset.shape[0],-1])
test_dataset = test_dataset.reshape([test_dataset.shape[0],-1])
#+END_SRC

#+RESULTS:
| 10 | 28 | 28 |

_warning_: consumes lots of memory and should probably be done in sql.
#+BEGIN_SRC python
  from math import ceil
  import pandas as pd  # for groupby
  bins = np.arange(ceil((pixel_depth+1)/radius))*radius
  train_bins = pd.RangeIndex(train_dataset.shape[0]).groupby(
    pd.Series(map(tuple, np.digitize(train_dataset.reshape([train_dataset.shape[0],-1]), 
                                     bins, right=False)-1)))
  test_bins = pd.RangeIndex(test_dataset.shape[0]).groupby(
    pd.Series(map(tuple, np.digitize(test_dataset.reshape([test_dataset.shape[0],-1]), 
                                     bins, right=False)-1)))
  valid_bins = pd.RangeIndex(valid_dataset.shape[0]).groupby(
    pd.Series(map(tuple, np.digitize(valid_dataset.reshape([valid_dataset.shape[0],-1]), 
                                     bins, right=False)-1)))
#+END_SRC

#+RESULTS:

Based on Fixed-Radius Near Neighbor on the Line by Bucketing, for example as described [[www.cs.wustl.edu/~pless/546/lectures/Lecture2.pdf][here]].
#+BEGIN_SRC python
  from scipy.sparse import csc_matrix
  import pdb
  def get_adjmx(dataset, bins):
    A = csc_matrix((dataset.shape[0], dataset.shape[0]), dtype=bool)
    keys = bins.keys()
    def find_neighbors(bin_orig, vec_length, delta):
      if len(delta) == vec_length and np.sum(np.abs(delta)) != 0:      
        if tuple(bin_orig+delta) in keys:
          bin_new = bins[bin_orig+delta]
          for e_0 in bin_orig:
            for e_1 in bin_new:
              A[min(e_0,e_1),max(e_0,e_1)] = np.sum(np.abs(dataset[e_0,:]-dataset[e_1,:]))<=radius
      elif len(delta) < vec_length:
        for d in [0,-1,1]:
          find_neighbors(bin_orig, vec_length, np.concatenate([delta,[d]]))
    i = 0      
    for b in keys:
      if i % 1 == 0:
        print("on key #"+str(i))
      find_neighbors(b, len(b), [])
      i+=1
    return A

  A_train = get_adjmx(train_dataset, train_bins)
  A_valid = get_adjmx(valid_dataset, valid_bins)
  A_test = get_adjmx(test_dataset, test_bins)
#+END_SRC

... This takes too long. Probably has a bug.


#+BEGIN_SRC julia :session a1jl
using PyCall
@pyimport pickle
pickle_file = "notMNISTint8.pickle"
fid = open(pickle_file,"r")
data = pickle.load(fid)
close(fid)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC julia
convert(Array{UInt8,3},data["test_dataset"])
#+END_SRC


*** Problem 6
:PROPERTIES:
:CUSTOM_ID: problem-6
:END:

Let's get an idea of what an off-the-shelf classifier can give you on
this data. It's always good to check that there is something to learn,
and that it's a problem that is not so trivial that a canned solution
solves it.

Train a simple model on this data using 50, 100, 1000 and 5000 training
samples. Hint: you can use the LogisticRegression model from
sklearn.linear\_model.

Optional question: train an off-the-shelf model on all the data!

#+BEGIN_SRC python
    import pickle
    pickle_file = 'eql_lsts.pickle'
    eql_lsts = np.load(pickle_file)
    apx_eql_lst = eql_lsts["apx_lst"]
#+END_SRC

#+BEGIN_SRC python
    pkl = np.load('notMNIST.pickle',mmap_mode='r')
    test_labels = pkl["test_labels"]
    valid_labels = pkl["valid_labels"]
    valid_dataset = pkl["valid_dataset"]
    train_labels = pkl["train_labels"]
    test_dataset = pkl["test_dataset"]
    train_dataset = pkl["train_dataset"]
#+END_SRC

#+BEGIN_SRC python
    import itertools
    import random
    import sklearn.linear_model
    bad_train_ix = map(lambda x: x[0], apx_eql_lst)
    good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                range(train_dataset.shape[0])))
#+END_SRC

#+BEGIN_SRC python
    def reservoir_sampling(iterable, r=1):
        "Random selection from itertools.permutations(iterable, r)"
        it = iter(iterable)
        R = [next(it) for i in range(r)]
        for i, item in enumerate(it, start=r+1):
          j = random.randrange(i)
          if j<r:
            R[j] = item
        return R
#+END_SRC

#+BEGIN_SRC python
    sample_size = 20000
    sample_train_ix = reservoir_sampling(good_train_ix, sample_size)
    logreg = sklearn.linear_model.LogisticRegression()
#+END_SRC

#+BEGIN_SRC python
    m = train_dataset.shape[1]*train_dataset.shape[2]
    X = train_dataset[sample_train_ix].reshape(sample_size,m)
    y = train_labels[sample_train_ix]
    M = logreg.fit(X,y)
#+END_SRC

#+BEGIN_SRC python
    X_hat = test_dataset.reshape(test_dataset.shape[0],m)
    y_hat = test_labels
    L = M.score(X_hat, y_hat)
    print(L)
#+END_SRC

#+BEGIN_EXAMPLE
    0.7081
#+END_EXAMPLE


** Assignment 2
:PROPERTIES:
:CUSTOM_ID: assignment-2
:header-args: :session a2py
:END:

Previously in =1_notmnist.ipynb=, we created a pickle with formatted
datasets for training, development and testing on the
[[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST
dataset]].

The goal of this assignment is to progressively train deeper and more
accurate models using TensorFlow.

#+BEGIN_SRC python
     # These are all the modules we'll be using later. Make sure you can import them
     # before proceeding further.
     from __future__ import print_function
     import numpy as np
     import tensorflow as tf
     from six.moves import cPickle as pickle
     from six.moves import range
#+END_SRC

#+RESULTS:

First reload the data we generated in =1_notmnist.ipynb=.

#+BEGIN_SRC python
     pickle_file = 'notMNIST.pickle'

     with open(pickle_file, 'rb') as f:
       save = pickle.load(f)
       train_dataset = save['train_dataset']
       train_labels = save['train_labels']
       valid_dataset = save['valid_dataset']
       valid_labels = save['valid_labels']
       test_dataset = save['test_dataset']
       test_labels = save['test_labels']
       del save  # hint to help gc free up memory
       print('Training set', train_dataset.shape, train_labels.shape)
       print('Validation set', valid_dataset.shape, valid_labels.shape)
       print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+BEGIN_EXAMPLE
  Training set (200000, 28, 28) (200000,)
  Validation set (10000, 28, 28) (10000,)
  Test set (10000, 28, 28) (10000,)
#+END_EXAMPLE

Reformat into a shape that's more adapted to the models we're going to
train:

-  data as a flat matrix,
-  labels as float 1-hot encodings.

#+BEGIN_SRC python
  image_size = 28
  num_labels = 10

  def reformat(dataset, labels):
    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels
  train_dataset, train_labels = reformat(train_dataset, train_labels)
  valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
  test_dataset, test_labels = reformat(test_dataset, test_labels)
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+BEGIN_EXAMPLE
     Training set (200000, 784) (200000, 10)
     Validation set (10000, 784) (10000, 10)
     Test set (10000, 784) (10000, 10)
#+END_EXAMPLE

We're first going to train a multinomial logistic regression using
simple gradient descent.

TensorFlow works like this:

-  First you describe the computation that you want to see performed:
what the inputs, the variables, and the operations look like. These
get created as nodes over a computation graph. This description is
all contained within the block below:

with graph.as\_default():\\
...

-  Then you can run the operations on this graph as many times as you
want by calling =session.run()=, providing it outputs to fetch from
the graph that get returned. This runtime operation is all contained
in the block below:

with tf.Session(graph=graph) as session:\\
...

Let's load all the data into TensorFlow and build the computation graph
corresponding to our training:

#+BEGIN_SRC python
     # With gradient descent training, even this much data is prohibitive.
     # Subset the training data for faster turnaround.
     train_subset = 10000

     graph = tf.Graph()
     with graph.as_default():

       # Input data.
       # Load the training, validation and test data into constants that are
       # attached to the graph.
       tf_train_dataset = tf.constant(train_dataset[:train_subset, :])
       tf_train_labels = tf.constant(train_labels[:train_subset])
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       # These are the parameters that we are going to be training. The weight
       # matrix will be initialized using random values following a (truncated)
       # normal distribution. The biases get initialized to zero.
       weights = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_labels]))
       biases = tf.Variable(tf.zeros([num_labels]))
      
       # Training computation.
       # We multiply the inputs with the weight matrix, and add biases. We compute
       # the softmax and cross-entropy (it's one operation in TensorFlow, because
       # it's very common, and it can be optimized). We take the average of this
       # cross-entropy across all training examples: that's our loss.
       logits = tf.matmul(tf_train_dataset, weights) + biases
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
      
       # Optimizer.
       # We are going to find the minimum of this loss using gradient descent.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       # These are not part of training, but merely here so that we can report
       # accuracy figures as we train.
       train_prediction = tf.nn.softmax(logits)
       valid_prediction = tf.nn.softmax(
         tf.matmul(tf_valid_dataset, weights) + biases)
       test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)
#+END_SRC

Let's run this computation and iterate:

#+BEGIN_SRC python
     num_steps = 801
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
#+END_SRC

#+BEGIN_SRC python
  with tf.Session(graph=graph) as session:
    # This is a one-time operation which ensures the parameters get initialized as
    # we described in the graph: random weights for the matrix, zeros for the
    # biases. 
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(num_steps):
      # Run the computations. We tell .run() that we want to run the optimizer,
      # and get the loss value and the training predictions returned as numpy
      # arrays.
      _, l, predictions = session.run([optimizer, loss, train_prediction])
      if (step % 100 == 0):
        print('Loss at step %d: %f' % (step, l))
        print('Training accuracy: %.1f%%' % accuracy(
          predictions, train_labels[:train_subset, :]))
        # Calling .eval() on valid_prediction is basically like calling run(), but
        # just to get that one numpy array. Note that it recomputes all its graph
        # dependencies.
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+BEGIN_EXAMPLE
  Initialized
  Loss at step 0: 22.018156
  Training accuracy: 6.6%
  Validation accuracy: 8.6%
  Loss at step 100: 2.022280
  Training accuracy: 75.2%
  Validation accuracy: 73.7%
  Loss at step 200: 1.623059
  Training accuracy: 78.1%
  Validation accuracy: 75.7%
  Loss at step 300: 1.408173
  Training accuracy: 79.2%
  Validation accuracy: 76.5%
  Loss at step 400: 1.262911
  Training accuracy: 80.0%
  Validation accuracy: 76.6%
  Loss at step 500: 1.154954
  Training accuracy: 80.7%
  Validation accuracy: 76.8%
  Loss at step 600: 1.070023
  Training accuracy: 81.2%
  Validation accuracy: 77.0%
  Loss at step 700: 1.000842
  Training accuracy: 81.7%
  Validation accuracy: 77.1%
  Loss at step 800: 0.943042
  Training accuracy: 82.2%
  Validation accuracy: 77.2%
  Test accuracy: 67.9%
#+END_EXAMPLE

Let's now switch to stochastic gradient descent training instead, which
is much faster.

The graph will be similar, except that instead of holding all the
training data into a constant node, we create a =Placeholder= node which
will be fed actual data at every call of =session.run()=.

#+BEGIN_SRC python
  batch_size = 128
  graph = tf.Graph()
  with graph.as_default():

    # Input data. For the training data, we use a placeholder that will be fed
    # at run time with a training minibatch.
    tf_train_dataset = tf.placeholder(tf.float32,
                                      shape=(batch_size, image_size * image_size))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)

    # Variables.
    weights = tf.Variable(
      tf.truncated_normal([image_size * image_size, num_labels]))
    biases = tf.Variable(tf.zeros([num_labels]))

    # Training computation.
    logits = tf.matmul(tf_train_dataset, weights) + biases
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))

    # Optimizer.
    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(
      tf.matmul(tf_valid_dataset, weights) + biases)
    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)
#+END_SRC

Let's run it:

#+BEGIN_SRC python
  num_steps = 3001

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print("Initialized")
    for step in range(num_steps):
      # Pick an offset within the training data, which has been randomized.
      # Note: we could use better randomization across epochs.
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      # Generate a minibatch.
      batch_data = train_dataset[offset:(offset + batch_size), :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      # Prepare a dictionary telling the session where to feed the minibatch.
      # The key of the dictionary is the placeholder node of the graph to be fed,
      # and the value is the numpy array to feed to it.
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
      _, l, predictions = session.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 500 == 0):
        print("Minibatch loss at step %d: %f" % (step, l))
        print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
        print("Validation accuracy: %.1f%%" % accuracy(
          valid_prediction.eval(), valid_labels))
    print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+BEGIN_EXAMPLE
     Initialized
     Minibatch loss at step 0: 16.320572
     Minibatch accuracy: 13.3%
     Validation accuracy: 14.4%
     Minibatch loss at step 500: 1.573464
     Minibatch accuracy: 73.4%
     Validation accuracy: 78.2%
     Minibatch loss at step 1000: 0.979085
     Minibatch accuracy: 81.2%
     Validation accuracy: 79.5%
     Minibatch loss at step 1500: 0.816317
     Minibatch accuracy: 81.2%
     Validation accuracy: 79.8%
     Minibatch loss at step 2000: 1.051737
     Minibatch accuracy: 79.7%
     Validation accuracy: 79.6%
     Minibatch loss at step 2500: 0.977313
     Minibatch accuracy: 76.6%
     Validation accuracy: 80.3%
     Minibatch loss at step 3000: 0.891709
     Minibatch accuracy: 76.6%
     Validation accuracy: 80.3%
     Test accuracy: 69.5%
#+END_EXAMPLE



*** Problem 1
:PROPERTIES:
:CUSTOM_ID: problem
:END:

Turn the logistic regression example with SGD into a 1-hidden layer
neural network with rectified linear units
[[https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu][nn.relu()]]
and 1024 hidden nodes. This model should improve your validation / test
accuracy.



#+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)

       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     num_steps = 3001
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
         # Generate a minibatch.
         batch_data = train_dataset[offset:(offset + batch_size), :]
         batch_labels = train_labels[offset:(offset + batch_size), :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Initialized
     Minibatch loss at step 0: 413.522797
     Minibatch accuracy: 9.4%
     Validation accuracy: 37.4%
     Minibatch loss at step 500: 17.026733
     Minibatch accuracy: 79.7%
     Validation accuracy: 82.2%
     Minibatch loss at step 1000: 3.879690
     Minibatch accuracy: 82.0%
     Validation accuracy: 83.4%
     Minibatch loss at step 1500: 7.037672
     Minibatch accuracy: 85.2%
     Validation accuracy: 82.5%
     Minibatch loss at step 2000: 2.705339
     Minibatch accuracy: 84.4%
     Validation accuracy: 83.2%
     Minibatch loss at step 2500: 3.578274
     Minibatch accuracy: 75.8%
     Validation accuracy: 84.1%
     Minibatch loss at step 3000: 5.462776
     Minibatch accuracy: 77.3%
     Validation accuracy: 83.7%
     Test accuracy: 71.8%
 #+END_EXAMPLE


*** Problem 2
:PROPERTIES:
:CUSTOM_ID: redo-previous-proper-sgd-randomize-order
:END:

#+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = map(lambda x: x[0], apx_eql_lst)
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 6001
     # ix list for actual SGD
     def fisher_yates_sampling(iterable):
       "l - random selection from permutations(iterable)"
       l = copy.deepcopy(iterable)
       n = len(l)
       for i in range(n-1):
         j = random.randrange(n-i)
         t = l[i]
         l[i] = l[i+j]
         l[i+j] = l[i]
       return l
     rand_train_ix = fisher_yates_sampling(good_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)

       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = fisher_yates_sampling(good_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Minibatch loss at step 3000: 1.604837
     Minibatch accuracy: 91.4%
     Validation accuracy: 84.7%
     Test accuracy: 72.5%
     Minibatch loss at step 2500: 3.163114
     Minibatch accuracy: 91.4%
     Validation accuracy: 83.7%
     Minibatch loss at step 2000: 7.948224
     Minibatch accuracy: 81.2%
     Validation accuracy: 83.3%
     Minibatch loss at step 1500: 2.736376
     Minibatch accuracy: 89.8%
     Validation accuracy: 82.7%
     Minibatch loss at step 1000: 11.304239
     Minibatch accuracy: 84.4%
     Validation accuracy: 82.5%
     Minibatch loss at step 500: 17.010756
     Minibatch accuracy: 85.9%
     Validation accuracy: 82.1%
     Initialized
     Minibatch loss at step 0: 339.549652
     Minibatch accuracy: 7.8%
     Validation accuracy: 26.5%
 #+END_EXAMPLE



** Assignment 3
:PROPERTIES:
:CUSTOM_ID: assignment-3
:header-args: :session a3py
:END:

 Previously in =2_fullyconnected.ipynb=, you trained a logistic
 regression and a neural network model.

 The goal of this assignment is to explore regularization techniques.

 #+BEGIN_SRC python
     # These are all the modules we'll be using later. Make sure you can import them
     # before proceeding further.
     from __future__ import print_function
     import numpy as np
     import tensorflow as tf
     from six.moves import cPickle as pickle
 #+END_SRC

 #+RESULTS:

 First reload the data we generated in /notmist.ipynb/.

 #+BEGIN_SRC python
   pickle_file = 'notMNIST.pickle'

   with open(pickle_file, 'rb') as f:
     save = pickle.load(f)
     train_dataset = save['train_dataset']
     train_labels = save['train_labels']
     valid_dataset = save['valid_dataset']
     valid_labels = save['valid_labels']
     test_dataset = save['test_dataset']
     test_labels = save['test_labels']
     del save  # hint to help gc free up memory
     print('Training set', train_dataset.shape, train_labels.shape)
     print('Validation set', valid_dataset.shape, valid_labels.shape)
     print('Test set', test_dataset.shape, test_labels.shape)
 #+END_SRC

 #+BEGIN_EXAMPLE
   Training set (200000, 28, 28) (200000,)
       Validation set (10000, 28, 28) (10000,)
       Test set (10000, 28, 28) (10000,)
 #+END_EXAMPLE

 Reformat into a shape that's more adapted to the models we're going to
 train:

 -  data as a flat matrix,
 -  labels as float 1-hot encodings.

 #+BEGIN_SRC python
   image_size = 28
   num_labels = 10

   def reformat(dataset, labels):
     dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
     # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]
     labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
     return dataset, labels
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
   train_dataset, train_labels = reformat(train_dataset, train_labels)
   valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
   test_dataset, test_labels = reformat(test_dataset, test_labels)
   print('Training set', train_dataset.shape, train_labels.shape)
   print('Validation set', valid_dataset.shape, valid_labels.shape)
   print('Test set', test_dataset.shape, test_labels.shape)
 #+END_SRC

 #+RESULTS:

 #+BEGIN_EXAMPLE
     Training set (200000, 784) (200000, 10)
     Validation set (10000, 784) (10000, 10)
     Test set (10000, 784) (10000, 10)
 #+END_EXAMPLE

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+RESULTS:



*** Problem 1
    :PROPERTIES:
    :CUSTOM_ID: problem-1
    :END:

 Introduce and tune L2 regularization for both logistic and neural
 network models. Remember that L2 amounts to adding a penalty on the norm
 of the weights to the loss. In TensorFlow, you can compute the L2 loss
 for a tensor =t= using =nn.l2_loss(t)=. The right amount of
 regularization should improve your validation / test accuracy.



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = map(lambda x: x[0], apx_eql_lst)
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 2001
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = random_permutation(good_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024*16
     beta = .01
     keep_prob = 0.5
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)


       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = (tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+
               beta*(tf.nn.l2_loss(weights0)+tf.nn.l2_loss(weights1)))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = random_permutation(good_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Initialized
     Minibatch loss at step 0: 7347.640625
     Minibatch accuracy: 5.5%
     Validation accuracy: 37.0%
     Minibatch loss at step 500: 0.876382
     Minibatch accuracy: 96.1%
     Validation accuracy: 78.7%
     Minibatch loss at step 1000: 0.620260
     Minibatch accuracy: 92.2%
     Validation accuracy: 78.6%
     Minibatch loss at step 1500: 0.658640
     Minibatch accuracy: 88.3%
     Validation accuracy: 78.3%
     Minibatch loss at step 2000: 0.555534
     Minibatch accuracy: 93.8%
     Validation accuracy: 76.1%
     Test accuracy: 66.2%
 #+END_EXAMPLE



*** Problem 2
    :PROPERTIES:
    :CUSTOM_ID: problem-2
    :END:

 Let's demonstrate an extreme case of overfitting. Restrict your training
 data to just a few batches. What happens?



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = map(lambda x: x[0], apx_eql_lst)
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
     num_train = 800
     good_train_ix = good_train_ix[:num_train]
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 2001
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = fisher_yates_sampling(good_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     num_hidden = 1024
     beta = .01
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)

       # Training computation.
       logits = tf.matmul(hidden_dataset, weights1) + biases1
       loss = (tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+
               beta*(tf.nn.l2_loss(weights0)+tf.nn.l2_loss(weights1)))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = fisher_yates_sampling(good_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Test accuracy: 68.6%
     Validation accuracy: 79.3%
     Minibatch loss at step 2000: 0.490076
     Minibatch accuracy: 96.9%
     Validation accuracy: 80.4%
     Minibatch loss at step 1500: 0.414823
     Minibatch accuracy: 99.2%
     Validation accuracy: 81.4%
     Minibatch loss at step 1000: 0.400443
     Minibatch accuracy: 100.0%
     Validation accuracy: 81.8%
     Minibatch loss at step 500: 0.780712
     Minibatch accuracy: 97.7%
     Validation accuracy: 30.3%
     Initialized
     Minibatch loss at step 0: 6528.056152
     Minibatch accuracy: 13.3%
 #+END_EXAMPLE



*** Problem 3
    :PROPERTIES:
    :CUSTOM_ID: problem-3
    :END:

 Introduce Dropout on the hidden layer of the neural network. Remember:
 Dropout should only be introduced during training, not evaluation,
 otherwise your evaluation results would be stochastic as well.
 TensorFlow provides =nn.dropout()= for that, but you have to make sure
 it's only inserted during training.

 What happens to our extreme overfitting case?



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = list(map(lambda x: x[0], apx_eql_lst))
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     train_fraction = .01
     num_train = round(train_fraction*len(good_train_ix))
     actual_train_ix = good_train_ix[:num_train]
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     num_steps = 1001
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = random_permutation(actual_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     batch_size = 128
     keep_prob = .5                  # 0<keep_prob<=1
     num_hidden = 1024*4
     beta = .01
     graph = tf.Graph()
     with graph.as_default():

       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(batch_size, image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables.
       weights0 = tf.Variable(
         tf.truncated_normal([image_size * image_size, num_hidden]))
       biases0 = tf.Variable(tf.zeros([num_hidden]))
       weights1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))
       biases1 = tf.Variable(tf.truncated_normal([num_labels]))

       # hidden
       hidden_dataset = tf.nn.relu(tf.matmul(tf_train_dataset, weights0) + biases0)
       hidden_drop = tf.nn.dropout(hidden_dataset,keep_prob)*(1/keep_prob)

       # Training computation.
       logits = tf.matmul(hidden_drop, weights1) + biases1
       loss = (tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+
               beta*(tf.nn.l2_loss(weights0)+tf.nn.l2_loss(weights1)))
      
       # Optimizer.
       optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
      
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights0) + biases0)
       valid_prediction = tf.nn.softmax(
         tf.matmul(valid_hidden, weights1) + biases1)
       test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights0) + biases0)
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights1) + biases1)
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(num_steps):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * batch_size) % (len(rand_train_ix) - batch_size)
         if offset < last_offset:
           rand_train_ix = random_permutation(actual_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + batch_size)], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + batch_size)], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 #+END_SRC

 #+BEGIN_EXAMPLE
     Test accuracy: 69.6%
     Minibatch loss at step 2000: 0.485670
     Minibatch accuracy: 94.5%
     Validation accuracy: 80.7%
     Minibatch loss at step 1500: 0.527620
     Minibatch accuracy: 96.9%
     Validation accuracy: 82.2%
     Minibatch loss at step 1000: 1.110798
     Minibatch accuracy: 95.3%
     Validation accuracy: 83.2%
     Minibatch loss at step 500: 98.461121
     Minibatch accuracy: 99.2%
     Validation accuracy: 82.6%
     Validation accuracy: 35.0%
     Initialized
     Minibatch loss at step 0: 14592.014648
     Minibatch accuracy: 12.5%
 #+END_EXAMPLE



*** Problem 4
    :PROPERTIES:
    :CUSTOM_ID: problem-4
    :END:

 Try to get the best performance you can using a multi-layer model! The
 best reported test accuracy using a deep network is
 [[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595][97.1%]].

 One avenue you can explore is to add multiple layers.

 Another one is to use learning rate decay:

 #+BEGIN_EXAMPLE
     global_step = tf.Variable(0)  # count the number of steps taken.
     learning_rate = tf.train.exponential_decay(0.5, global_step, ...)
     optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)
 #+END_EXAMPLE



 #+BEGIN_SRC python
     import numpy as np
     pickle_file = 'eql_lsts.pickle'
     eql_lsts = np.load(pickle_file)
     apx_eql_lst = eql_lsts["apx_lst"]
 #+END_SRC

 #+BEGIN_SRC python
     import itertools
     bad_train_ix = list(map(lambda x: x[0], apx_eql_lst))
     good_train_ix = list(filter(lambda i: i not in bad_train_ix, 
                                 range(train_dataset.shape[0])))
 #+END_SRC

 #+BEGIN_SRC python
     train_fraction = 1
     num_train = round(train_fraction*len(good_train_ix))
     actual_train_ix = good_train_ix[:num_train]
 #+END_SRC

 #+BEGIN_SRC python
     import copy
     import random
     # ix list for actual SGD
     def random_permutation(iterable):
       return np.random.permutation(len(iterable))
       # # fisher/yates:
       # "l - random selection from permutations(iterable)"
       # l = copy.deepcopy(iterable)
       # n = len(l)
       # for i in range(n-1):
       #   j = random.randrange(n-i)
       #   t = l[i]
       #   l[i] = l[i+j]
       #   l[i+j] = l[i]
       # return l
     rand_train_ix = random_permutation(actual_train_ix)
 #+END_SRC

 #+BEGIN_SRC python
     parameters = {
       'num_steps':6501,
       'batch_size':128,
       'keep_prob':.8,                  # 0<keep_prob<=1
       'learning_rate':[
         0.001,          # Base learning rate.
         128,           # Current index into the dataset (multiply by batch size).
         num_train,     # Decay steps.
         0.8           # Decay rate.
         ],
       'beta':.01,   # regularization parameter
       'num_hidden':[2^10,2^10],
       'layer_fcn':[tf.nn.relu,tf.nn.relu] ,
       'num_hidden_layers':2,
       'momentum':.9,
       'opt_fcn':tf.train.MomentumOptimizer # AdamOptimizer,MomentumOptimizer,GradientDescentOptimizer
     }
     assert parameters['num_hidden_layers'] == len(parameters['layer_fcn']) == len(parameters['num_hidden'])
     graph = tf.Graph()
     with graph.as_default():
       batch = tf.Variable(0)
       learning_rate = tf.train.exponential_decay(
         parameters['learning_rate'][0],
         parameters['learning_rate'][1]*batch,
         *parameters['learning_rate'][2:],
         staircase=True)
       # Input data. For the training data, we use a placeholder that will be fed
       # at run time with a training minibatch.
       tf_train_dataset = tf.placeholder(tf.float32,
                                         shape=(parameters['batch_size'], image_size * image_size))
       tf_train_labels = tf.placeholder(tf.float32, shape=(parameters['batch_size'], num_labels))
       tf_valid_dataset = tf.constant(valid_dataset)
       tf_test_dataset = tf.constant(test_dataset)
      
       # Variables
       weights = [tf.Variable(
         tf.truncated_normal([image_size * image_size, parameters['num_hidden'][0]]))]
       biases = [tf.Variable(tf.zeros([parameters['num_hidden'][0]]))]
       hidden_dataset = tf.nn.dropout(
         parameters['layer_fcn'][0](tf.add(tf.matmul(tf_train_dataset, weights[0]), biases[0])), 
         parameters['keep_prob'])*(1/parameters['keep_prob'])

       for l in range(1,parameters['num_hidden_layers']):
         weights += [tf.Variable(tf.truncated_normal([parameters['num_hidden'][l-1], parameters['num_hidden'][l]]))]
         biases += [tf.Variable(tf.zeros([parameters['num_hidden'][l]]))]
         hidden_dataset = tf.nn.dropout(
           parameters['layer_fcn'][l](tf.add(tf.matmul(hidden_dataset, weights[l]), biases[l])),
           parameters['keep_prob'])*(1/parameters['keep_prob'])


       weights += [tf.Variable(tf.truncated_normal([parameters['num_hidden'][-1], num_labels]))]
       biases += [tf.Variable(tf.zeros([num_labels]))]
     
       # Training computation.
       logits = tf.matmul(hidden_dataset, weights[-1]) + biases[-1]
       loss = tf.reduce_mean(
         tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
       for i in range(parameters['num_hidden_layers']+1):
         loss += parameters['beta']*tf.nn.l2_loss(weights[i])

       # Optimizer
       # optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
       optimizer = parameters['opt_fcn'](
         learning_rate,
         parameters['momentum']).minimize(loss, global_step=batch)
       # optimizer = parameters['opt_fcn'](
       #   learning_rate=parameters['learning_rate'], 
       #   global_step=parameters['global_step']).minimize(loss)
       # Predictions for the training, validation, and test data.
       train_prediction = tf.nn.softmax(logits)

       valid_hidden = parameters['layer_fcn'][0](tf.matmul(tf_valid_dataset, weights[0]) + 
                                   biases[0])
       test_hidden = parameters['layer_fcn'][0](tf.matmul(tf_test_dataset, weights[0]) + 
                                  biases[0])
       for l in range(1,parameters['num_hidden_layers']):
         valid_hidden = parameters['layer_fcn'][l](tf.matmul(valid_hidden, weights[l]) +
                                     biases[l])
         test_hidden = parameters['layer_fcn'][l](tf.matmul(test_hidden, weights[l]) + 
                                    biases[l])

       valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, weights[-1]) + 
                                        biases[-1])
       test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights[-1]) + 
                                       biases[-1])
 #+END_SRC

 #+BEGIN_SRC python
     def accuracy(predictions, labels):
       return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
               / predictions.shape[0])
 #+END_SRC

 #+BEGIN_SRC python
     offset = 0
     test_accuracy = 0
     with tf.Session(graph=graph) as session:
       tf.initialize_all_variables().run()
       print("Initialized")
       for step in range(parameters['num_steps']):
         # Pick an offset within the training data, which has been randomized.
         # Note: we could use better randomization across epochs.
         last_offset = offset
         offset = (step * parameters['batch_size']) % (len(rand_train_ix) - parameters['batch_size'])
         if offset < last_offset:
           rand_train_ix = random_permutation(actual_train_ix)
         # Generate a minibatch.
         batch_data = train_dataset[rand_train_ix[offset:(offset + parameters['batch_size'])], :]
         batch_labels = train_labels[rand_train_ix[offset:(offset + parameters['batch_size'])], :]
         # Prepare a dictionary telling the session where to feed the minibatch.
         # The key of the dictionary is the placeholder node of the graph to be fed,
         # and the value is the numpy array to feed to it.
         feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
         _, l, predictions = session.run(
           [optimizer, loss, train_prediction], feed_dict=feed_dict)
         if (step % 500 == 0):
           print("Minibatch loss at step %d: %f" % (step, l))
           print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
           print("Validation accuracy: %.1f%%" % accuracy(
             valid_prediction.eval(), valid_labels))
       test_accuracy = accuracy(test_prediction.eval(), test_labels)
       print("Test accuracy: %.1f%%" % test_accuracy)
 #+END_SRC

 #+BEGIN_EXAMPLE
     Test accuracy: 25.1%
     Minibatch loss at step 6500: 11.521459
     Minibatch accuracy: 28.9%
     Validation accuracy: 28.7%
     Minibatch loss at step 6000: 12.083858
     Minibatch accuracy: 21.9%
     Validation accuracy: 27.6%
     Minibatch loss at step 5500: 12.607504
     Minibatch accuracy: 21.1%
     Validation accuracy: 26.5%
     Minibatch loss at step 5000: 13.112435
     Minibatch accuracy: 28.1%
     Validation accuracy: 25.1%
     Minibatch loss at step 4500: 13.792360
     Minibatch accuracy: 22.7%
     Validation accuracy: 23.3%
     Minibatch loss at step 4000: 14.609591
     Minibatch accuracy: 18.8%
     Validation accuracy: 22.4%
     Minibatch loss at step 3500: 15.401945
     Minibatch accuracy: 18.8%
     Validation accuracy: 21.5%
     Minibatch loss at step 3000: 16.365429
     Minibatch accuracy: 21.1%
     Validation accuracy: 20.2%
     Minibatch loss at step 2500: 17.581362
     Minibatch accuracy: 17.2%
     Validation accuracy: 19.9%
     Minibatch loss at step 2000: 18.865662
     Minibatch accuracy: 14.8%
     Validation accuracy: 18.9%
     Minibatch loss at step 1500: 20.277245
     Minibatch accuracy: 19.5%
     Validation accuracy: 18.5%
     Minibatch loss at step 1000: 22.221041
     Minibatch accuracy: 25.0%
     Validation accuracy: 17.8%
     Minibatch loss at step 500: 24.418331
     Minibatch accuracy: 11.7%
     Validation accuracy: 17.3%
     Initialized
     Minibatch loss at step 0: 65.115540
     Minibatch accuracy: 11.7%
     Validation accuracy: 8.8%
 #+END_EXAMPLE

 #+BEGIN_SRC python
     with open("results.txt", "a") as myfile:
       myfile.write(str(parameters))
       myfile.write("\n"+str(test_accuracy))
 #+END_SRC


** Assignment 4
:PROPERTIES:
:header-args: :session a4py
:END:

*** Starter code

Previously in =2_fullyconnected.ipynb= and =3_regularization.ipynb=, we
trained fully connected networks to classify
[[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST]]
characters.

The goal of this assignment is make the neural network convolutional.

#+BEGIN_SRC python :session a4aspy
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import numpy as np
  import tensorflow as tf
  from six.moves import cPickle as pickle
  from six.moves import range
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output
  pickle_file = 'notMNIST.pickle'

  with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_dataset = save['train_dataset']
    train_labels = save['train_labels']
    valid_dataset = save['valid_dataset']
    valid_labels = save['valid_labels']
    test_dataset = save['test_dataset']
    test_labels = save['test_labels']
    del save  # hint to help gc free up memory
#+END_SRC

#+RESULTS:
    
#+BEGIN_SRC python :session a4aspy :results output
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+RESULTS:
: Training set (200000, 28, 28) (200000,)
: Validation set (10000, 28, 28) (10000,)
: Test set (10000, 28, 28) (10000,)


Reformat into a TensorFlow-friendly shape: - convolutions need the image
data formatted as a cube (width by height by #channels) - labels as
float 1-hot encodings.

#+BEGIN_SRC python :session a4aspy :results output
  image_size = 28
  num_labels = 10
  num_channels = 1 # grayscale

  import numpy as np

  def reformat(dataset, labels):
    dataset = dataset.reshape(
      (-1, image_size, image_size, num_channels)).astype(np.float32)
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels
#+END_SRC

#+RESULTS:
  
#+BEGIN_SRC python :session a4aspy :results output
  train_dataset, train_labels = reformat(train_dataset, train_labels)
  valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
  test_dataset, test_labels = reformat(test_dataset, test_labels)
#+END_SRC

#+RESULTS:
   
#+BEGIN_SRC python :session a4aspy :results output
   print('Training set', train_dataset.shape, train_labels.shape)
   print('Validation set', valid_dataset.shape, valid_labels.shape)
   print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+RESULTS:
: Training set (200000, 28, 28, 1) (200000, 10)
: Validation set (10000, 28, 28, 1) (10000, 10)
: Test set (10000, 28, 28, 1) (10000, 10)


#+BEGIN_SRC python :session a4aspy :results none
  def accuracy(predictions, labels):
    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
            / predictions.shape[0])
#+END_SRC

#+RESULTS:

Let's build a small network with two convolutional layers, followed by
one fully connected layer. Convolutional networks are more expensive
computationally, so we'll limit its depth and number of fully connected
nodes.

#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      shape = hidden.get_shape().as_list()
      reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
      hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(learning_rate=sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    test_prediction = tf.nn.softmax(model(tf_test_dataset))
#+END_SRC

#+RESULTS:

#+NAME: run_graph
#+BEGIN_SRC python :var nsteps = 1001 :var keep_prob = 1 :session a4aspy :results output
  num_steps = nsteps
  lr = 1e-3
  lr_decay = 0.999995
  with tf.Session(graph=graph) as sess:
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(1,num_steps):
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      lr *= lr_decay
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, 
                   sy_learn_rate: lr, sy_keep_prob: keep_prob}
      _, l, predictions = sess.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 50 == 0):
        print('Minibatch loss at step %d: %f' % (step, l))
        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+RESULTS: run_graph
#+begin_example

>>> >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2017-06-15 07:45:35.562943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 1.861184
Minibatch accuracy: 68.8%
Validation accuracy: 41.9%
Minibatch loss at step 100: 5.072113
Minibatch accuracy: 31.2%
Validation accuracy: 51.2%
Minibatch loss at step 150: 4.068310
Minibatch accuracy: 37.5%
Validation accuracy: 57.7%
Minibatch loss at step 200: 2.123676
Minibatch accuracy: 50.0%
Validation accuracy: 57.4%
Minibatch loss at step 250: 1.832294
Minibatch accuracy: 56.2%
Validation accuracy: 61.3%
Minibatch loss at step 300: 0.926931
Minibatch accuracy: 75.0%
Validation accuracy: 67.6%
Minibatch loss at step 350: 0.491697
Minibatch accuracy: 81.2%
Validation accuracy: 68.0%
Minibatch loss at step 400: 0.317087
Minibatch accuracy: 87.5%
Validation accuracy: 70.7%
Minibatch loss at step 450: 0.814103
Minibatch accuracy: 81.2%
Validation accuracy: 72.2%
Minibatch loss at step 500: 1.340266
Minibatch accuracy: 68.8%
Validation accuracy: 73.4%
Minibatch loss at step 550: 0.328524
Minibatch accuracy: 87.5%
Validation accuracy: 73.7%
Minibatch loss at step 600: 0.437495
Minibatch accuracy: 87.5%
Validation accuracy: 74.5%
Minibatch loss at step 650: 0.908238
Minibatch accuracy: 75.0%
Validation accuracy: 75.5%
Minibatch loss at step 700: 0.803429
Minibatch accuracy: 75.0%
Validation accuracy: 75.7%
Minibatch loss at step 750: 1.014526
Minibatch accuracy: 81.2%
Validation accuracy: 76.5%
Minibatch loss at step 800: 0.558631
Minibatch accuracy: 81.2%
Validation accuracy: 77.7%
Minibatch loss at step 850: 0.429926
Minibatch accuracy: 87.5%
Validation accuracy: 78.2%
Minibatch loss at step 900: 0.420720
Minibatch accuracy: 81.2%
Validation accuracy: 78.2%
Minibatch loss at step 950: 0.818006
Minibatch accuracy: 68.8%
Validation accuracy: 79.2%
Minibatch loss at step 1000: 0.752457
Minibatch accuracy: 81.2%
Validation accuracy: 79.0%
Test accuracy: 85.7%
#+end_example



*** Problem 1 

The convolutional model above uses convolutions with stride 2 to reduce
the dimensionality. Replace the strides by a max pooling operation
(=nn.max_pool()=) of stride 2 and kernel size 2.

#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      hidden = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      hidden = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = hidden.get_shape().as_list()
      reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
      hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(learning_rate=sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    test_prediction = tf.nn.softmax(model(tf_test_dataset))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
08:09:56.957957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 2.900611
Minibatch accuracy: 68.8%
Validation accuracy: 47.1%
Minibatch loss at step 100: 1.479307
Minibatch accuracy: 56.2%
Validation accuracy: 54.6%
Minibatch loss at step 150: 4.690967
Minibatch accuracy: 31.2%
Validation accuracy: 58.4%
Minibatch loss at step 200: 1.523903
Minibatch accuracy: 62.5%
Validation accuracy: 61.9%
Minibatch loss at step 250: 1.892011
Minibatch accuracy: 50.0%
Validation accuracy: 66.5%
Minibatch loss at step 300: 1.033208
Minibatch accuracy: 75.0%
Validation accuracy: 67.8%
Minibatch loss at step 350: 1.215178
Minibatch accuracy: 75.0%
Validation accuracy: 69.8%
Minibatch loss at step 400: 0.688676
Minibatch accuracy: 62.5%
Validation accuracy: 70.1%
Minibatch loss at step 450: 1.180127
Minibatch accuracy: 75.0%
Validation accuracy: 73.0%
Minibatch loss at step 500: 1.524866
Minibatch accuracy: 68.8%
Validation accuracy: 73.9%
Minibatch loss at step 550: 0.290829
Minibatch accuracy: 87.5%
Validation accuracy: 75.3%
Minibatch loss at step 600: 0.348496
Minibatch accuracy: 87.5%
Validation accuracy: 75.7%
Minibatch loss at step 650: 1.442225
Minibatch accuracy: 68.8%
Validation accuracy: 74.8%
Minibatch loss at step 700: 0.850661
Minibatch accuracy: 68.8%
Validation accuracy: 75.8%
Minibatch loss at step 750: 1.095397
Minibatch accuracy: 62.5%
Validation accuracy: 77.4%
Minibatch loss at step 800: 0.330389
Minibatch accuracy: 87.5%
Validation accuracy: 78.0%
Minibatch loss at step 850: 0.832846
Minibatch accuracy: 81.2%
Validation accuracy: 78.1%
Minibatch loss at step 900: 0.486584
Minibatch accuracy: 87.5%
Validation accuracy: 77.4%
Minibatch loss at step 950: 1.308142
Minibatch accuracy: 68.8%
Validation accuracy: 78.9%
Minibatch loss at step 1000: 0.909435
Minibatch accuracy: 75.0%
Validation accuracy: 79.4%
Minibatch loss at step 1050: 1.192717
Minibatch accuracy: 56.2%
Validation accuracy: 80.1%
Minibatch loss at step 1100: 0.584909
Minibatch accuracy: 81.2%
Validation accuracy: 80.2%
Minibatch loss at step 1150: 0.142735
Minibatch accuracy: 100.0%
Validation accuracy: 79.5%
Minibatch loss at step 1200: 0.088882
Minibatch accuracy: 100.0%
Validation accuracy: 80.1%
Minibatch loss at step 1250: 0.535772
Minibatch accuracy: 81.2%
Validation accuracy: 79.1%
Minibatch loss at step 1300: 0.890280
Minibatch accuracy: 81.2%
Validation accuracy: 81.3%
Minibatch loss at step 1350: 0.338932
Minibatch accuracy: 87.5%
Validation accuracy: 80.3%
Minibatch loss at step 1400: 0.214247
Minibatch accuracy: 93.8%
Validation accuracy: 81.0%
Minibatch loss at step 1450: 1.065812
Minibatch accuracy: 62.5%
Validation accuracy: 81.4%
Minibatch loss at step 1500: 0.647235
Minibatch accuracy: 75.0%
Validation accuracy: 82.0%
Minibatch loss at step 1550: 0.544307
Minibatch accuracy: 87.5%
Validation accuracy: 81.8%
Minibatch loss at step 1600: 0.329639
Minibatch accuracy: 87.5%
Validation accuracy: 81.3%
Minibatch loss at step 1650: 0.874878
Minibatch accuracy: 68.8%
Validation accuracy: 81.2%
Minibatch loss at step 1700: 1.150976
Minibatch accuracy: 62.5%
Validation accuracy: 81.8%
Minibatch loss at step 1750: 0.856514
Minibatch accuracy: 81.2%
Validation accuracy: 81.4%
Minibatch loss at step 1800: 0.306168
Minibatch accuracy: 93.8%
Validation accuracy: 81.8%
Minibatch loss at step 1850: 0.579684
Minibatch accuracy: 81.2%
Validation accuracy: 81.7%
Minibatch loss at step 1900: 0.453013
Minibatch accuracy: 87.5%
Validation accuracy: 82.7%
Minibatch loss at step 1950: 0.422659
Minibatch accuracy: 87.5%
Validation accuracy: 83.0%
Minibatch loss at step 2000: 0.557044
Minibatch accuracy: 75.0%
Validation accuracy: 82.8%
Minibatch loss at step 2050: 0.886053
Minibatch accuracy: 75.0%
Validation accuracy: 83.4%
Minibatch loss at step 2100: 0.506418
Minibatch accuracy: 87.5%
Validation accuracy: 82.9%
Minibatch loss at step 2150: 0.170724
Minibatch accuracy: 100.0%
Validation accuracy: 82.1%
Minibatch loss at step 2200: 0.240701
Minibatch accuracy: 93.8%
Validation accuracy: 83.3%
Minibatch loss at step 2250: 0.123136
Minibatch accuracy: 100.0%
Validation accuracy: 83.7%
Minibatch loss at step 2300: 0.540390
Minibatch accuracy: 81.2%
Validation accuracy: 83.4%
Minibatch loss at step 2350: 0.372324
Minibatch accuracy: 93.8%
Validation accuracy: 82.9%
Minibatch loss at step 2400: 0.453190
Minibatch accuracy: 81.2%
Validation accuracy: 83.9%
Minibatch loss at step 2450: 1.779238
Minibatch accuracy: 62.5%
Validation accuracy: 83.5%
Minibatch loss at step 2500: 0.705045
Minibatch accuracy: 68.8%
Validation accuracy: 84.0%
Minibatch loss at step 2550: 1.292835
Minibatch accuracy: 68.8%
Validation accuracy: 83.6%
Minibatch loss at step 2600: 0.377887
Minibatch accuracy: 87.5%
Validation accuracy: 83.2%
Minibatch loss at step 2650: 1.297781
Minibatch accuracy: 68.8%
Validation accuracy: 83.7%
Minibatch loss at step 2700: 0.512231
Minibatch accuracy: 81.2%
Validation accuracy: 83.9%
Minibatch loss at step 2750: 1.071017
Minibatch accuracy: 81.2%
Validation accuracy: 84.2%
Minibatch loss at step 2800: 0.298933
Minibatch accuracy: 81.2%
Validation accuracy: 84.4%
Minibatch loss at step 2850: 1.001704
Minibatch accuracy: 62.5%
Validation accuracy: 83.7%
Minibatch loss at step 2900: 0.356880
Minibatch accuracy: 93.8%
Validation accuracy: 84.4%
Minibatch loss at step 2950: 0.796455
Minibatch accuracy: 62.5%
Validation accuracy: 84.3%
Minibatch loss at step 3000: 0.895969
Minibatch accuracy: 68.8%
Validation accuracy: 84.5%
Minibatch loss at step 3050: 0.272711
Minibatch accuracy: 93.8%
Validation accuracy: 84.1%
Minibatch loss at step 3100: 0.733070
Minibatch accuracy: 81.2%
Validation accuracy: 84.0%
Minibatch loss at step 3150: 0.692155
Minibatch accuracy: 81.2%
Validation accuracy: 84.1%
Minibatch loss at step 3200: 0.413291
Minibatch accuracy: 87.5%
Validation accuracy: 84.3%
Minibatch loss at step 3250: 0.643606
Minibatch accuracy: 75.0%
Validation accuracy: 84.4%
Minibatch loss at step 3300: 0.573235
Minibatch accuracy: 87.5%
Validation accuracy: 85.2%
Minibatch loss at step 3350: 0.737073
Minibatch accuracy: 68.8%
Validation accuracy: 85.1%
Minibatch loss at step 3400: 0.278904
Minibatch accuracy: 93.8%
Validation accuracy: 84.3%
Minibatch loss at step 3450: 0.902674
Minibatch accuracy: 81.2%
Validation accuracy: 85.0%
Minibatch loss at step 3500: 0.291990
Minibatch accuracy: 87.5%
Validation accuracy: 84.5%
Minibatch loss at step 3550: 0.689245
Minibatch accuracy: 75.0%
Validation accuracy: 85.0%
Minibatch loss at step 3600: 0.756285
Minibatch accuracy: 68.8%
Validation accuracy: 85.4%
Minibatch loss at step 3650: 0.357790
Minibatch accuracy: 87.5%
Validation accuracy: 84.7%
Minibatch loss at step 3700: 1.061131
Minibatch accuracy: 62.5%
Validation accuracy: 84.6%
Minibatch loss at step 3750: 0.835744
Minibatch accuracy: 75.0%
Validation accuracy: 85.7%
Minibatch loss at step 3800: 0.615569
Minibatch accuracy: 87.5%
Validation accuracy: 85.9%
Minibatch loss at step 3850: 0.258741
Minibatch accuracy: 93.8%
Validation accuracy: 85.5%
Minibatch loss at step 3900: 0.446685
Minibatch accuracy: 81.2%
Validation accuracy: 85.8%
Minibatch loss at step 3950: 0.727674
Minibatch accuracy: 68.8%
Validation accuracy: 85.2%
Minibatch loss at step 4000: 0.505325
Minibatch accuracy: 87.5%
Validation accuracy: 85.4%
Minibatch loss at step 4050: 0.447677
Minibatch accuracy: 87.5%
Validation accuracy: 86.2%
Minibatch loss at step 4100: 0.272943
Minibatch accuracy: 93.8%
Validation accuracy: 85.3%
Minibatch loss at step 4150: 0.159460
Minibatch accuracy: 93.8%
Validation accuracy: 85.2%
Minibatch loss at step 4200: 0.271813
Minibatch accuracy: 93.8%
Validation accuracy: 85.3%
Minibatch loss at step 4250: 0.403566
Minibatch accuracy: 87.5%
Validation accuracy: 85.1%
Minibatch loss at step 4300: 0.862058
Minibatch accuracy: 75.0%
Validation accuracy: 85.2%
Minibatch loss at step 4350: 0.403553
Minibatch accuracy: 93.8%
Validation accuracy: 86.0%
Minibatch loss at step 4400: 0.576407
Minibatch accuracy: 81.2%
Validation accuracy: 85.5%
Minibatch loss at step 4450: 0.401695
Minibatch accuracy: 87.5%
Validation accuracy: 85.7%
Minibatch loss at step 4500: 0.616162
Minibatch accuracy: 81.2%
Validation accuracy: 83.4%
Minibatch loss at step 4550: 0.346180
Minibatch accuracy: 87.5%
Validation accuracy: 86.1%
Minibatch loss at step 4600: 0.267765
Minibatch accuracy: 93.8%
Validation accuracy: 86.2%
Minibatch loss at step 4650: 0.444128
Minibatch accuracy: 87.5%
Validation accuracy: 85.7%
Minibatch loss at step 4700: 0.482445
Minibatch accuracy: 87.5%
Validation accuracy: 86.0%
Minibatch loss at step 4750: 0.698112
Minibatch accuracy: 68.8%
Validation accuracy: 85.8%
Minibatch loss at step 4800: 0.499284
Minibatch accuracy: 81.2%
Validation accuracy: 85.4%
Minibatch loss at step 4850: 0.811626
Minibatch accuracy: 81.2%
Validation accuracy: 86.4%
Minibatch loss at step 4900: 0.526118
Minibatch accuracy: 75.0%
Validation accuracy: 86.0%
Minibatch loss at step 4950: 0.191419
Minibatch accuracy: 93.8%
Validation accuracy: 85.8%
Minibatch loss at step 5000: 0.389479
Minibatch accuracy: 87.5%
Validation accuracy: 85.9%
Test accuracy: 91.7%
#+end_example


*** Problem 2
Try to get the best performance you can using a convolutional net. Look for example at the classic [[http://yann.lecun.com/exdb/lenet/][LeNet5]] architecture, adding Dropout, and/or adding learning rate decay.

**** experiments

***** 3
learning rate_decay + dropout+ regularization
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  SEED = np.random.randint(low=np.iinfo(np.uint32).min, 
                           high=np.iinfo(np.uint32).max, size=1)[0]
  beta = 5e-28
  graph = tf.Graph()

  with graph.as_default():
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    sy_keep_prob = tf.placeholder(tf.float32)
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1, seed = SEED))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1, seed=SEED))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1, seed=SEED))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1, seed=SEED))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, sy_keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.div(tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), sy_keep_prob, seed=SEED),sy_keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    regularizers = (tf.nn.l2_loss(layer1_weights)+ 
                    tf.nn.l2_loss(layer2_weights)+ 
                    tf.nn.l2_loss(layer3_weights)+ 
                    tf.nn.l2_loss(layer4_weights))
    loss += beta*regularizers
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001, keep_prob=.5) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
 2017-06-15 09:39:23.291681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 4.247050
Minibatch accuracy: 37.5%
Validation accuracy: 27.4%
Minibatch loss at step 100: 4.601756
Minibatch accuracy: 18.8%
Validation accuracy: 35.9%
Minibatch loss at step 150: 3.145462
Minibatch accuracy: 12.5%
Validation accuracy: 44.5%
Minibatch loss at step 200: 3.035318
Minibatch accuracy: 18.8%
Validation accuracy: 46.6%
Minibatch loss at step 250: 2.477204
Minibatch accuracy: 31.2%
Validation accuracy: 51.1%
Minibatch loss at step 300: 1.430561
Minibatch accuracy: 43.8%
Validation accuracy: 55.5%
Minibatch loss at step 350: 1.477067
Minibatch accuracy: 56.2%
Validation accuracy: 61.2%
Minibatch loss at step 400: 1.614931
Minibatch accuracy: 43.8%
Validation accuracy: 65.2%
Minibatch loss at step 450: 1.169079
Minibatch accuracy: 62.5%
Validation accuracy: 66.7%
Minibatch loss at step 500: 1.250844
Minibatch accuracy: 62.5%
Validation accuracy: 68.4%
Minibatch loss at step 550: 0.673903
Minibatch accuracy: 68.8%
Validation accuracy: 69.3%
Minibatch loss at step 600: 0.872271
Minibatch accuracy: 75.0%
Validation accuracy: 70.5%
Minibatch loss at step 650: 1.511239
Minibatch accuracy: 43.8%
Validation accuracy: 71.8%
Minibatch loss at step 700: 1.044477
Minibatch accuracy: 50.0%
Validation accuracy: 70.9%
Minibatch loss at step 750: 1.326239
Minibatch accuracy: 56.2%
Validation accuracy: 73.6%
Minibatch loss at step 800: 0.657693
Minibatch accuracy: 81.2%
Validation accuracy: 74.0%
Minibatch loss at step 850: 0.522227
Minibatch accuracy: 87.5%
Validation accuracy: 74.5%
Minibatch loss at step 900: 0.841039
Minibatch accuracy: 81.2%
Validation accuracy: 75.1%
Minibatch loss at step 950: 1.378965
Minibatch accuracy: 56.2%
Validation accuracy: 75.7%
Minibatch loss at step 1000: 0.972474
Minibatch accuracy: 62.5%
Validation accuracy: 75.6%
Minibatch loss at step 1050: 1.275453
Minibatch accuracy: 50.0%
Validation accuracy: 77.0%
Minibatch loss at step 1100: 0.984924
Minibatch accuracy: 81.2%
Validation accuracy: 77.4%
Minibatch loss at step 1150: 0.632606
Minibatch accuracy: 81.2%
Validation accuracy: 78.3%
Minibatch loss at step 1200: 0.348040
Minibatch accuracy: 87.5%
Validation accuracy: 76.6%
Minibatch loss at step 1250: 1.205865
Minibatch accuracy: 75.0%
Validation accuracy: 77.1%
Minibatch loss at step 1300: 1.500824
Minibatch accuracy: 62.5%
Validation accuracy: 78.6%
Minibatch loss at step 1350: 0.564847
Minibatch accuracy: 87.5%
Validation accuracy: 78.8%
Minibatch loss at step 1400: 1.520104
Minibatch accuracy: 75.0%
Validation accuracy: 78.6%
Minibatch loss at step 1450: 1.583120
Minibatch accuracy: 68.8%
Validation accuracy: 78.7%
Minibatch loss at step 1500: 0.723130
Minibatch accuracy: 75.0%
Validation accuracy: 79.1%
Minibatch loss at step 1550: 1.007327
Minibatch accuracy: 75.0%
Validation accuracy: 78.4%
Minibatch loss at step 1600: 0.972170
Minibatch accuracy: 62.5%
Validation accuracy: 78.7%
Minibatch loss at step 1650: 1.221925
Minibatch accuracy: 43.8%
Validation accuracy: 78.9%
Minibatch loss at step 1700: 1.277078
Minibatch accuracy: 43.8%
Validation accuracy: 77.3%
Minibatch loss at step 1750: 1.305301
Minibatch accuracy: 81.2%
Validation accuracy: 79.1%
Minibatch loss at step 1800: 1.140792
Minibatch accuracy: 68.8%
Validation accuracy: 80.2%
Minibatch loss at step 1850: 0.916527
Minibatch accuracy: 68.8%
Validation accuracy: 79.4%
Minibatch loss at step 1900: 0.319315
Minibatch accuracy: 93.8%
Validation accuracy: 80.2%
Minibatch loss at step 1950: 0.566165
Minibatch accuracy: 75.0%
Validation accuracy: 80.6%
Minibatch loss at step 2000: 0.734313
Minibatch accuracy: 75.0%
Validation accuracy: 80.3%
Minibatch loss at step 2050: 0.861633
Minibatch accuracy: 75.0%
Validation accuracy: 79.8%
Minibatch loss at step 2100: 0.872185
Minibatch accuracy: 75.0%
Validation accuracy: 81.3%
Minibatch loss at step 2150: 0.436945
Minibatch accuracy: 81.2%
Validation accuracy: 80.5%
Minibatch loss at step 2200: 0.475010
Minibatch accuracy: 81.2%
Validation accuracy: 80.8%
Minibatch loss at step 2250: 0.455218
Minibatch accuracy: 81.2%
Validation accuracy: 81.5%
Minibatch loss at step 2300: 0.766944
Minibatch accuracy: 75.0%
Validation accuracy: 81.7%
Minibatch loss at step 2350: 0.863824
Minibatch accuracy: 68.8%
Validation accuracy: 81.7%
Minibatch loss at step 2400: 0.604405
Minibatch accuracy: 81.2%
Validation accuracy: 81.2%
Minibatch loss at step 2450: 1.457657
Minibatch accuracy: 50.0%
Validation accuracy: 79.8%
Minibatch loss at step 2500: 1.104749
Minibatch accuracy: 62.5%
Validation accuracy: 81.8%
Minibatch loss at step 2550: 1.174053
Minibatch accuracy: 68.8%
Validation accuracy: 82.1%
Minibatch loss at step 2600: 0.624880
Minibatch accuracy: 75.0%
Validation accuracy: 81.6%
Minibatch loss at step 2650: 1.463562
Minibatch accuracy: 56.2%
Validation accuracy: 80.8%
Minibatch loss at step 2700: 0.774168
Minibatch accuracy: 68.8%
Validation accuracy: 82.5%
Minibatch loss at step 2750: 1.511965
Minibatch accuracy: 56.2%
Validation accuracy: 82.4%
Minibatch loss at step 2800: 0.246308
Minibatch accuracy: 93.8%
Validation accuracy: 82.2%
Minibatch loss at step 2850: 1.437925
Minibatch accuracy: 68.8%
Validation accuracy: 81.4%
Minibatch loss at step 2900: 1.087213
Minibatch accuracy: 62.5%
Validation accuracy: 82.3%
Minibatch loss at step 2950: 1.135118
Minibatch accuracy: 75.0%
Validation accuracy: 82.2%
Minibatch loss at step 3000: 0.933039
Minibatch accuracy: 75.0%
Validation accuracy: 83.0%
Minibatch loss at step 3050: 0.496922
Minibatch accuracy: 87.5%
Validation accuracy: 81.6%
Minibatch loss at step 3100: 0.832652
Minibatch accuracy: 75.0%
Validation accuracy: 82.0%
Minibatch loss at step 3150: 0.647313
Minibatch accuracy: 75.0%
Validation accuracy: 82.7%
Minibatch loss at step 3200: 0.759036
Minibatch accuracy: 75.0%
Validation accuracy: 83.3%
Minibatch loss at step 3250: 0.980326
Minibatch accuracy: 68.8%
Validation accuracy: 81.7%
Minibatch loss at step 3300: 0.746954
Minibatch accuracy: 62.5%
Validation accuracy: 82.2%
Minibatch loss at step 3350: 0.861651
Minibatch accuracy: 81.2%
Validation accuracy: 82.4%
Minibatch loss at step 3400: 0.470646
Minibatch accuracy: 75.0%
Validation accuracy: 81.9%
Minibatch loss at step 3450: 1.103467
Minibatch accuracy: 56.2%
Validation accuracy: 82.9%
Minibatch loss at step 3500: 0.320315
Minibatch accuracy: 87.5%
Validation accuracy: 82.1%
Minibatch loss at step 3550: 1.318188
Minibatch accuracy: 62.5%
Validation accuracy: 81.7%
Minibatch loss at step 3600: 0.733523
Minibatch accuracy: 62.5%
Validation accuracy: 82.2%
Minibatch loss at step 3650: 0.467246
Minibatch accuracy: 81.2%
Validation accuracy: 82.5%
Minibatch loss at step 3700: 0.993713
Minibatch accuracy: 62.5%
Validation accuracy: 80.3%
Minibatch loss at step 3750: 0.973567
Minibatch accuracy: 87.5%
Validation accuracy: 83.3%
Minibatch loss at step 3800: 0.563805
Minibatch accuracy: 81.2%
Validation accuracy: 83.7%
Minibatch loss at step 3850: 0.605255
Minibatch accuracy: 81.2%
Validation accuracy: 83.3%
Minibatch loss at step 3900: 0.619934
Minibatch accuracy: 81.2%
Validation accuracy: 83.0%
Minibatch loss at step 3950: 1.044772
Minibatch accuracy: 68.8%
Validation accuracy: 83.5%
Minibatch loss at step 4000: 0.891935
Minibatch accuracy: 56.2%
Validation accuracy: 83.8%
Minibatch loss at step 4050: 0.806958
Minibatch accuracy: 81.2%
Validation accuracy: 83.7%
Minibatch loss at step 4100: 0.429208
Minibatch accuracy: 81.2%
Validation accuracy: 83.4%
Minibatch loss at step 4150: 0.307217
Minibatch accuracy: 87.5%
Validation accuracy: 84.3%
Minibatch loss at step 4200: 0.383367
Minibatch accuracy: 93.8%
Validation accuracy: 83.3%
Minibatch loss at step 4250: 0.518670
Minibatch accuracy: 81.2%
Validation accuracy: 83.5%
Minibatch loss at step 4300: 1.212439
Minibatch accuracy: 81.2%
Validation accuracy: 84.3%
Minibatch loss at step 4350: 0.709060
Minibatch accuracy: 75.0%
Validation accuracy: 84.6%
Minibatch loss at step 4400: 0.325387
Minibatch accuracy: 93.8%
Validation accuracy: 84.6%
Minibatch loss at step 4450: 0.746855
Minibatch accuracy: 75.0%
Validation accuracy: 83.1%
Minibatch loss at step 4500: 0.570571
Minibatch accuracy: 81.2%
Validation accuracy: 84.2%
Minibatch loss at step 4550: 0.623103
Minibatch accuracy: 75.0%
Validation accuracy: 84.4%
Minibatch loss at step 4600: 0.710920
Minibatch accuracy: 68.8%
Validation accuracy: 84.2%
Minibatch loss at step 4650: 0.657528
Minibatch accuracy: 87.5%
Validation accuracy: 84.4%
Minibatch loss at step 4700: 0.913127
Minibatch accuracy: 62.5%
Validation accuracy: 84.0%
Minibatch loss at step 4750: 0.738293
Minibatch accuracy: 81.2%
Validation accuracy: 84.0%
Minibatch loss at step 4800: 0.820862
Minibatch accuracy: 81.2%
Validation accuracy: 84.7%
Minibatch loss at step 4850: 0.734651
Minibatch accuracy: 68.8%
Validation accuracy: 84.7%
Minibatch loss at step 4900: 1.284054
Minibatch accuracy: 68.8%
Validation accuracy: 84.6%
Minibatch loss at step 4950: 0.473267
Minibatch accuracy: 81.2%
Validation accuracy: 84.9%
Minibatch loss at step 5000: 0.622454
Minibatch accuracy: 68.8%
Validation accuracy: 84.5%
Test accuracy: 90.4%
#+end_example


***** 2

learning rate_decay + dropout 1
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    sy_keep_prob = tf.placeholder(tf.float32)
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, sy_keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.div(tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), sy_keep_prob),sy_keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001, keep_prob=.5) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
 2017-06-15 09:31:31.720257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 1.868340
Minibatch accuracy: 25.0%
Validation accuracy: 19.3%
Minibatch loss at step 100: 2.491972
Minibatch accuracy: 25.0%
Validation accuracy: 16.3%
Minibatch loss at step 150: 2.421418
Minibatch accuracy: 18.8%
Validation accuracy: 15.0%
Minibatch loss at step 200: 2.552014
Minibatch accuracy: 12.5%
Validation accuracy: 15.9%
Minibatch loss at step 250: 2.219394
Minibatch accuracy: 25.0%
Validation accuracy: 18.4%
Minibatch loss at step 300: 2.048537
Minibatch accuracy: 12.5%
Validation accuracy: 20.2%
Minibatch loss at step 350: 2.132483
Minibatch accuracy: 25.0%
Validation accuracy: 24.1%
Minibatch loss at step 400: 2.160150
Minibatch accuracy: 12.5%
Validation accuracy: 24.9%
Minibatch loss at step 450: 3.029988
Minibatch accuracy: 12.5%
Validation accuracy: 23.0%
Minibatch loss at step 500: 2.291509
Minibatch accuracy: 18.8%
Validation accuracy: 25.3%
Minibatch loss at step 550: 2.146008
Minibatch accuracy: 18.8%
Validation accuracy: 24.2%
Minibatch loss at step 600: 2.225887
Minibatch accuracy: 12.5%
Validation accuracy: 26.8%
Minibatch loss at step 650: 2.287751
Minibatch accuracy: 6.2%
Validation accuracy: 24.4%
Minibatch loss at step 700: 2.191564
Minibatch accuracy: 6.2%
Validation accuracy: 25.7%
Minibatch loss at step 750: 2.458062
Minibatch accuracy: 25.0%
Validation accuracy: 32.0%
Minibatch loss at step 800: 1.455145
Minibatch accuracy: 56.2%
Validation accuracy: 31.2%
Minibatch loss at step 850: 1.523927
Minibatch accuracy: 43.8%
Validation accuracy: 35.2%
Minibatch loss at step 900: 1.448095
Minibatch accuracy: 43.8%
Validation accuracy: 34.5%
Minibatch loss at step 950: 2.200823
Minibatch accuracy: 12.5%
Validation accuracy: 39.3%
Minibatch loss at step 1000: 2.055672
Minibatch accuracy: 12.5%
Validation accuracy: 37.9%
Minibatch loss at step 1050: 1.998952
Minibatch accuracy: 25.0%
Validation accuracy: 37.1%
Minibatch loss at step 1100: 2.184813
Minibatch accuracy: 25.0%
Validation accuracy: 35.5%
Minibatch loss at step 1150: 1.700816
Minibatch accuracy: 50.0%
Validation accuracy: 39.0%
Minibatch loss at step 1200: 1.727975
Minibatch accuracy: 31.2%
Validation accuracy: 41.0%
Minibatch loss at step 1250: 1.804940
Minibatch accuracy: 18.8%
Validation accuracy: 39.0%
Minibatch loss at step 1300: 1.738092
Minibatch accuracy: 37.5%
Validation accuracy: 45.6%
Minibatch loss at step 1350: 1.788531
Minibatch accuracy: 37.5%
Validation accuracy: 40.2%
Minibatch loss at step 1400: 1.873891
Minibatch accuracy: 31.2%
Validation accuracy: 39.4%
Minibatch loss at step 1450: 2.233442
Minibatch accuracy: 12.5%
Validation accuracy: 33.6%
Minibatch loss at step 1500: 1.529142
Minibatch accuracy: 50.0%
Validation accuracy: 41.5%
Minibatch loss at step 1550: 2.086451
Minibatch accuracy: 12.5%
Validation accuracy: 43.5%
Minibatch loss at step 1600: 1.694917
Minibatch accuracy: 43.8%
Validation accuracy: 43.5%
Minibatch loss at step 1650: 2.215186
Minibatch accuracy: 12.5%
Validation accuracy: 42.9%
Minibatch loss at step 1700: 2.049239
Minibatch accuracy: 25.0%
Validation accuracy: 46.1%
Minibatch loss at step 1750: 2.325694
Minibatch accuracy: 31.2%
Validation accuracy: 48.3%
Minibatch loss at step 1800: 1.955550
Minibatch accuracy: 18.8%
Validation accuracy: 44.5%
Minibatch loss at step 1850: 2.122712
Minibatch accuracy: 25.0%
Validation accuracy: 46.0%
Minibatch loss at step 1900: 3.525293
Minibatch accuracy: 56.2%
Validation accuracy: 49.0%
Minibatch loss at step 1950: 1.769775
Minibatch accuracy: 37.5%
Validation accuracy: 46.7%
Minibatch loss at step 2000: 1.658909
Minibatch accuracy: 31.2%
Validation accuracy: 50.0%
Minibatch loss at step 2050: 1.398658
Minibatch accuracy: 43.8%
Validation accuracy: 50.0%
Minibatch loss at step 2100: 1.706336
Minibatch accuracy: 31.2%
Validation accuracy: 54.1%
Minibatch loss at step 2150: 1.517875
Minibatch accuracy: 37.5%
Validation accuracy: 44.2%
Minibatch loss at step 2200: 1.972822
Minibatch accuracy: 25.0%
Validation accuracy: 43.0%
Minibatch loss at step 2250: 1.748646
Minibatch accuracy: 18.8%
Validation accuracy: 49.4%
Minibatch loss at step 2300: 1.821078
Minibatch accuracy: 18.8%
Validation accuracy: 43.1%
Minibatch loss at step 2350: 1.457965
Minibatch accuracy: 56.2%
Validation accuracy: 51.2%
Minibatch loss at step 2400: 1.607498
Minibatch accuracy: 37.5%
Validation accuracy: 54.3%
Minibatch loss at step 2450: 2.106126
Minibatch accuracy: 25.0%
Validation accuracy: 54.3%
Minibatch loss at step 2500: 2.163781
Minibatch accuracy: 12.5%
Validation accuracy: 55.3%
Minibatch loss at step 2550: 1.766758
Minibatch accuracy: 37.5%
Validation accuracy: 57.4%
Minibatch loss at step 2600: 1.667016
Minibatch accuracy: 31.2%
Validation accuracy: 51.7%
Minibatch loss at step 2650: 1.942839
Minibatch accuracy: 18.8%
Validation accuracy: 52.8%
Minibatch loss at step 2700: 1.667981
Minibatch accuracy: 50.0%
Validation accuracy: 52.8%
Minibatch loss at step 2750: 2.108427
Minibatch accuracy: 25.0%
Validation accuracy: 54.7%
Minibatch loss at step 2800: 2.137871
Minibatch accuracy: 12.5%
Validation accuracy: 57.0%
Minibatch loss at step 2850: 1.423964
Minibatch accuracy: 43.8%
Validation accuracy: 55.0%
Minibatch loss at step 2900: 1.178312
Minibatch accuracy: 62.5%
Validation accuracy: 60.3%
Minibatch loss at step 2950: 1.497456
Minibatch accuracy: 43.8%
Validation accuracy: 64.3%
Minibatch loss at step 3000: 1.899452
Minibatch accuracy: 25.0%
Validation accuracy: 65.9%
Minibatch loss at step 3050: 1.557965
Minibatch accuracy: 43.8%
Validation accuracy: 61.8%
Minibatch loss at step 3100: 2.167020
Minibatch accuracy: 43.8%
Validation accuracy: 60.1%
Minibatch loss at step 3150: 1.574410
Minibatch accuracy: 31.2%
Validation accuracy: 65.0%
Minibatch loss at step 3200: 1.563329
Minibatch accuracy: 31.2%
Validation accuracy: 63.9%
Minibatch loss at step 3250: 1.821066
Minibatch accuracy: 31.2%
Validation accuracy: 61.9%
Minibatch loss at step 3300: 1.937753
Minibatch accuracy: 12.5%
Validation accuracy: 65.5%
Minibatch loss at step 3350: 1.285227
Minibatch accuracy: 43.8%
Validation accuracy: 66.4%
Minibatch loss at step 3400: 1.164702
Minibatch accuracy: 50.0%
Validation accuracy: 60.1%
Minibatch loss at step 3450: 1.756230
Minibatch accuracy: 37.5%
Validation accuracy: 67.9%
Minibatch loss at step 3500: 1.453190
Minibatch accuracy: 50.0%
Validation accuracy: 69.2%
Minibatch loss at step 3550: 5.874414
Minibatch accuracy: 25.0%
Validation accuracy: 64.8%
Minibatch loss at step 3600: 1.309912
Minibatch accuracy: 31.2%
Validation accuracy: 63.2%
Minibatch loss at step 3650: 1.374123
Minibatch accuracy: 50.0%
Validation accuracy: 72.3%
Minibatch loss at step 3700: 2.032023
Minibatch accuracy: 12.5%
Validation accuracy: 71.4%
Minibatch loss at step 3750: 1.890442
Minibatch accuracy: 31.2%
Validation accuracy: 72.5%
Minibatch loss at step 3800: 1.468653
Minibatch accuracy: 43.8%
Validation accuracy: 70.6%
Minibatch loss at step 3850: 1.072326
Minibatch accuracy: 68.8%
Validation accuracy: 73.0%
Minibatch loss at step 3900: 1.276820
Minibatch accuracy: 37.5%
Validation accuracy: 73.2%
Minibatch loss at step 3950: 1.458550
Minibatch accuracy: 56.2%
Validation accuracy: 74.3%
Minibatch loss at step 4000: 1.425846
Minibatch accuracy: 50.0%
Validation accuracy: 75.0%
Minibatch loss at step 4050: 1.352317
Minibatch accuracy: 37.5%
Validation accuracy: 73.2%
Minibatch loss at step 4100: 1.841503
Minibatch accuracy: 37.5%
Validation accuracy: 69.4%
Minibatch loss at step 4150: 1.434784
Minibatch accuracy: 37.5%
Validation accuracy: 69.5%
Minibatch loss at step 4200: 1.577219
Minibatch accuracy: 43.8%
Validation accuracy: 73.9%
Minibatch loss at step 4250: 1.199322
Minibatch accuracy: 62.5%
Validation accuracy: 74.3%
Minibatch loss at step 4300: 3.041095
Minibatch accuracy: 31.2%
Validation accuracy: 74.4%
Minibatch loss at step 4350: 1.320103
Minibatch accuracy: 50.0%
Validation accuracy: 74.6%
Minibatch loss at step 4400: 1.233494
Minibatch accuracy: 50.0%
Validation accuracy: 74.9%
Minibatch loss at step 4450: 1.534841
Minibatch accuracy: 31.2%
Validation accuracy: 76.6%
Minibatch loss at step 4500: 1.474233
Minibatch accuracy: 31.2%
Validation accuracy: 74.2%
Minibatch loss at step 4550: 1.027656
Minibatch accuracy: 62.5%
Validation accuracy: 77.2%
Minibatch loss at step 4600: 1.420190
Minibatch accuracy: 43.8%
Validation accuracy: 77.1%
Minibatch loss at step 4650: 0.945686
Minibatch accuracy: 68.8%
Validation accuracy: 75.6%
Minibatch loss at step 4700: 1.448259
Minibatch accuracy: 50.0%
Validation accuracy: 76.8%
Minibatch loss at step 4750: 1.097918
Minibatch accuracy: 62.5%
Validation accuracy: 78.6%
Minibatch loss at step 4800: 1.283625
Minibatch accuracy: 56.2%
Validation accuracy: 76.1%
Minibatch loss at step 4850: 1.620434
Minibatch accuracy: 62.5%
Validation accuracy: 77.8%
Minibatch loss at step 4900: 1.379436
Minibatch accuracy: 50.0%
Validation accuracy: 73.8%
Minibatch loss at step 4950: 0.794228
Minibatch accuracy: 68.8%
Validation accuracy: 76.8%
Minibatch loss at step 5000: 1.118136
Minibatch accuracy: 68.8%
Validation accuracy: 77.3%
Test accuracy: 83.8%
#+end_example


***** 1

only dropout
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  tf.reset_default_graph()
  graph = tf.Graph()

  with graph.as_default():
    sy_keep_prob = tf.placeholder(tf.float32)
    sy_learn_rate = tf.placeholder(tf.float32, shape=())
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, sy_keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1],
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.div(tf.nn.dropout(
        tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), 
        sy_keep_prob), sy_keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=tf_train_labels))
    # Optimizer.
    optimizer = tf.train.AdamOptimizer(learning_rate=sy_learn_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=5001, keep_prob=.5) :results output
print(results)
#+END_SRC

#+RESULTS:
#+begin_example
 2017-06-15 09:29:31.169213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
WARNING:tensorflow:From /home/ishai/.virtualenvs/ml353_2/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:133: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.
Instructions for updating:
Use `tf.global_variables_initializer` instead.
Initialized
Minibatch loss at step 50: 11.566952
Minibatch accuracy: 6.2%
Validation accuracy: 21.2%
Minibatch loss at step 100: 3.870863
Minibatch accuracy: 12.5%
Validation accuracy: 19.8%
Minibatch loss at step 150: 2.895633
Minibatch accuracy: 31.2%
Validation accuracy: 18.5%
Minibatch loss at step 200: 2.107081
Minibatch accuracy: 31.2%
Validation accuracy: 18.0%
Minibatch loss at step 250: 1.950364
Minibatch accuracy: 25.0%
Validation accuracy: 20.6%
Minibatch loss at step 300: 1.999945
Minibatch accuracy: 31.2%
Validation accuracy: 20.1%
Minibatch loss at step 350: 2.289965
Minibatch accuracy: 6.2%
Validation accuracy: 24.4%
Minibatch loss at step 400: 2.300172
Minibatch accuracy: 12.5%
Validation accuracy: 25.6%
Minibatch loss at step 450: 3.869735
Minibatch accuracy: 12.5%
Validation accuracy: 26.2%
Minibatch loss at step 500: 2.144319
Minibatch accuracy: 18.8%
Validation accuracy: 29.3%
Minibatch loss at step 550: 1.584909
Minibatch accuracy: 43.8%
Validation accuracy: 29.3%
Minibatch loss at step 600: 2.151093
Minibatch accuracy: 12.5%
Validation accuracy: 27.3%
Minibatch loss at step 650: 2.215356
Minibatch accuracy: 18.8%
Validation accuracy: 29.4%
Minibatch loss at step 700: 2.065422
Minibatch accuracy: 31.2%
Validation accuracy: 28.2%
Minibatch loss at step 750: 2.016208
Minibatch accuracy: 43.8%
Validation accuracy: 30.5%
Minibatch loss at step 800: 1.894511
Minibatch accuracy: 31.2%
Validation accuracy: 32.8%
Minibatch loss at step 850: 1.741181
Minibatch accuracy: 31.2%
Validation accuracy: 33.9%
Minibatch loss at step 900: 1.899962
Minibatch accuracy: 37.5%
Validation accuracy: 36.7%
Minibatch loss at step 950: 1.929227
Minibatch accuracy: 37.5%
Validation accuracy: 33.0%
Minibatch loss at step 1000: 1.671435
Minibatch accuracy: 37.5%
Validation accuracy: 36.7%
Minibatch loss at step 1050: 2.124148
Minibatch accuracy: 25.0%
Validation accuracy: 37.7%
Minibatch loss at step 1100: 1.898282
Minibatch accuracy: 18.8%
Validation accuracy: 37.7%
Minibatch loss at step 1150: 1.952750
Minibatch accuracy: 31.2%
Validation accuracy: 36.0%
Minibatch loss at step 1200: 1.512580
Minibatch accuracy: 50.0%
Validation accuracy: 43.4%
Minibatch loss at step 1250: 1.629997
Minibatch accuracy: 43.8%
Validation accuracy: 40.1%
Minibatch loss at step 1300: 1.712072
Minibatch accuracy: 62.5%
Validation accuracy: 46.1%
Minibatch loss at step 1350: 2.116345
Minibatch accuracy: 50.0%
Validation accuracy: 40.9%
Minibatch loss at step 1400: 1.630803
Minibatch accuracy: 43.8%
Validation accuracy: 39.9%
Minibatch loss at step 1450: 1.668554
Minibatch accuracy: 43.8%
Validation accuracy: 43.1%
Minibatch loss at step 1500: 1.650850
Minibatch accuracy: 31.2%
Validation accuracy: 44.3%
Minibatch loss at step 1550: 1.749657
Minibatch accuracy: 31.2%
Validation accuracy: 48.2%
Minibatch loss at step 1600: 1.506760
Minibatch accuracy: 37.5%
Validation accuracy: 47.2%
Minibatch loss at step 1650: 1.655661
Minibatch accuracy: 37.5%
Validation accuracy: 47.4%
Minibatch loss at step 1700: 2.183458
Minibatch accuracy: 12.5%
Validation accuracy: 43.5%
Minibatch loss at step 1750: 1.683201
Minibatch accuracy: 37.5%
Validation accuracy: 46.4%
Minibatch loss at step 1800: 1.963095
Minibatch accuracy: 12.5%
Validation accuracy: 47.1%
Minibatch loss at step 1850: 1.611518
Minibatch accuracy: 31.2%
Validation accuracy: 39.4%
Minibatch loss at step 1900: 1.658273
Minibatch accuracy: 50.0%
Validation accuracy: 48.3%
Minibatch loss at step 1950: 1.590845
Minibatch accuracy: 25.0%
Validation accuracy: 47.9%
Minibatch loss at step 2000: 2.187780
Minibatch accuracy: 18.8%
Validation accuracy: 52.0%
Minibatch loss at step 2050: 1.394493
Minibatch accuracy: 43.8%
Validation accuracy: 50.4%
Minibatch loss at step 2100: 2.159330
Minibatch accuracy: 18.8%
Validation accuracy: 49.3%
Minibatch loss at step 2150: 1.423224
Minibatch accuracy: 37.5%
Validation accuracy: 50.3%
Minibatch loss at step 2200: 1.767830
Minibatch accuracy: 31.2%
Validation accuracy: 51.1%
Minibatch loss at step 2250: 1.687594
Minibatch accuracy: 37.5%
Validation accuracy: 52.3%
Minibatch loss at step 2300: 1.296938
Minibatch accuracy: 50.0%
Validation accuracy: 51.9%
Minibatch loss at step 2350: 2.348637
Minibatch accuracy: 18.8%
Validation accuracy: 53.7%
Minibatch loss at step 2400: 1.952907
Minibatch accuracy: 18.8%
Validation accuracy: 57.0%
Minibatch loss at step 2450: 2.153319
Minibatch accuracy: 37.5%
Validation accuracy: 56.1%
Minibatch loss at step 2500: 2.116488
Minibatch accuracy: 25.0%
Validation accuracy: 60.0%
Minibatch loss at step 2550: 1.489463
Minibatch accuracy: 50.0%
Validation accuracy: 60.7%
Minibatch loss at step 2600: 1.149191
Minibatch accuracy: 43.8%
Validation accuracy: 56.6%
Minibatch loss at step 2650: 2.170089
Minibatch accuracy: 25.0%
Validation accuracy: 63.5%
Minibatch loss at step 2700: 1.525531
Minibatch accuracy: 37.5%
Validation accuracy: 59.3%
Minibatch loss at step 2750: 1.509228
Minibatch accuracy: 37.5%
Validation accuracy: 61.9%
Minibatch loss at step 2800: 1.147282
Minibatch accuracy: 56.2%
Validation accuracy: 62.5%
Minibatch loss at step 2850: 1.438620
Minibatch accuracy: 50.0%
Validation accuracy: 63.5%
Minibatch loss at step 2900: 1.439854
Minibatch accuracy: 43.8%
Validation accuracy: 64.3%
Minibatch loss at step 2950: 1.930614
Minibatch accuracy: 25.0%
Validation accuracy: 62.6%
Minibatch loss at step 3000: 1.589028
Minibatch accuracy: 43.8%
Validation accuracy: 62.4%
Minibatch loss at step 3050: 1.181139
Minibatch accuracy: 50.0%
Validation accuracy: 59.9%
Minibatch loss at step 3100: 1.706787
Minibatch accuracy: 50.0%
Validation accuracy: 64.2%
Minibatch loss at step 3150: 1.876871
Minibatch accuracy: 56.2%
Validation accuracy: 68.5%
Minibatch loss at step 3200: 1.360961
Minibatch accuracy: 37.5%
Validation accuracy: 69.2%
Minibatch loss at step 3250: 2.285524
Minibatch accuracy: 50.0%
Validation accuracy: 70.5%
Minibatch loss at step 3300: 1.441857
Minibatch accuracy: 50.0%
Validation accuracy: 70.5%
Minibatch loss at step 3350: 1.000102
Minibatch accuracy: 68.8%
Validation accuracy: 69.4%
Minibatch loss at step 3400: 0.899509
Minibatch accuracy: 68.8%
Validation accuracy: 71.0%
Minibatch loss at step 3450: 1.446827
Minibatch accuracy: 50.0%
Validation accuracy: 73.5%
Minibatch loss at step 3500: 1.269377
Minibatch accuracy: 50.0%
Validation accuracy: 67.3%
Minibatch loss at step 3550: 1.662549
Minibatch accuracy: 31.2%
Validation accuracy: 70.9%
Minibatch loss at step 3600: 1.274959
Minibatch accuracy: 56.2%
Validation accuracy: 72.7%
Minibatch loss at step 3650: 0.897081
Minibatch accuracy: 62.5%
Validation accuracy: 74.3%
Minibatch loss at step 3700: 1.394020
Minibatch accuracy: 50.0%
Validation accuracy: 74.4%
Minibatch loss at step 3750: 1.613516
Minibatch accuracy: 50.0%
Validation accuracy: 74.4%
Minibatch loss at step 3800: 0.905103
Minibatch accuracy: 68.8%
Validation accuracy: 72.7%
Minibatch loss at step 3850: 1.036171
Minibatch accuracy: 75.0%
Validation accuracy: 76.2%
Minibatch loss at step 3900: 0.679254
Minibatch accuracy: 81.2%
Validation accuracy: 74.8%
Minibatch loss at step 3950: 1.198567
Minibatch accuracy: 43.8%
Validation accuracy: 76.8%
Minibatch loss at step 4000: 1.513273
Minibatch accuracy: 43.8%
Validation accuracy: 74.3%
Minibatch loss at step 4050: 1.243160
Minibatch accuracy: 56.2%
Validation accuracy: 74.3%
Minibatch loss at step 4100: 0.809120
Minibatch accuracy: 62.5%
Validation accuracy: 75.2%
Minibatch loss at step 4150: 0.775453
Minibatch accuracy: 75.0%
Validation accuracy: 76.8%
Minibatch loss at step 4200: 0.922803
Minibatch accuracy: 75.0%
Validation accuracy: 76.6%
Minibatch loss at step 4250: 0.865568
Minibatch accuracy: 68.8%
Validation accuracy: 76.1%
Minibatch loss at step 4300: 1.376431
Minibatch accuracy: 56.2%
Validation accuracy: 77.6%
Minibatch loss at step 4350: 1.052437
Minibatch accuracy: 56.2%
Validation accuracy: 77.3%
Minibatch loss at step 4400: 0.973835
Minibatch accuracy: 50.0%
Validation accuracy: 78.2%
Minibatch loss at step 4450: 0.995051
Minibatch accuracy: 56.2%
Validation accuracy: 78.1%
Minibatch loss at step 4500: 1.235685
Minibatch accuracy: 50.0%
Validation accuracy: 78.3%
Minibatch loss at step 4550: 1.097849
Minibatch accuracy: 43.8%
Validation accuracy: 78.2%
Minibatch loss at step 4600: 1.154400
Minibatch accuracy: 62.5%
Validation accuracy: 77.8%
Minibatch loss at step 4650: 1.348891
Minibatch accuracy: 50.0%
Validation accuracy: 76.6%
Minibatch loss at step 4700: 0.996796
Minibatch accuracy: 62.5%
Validation accuracy: 79.4%
Minibatch loss at step 4750: 0.857328
Minibatch accuracy: 56.2%
Validation accuracy: 79.3%
Minibatch loss at step 4800: 0.978085
Minibatch accuracy: 62.5%
Validation accuracy: 79.3%
Minibatch loss at step 4850: 1.388877
Minibatch accuracy: 50.0%
Validation accuracy: 78.6%
Minibatch loss at step 4900: 1.505564
Minibatch accuracy: 50.0%
Validation accuracy: 78.1%
Minibatch loss at step 4950: 0.628969
Minibatch accuracy: 75.0%
Validation accuracy: 76.9%
Minibatch loss at step 5000: 0.963439
Minibatch accuracy: 56.2%
Validation accuracy: 79.2%
Test accuracy: 85.4%
#+end_example



** Assignment 5
:PROPERTIES:
:header-args: :session a5py :results output
:END:

*** starter
:PROPERTIES:
:ATTACH_DIR_INHERIT: t
:END:

The goal of this assignment is to train a Word2Vec skip-gram model over
[[http://mattmahoney.net/dc/textdata][Text8]] data.

#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  # %matplotlib inline
  from __future__ import print_function
  import collections
  import math
  import numpy as np
  import os
  import random
  import tensorflow as tf
  import zipfile
  from matplotlib import pylab
  from six.moves import range
  from six.moves.urllib.request import urlretrieve
  from sklearn.manifold import TSNE
#+END_SRC

#+RESULTS:
: 
: >>> >>> ... ... ... >>> >>> >>> >>> I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.7.5 locally
: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5.0.5 locally
: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.7.5 locally
: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.7.5 locally

Download the data from the source website if necessary.

#+BEGIN_SRC python
  Url = 'http://mattmahoney.net/dc/'

  def maybe_download(filename, expected_bytes):
    """Download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
      filename, _ = urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified %s' % filename)
    else:
      print(statinfo.st_size)
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  filename = maybe_download('text8.zip', 31344016)
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... >>> Found and verified text8.zip

#+BEGIN_EXAMPLE
     Found and verified text8.zip
#+END_EXAMPLE

Read the data into a string.

#+BEGIN_SRC python
     def read_data(filename):
       """Extract the first file enclosed in a zip file as a list of words"""
       with zipfile.ZipFile(filename) as f:
         data = tf.compat.as_str(f.read(f.namelist()[0])).split()
       return data
      
     words = read_data(filename)
     print('Data size %d' % len(words))
#+END_SRC

#+RESULTS:
: 
: ... ... ... ... >>> >>> Data size 17005207

#+BEGIN_EXAMPLE
     Data size 17005207
#+END_EXAMPLE

Build the dictionary and replace rare words with UNK token.

#+BEGIN_SRC python
     vocabulary_size = 50000

     def build_dataset(words):
       count = [['UNK', -1]]
       count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
       dictionary = dict()
       for word, _ in count:
         dictionary[word] = len(dictionary)
       data = list()
       unk_count = 0
       for word in words:
         if word in dictionary:
           index = dictionary[word]
         else:
           index = 0  # dictionary['UNK']
           unk_count = unk_count + 1
         data.append(index)
       count[0][1] = unk_count
       reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) 
       return data, count, dictionary, reverse_dictionary

     data, count, dictionary, reverse_dictionary = build_dataset(words)
     print('Most common words (+UNK)', count[:5])
     print('Sample data', data[:10])
     del words  # Hint to reduce memory.
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> >>> Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
: Sample data [5242, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]

#+BEGIN_EXAMPLE
     Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
     Sample data [5243, 3083, 12, 6, 195, 2, 3136, 46, 59, 156]
#+END_EXAMPLE

Function to generate a training batch for the skip-gram model.

#+BEGIN_SRC python
  data_index = 0

  def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips <= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1 # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
      buffer.append(data[data_index])
      data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
      target = skip_window  # target label at the center of the buffer
      targets_to_avoid = [ skip_window ]
      for j in range(num_skips):
        while target in targets_to_avoid:
          target = random.randint(0, span - 1)
        targets_to_avoid.append(target)
        batch[i * num_skips + j] = buffer[skip_window]
        labels[i * num_skips + j, 0] = buffer[target]
      buffer.append(data[data_index])
      data_index = (data_index + 1) % len(data)
    return batch, labels

  print('data:', [reverse_dictionary[di] for di in data[:8]])

  for num_skips, skip_window in [(2, 1), (4, 2)]:
      data_index = 0
      batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)
      print('\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))
      print('    batch:', [reverse_dictionary[bi] for bi in batch])
      print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])
#+END_SRC

#+RESULTS:
#+begin_example

>>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']
>>> ... ... ... ... ... ... 
with num_skips = 2 and skip_window = 1:
    batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']
    labels: ['as', 'anarchism', 'a', 'originated', 'as', 'term', 'a', 'of']

with num_skips = 4 and skip_window = 2:
    batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']
    labels: ['anarchism', 'term', 'a', 'originated', 'originated', 'of', 'term', 'as']
#+end_example

#+BEGIN_EXAMPLE
     data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']

     with num_skips = 2 and skip_window = 1:
         batch: ['originated', 'originated', 'as', 'as', 'a', 'a', 'term', 'term']
         labels: ['as', 'anarchism', 'a', 'originated', 'term', 'as', 'a', 'of']

     with num_skips = 4 and skip_window = 2:
         batch: ['as', 'as', 'as', 'as', 'a', 'a', 'a', 'a']
         labels: ['anarchism', 'originated', 'term', 'a', 'as', 'of', 'originated', 'term']
#+END_EXAMPLE

Train a skip-gram model.

#+BEGIN_SRC python
  batch_size = 128
  embedding_size = 128 # Dimension of the embedding vector.
  skip_window = 1 # How many words to consider left and right.
  num_skips = 2 # How many times to reuse an input to generate a label.
  # We pick a random validation set to sample nearest neighbors. here we limit the
  # validation samples to the words that have a low numeric ID, which by
  # construction are also the most frequent. 
  valid_size = 16 # Random set of words to evaluate similarity on.
  valid_window = 100 # Only pick dev samples in the head of the distribution.
  valid_examples = np.array(random.sample(range(valid_window), valid_size))
  num_sampled = 64 # Number of negative examples to sample.

  graph = tf.Graph()

  with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)
    # Variables.
    embeddings = tf.Variable(
      tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
      tf.truncated_normal([vocabulary_size, embedding_size],
                           stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))
    # Model.
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
      tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                                 train_labels, num_sampled, vocabulary_size))
    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities 
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  num_steps = 100001

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    average_loss = 0
    for step in range(num_steps):
      batch_data, batch_labels = generate_batch(
        batch_size, num_skips, skip_window)
      feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
      _, l = session.run([optimizer, loss], feed_dict=feed_dict)
      average_loss += l
      if step % 2000 == 0:
        if step > 0:
          average_loss = average_loss / 2000
        # The average loss is an estimate of the loss over the last 2000 batches.
        print('Average loss at step %d: %f' % (step, average_loss))
        average_loss = 0
      # note that this is expensive (~20% slowdown if computed every 500 steps)
      if step % 10000 == 0:
        sim = similarity.eval()
        for i in range(valid_size):
          valid_word = reverse_dictionary[valid_examples[i]]
          top_k = 8 # number of nearest neighbors
          nearest = (-sim[i, :]).argsort()[1:top_k+1]
          log = 'Nearest to %s:' % valid_word
          for k in range(top_k):
            close_word = reverse_dictionary[nearest[k]]
            log = '%s %s,' % (log, close_word)
          print(log)
    final_embeddings = normalized_embeddings.eval()
#+END_SRC

#+RESULTS:
#+begin_example

>>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... E tensorflow/stream_executor/cuda/cuda_driver.cc:504] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: iThinkPad
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: iThinkPad
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.57.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.57  Mon Oct  3 20:37:01 PDT 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.57.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.57.0
Initialized
Average loss at step 0: 7.916452
Nearest to one: processions, were, prophecies, overlapped, perkins, biddle, sapkowski, hagi,
Nearest to during: updating, comedian, destroys, kindred, moose, subordinated, vivant, resumption,
Nearest to was: umts, cingular, dynastic, malibu, iie, pedophilia, apparitions, yoakum,
Nearest to first: jadwiga, deposing, salazar, lvares, cups, extensions, tramiel, voivodship,
Nearest to a: cnidaria, henriques, depending, sens, avesta, examined, hers, ajax,
Nearest to their: blemish, children, kanu, nonexistence, ath, frac, reload, laconia,
Nearest to th: yuma, hayek, behaviorist, unconnected, grammatically, glorantha, thicknesses, intentionality,
Nearest to so: coolant, oder, baryogenesis, interruption, ladino, gaelic, giga, dukes,
Nearest to two: visual, percival, sarti, aligned, recognizably, howstuffworks, objector, showers,
Nearest to also: hyperlinks, injection, variational, width, prey, iced, cleanse, tardis,
Nearest to known: habilis, pngimage, apply, africana, tee, ww, abdicates, tear,
Nearest to has: bud, hrothgar, stipulation, irvine, archives, mercantile, negated, softening,
Nearest to often: sns, elucidate, fijian, pinscher, brigade, equiv, causes, opposed,
Nearest to most: bandit, ground, empresses, strengthens, ideologue, run, vir, tourists,
Nearest to are: canes, cplp, macs, dagger, quests, doodle, desperate, cheap,
Nearest to were: one, razed, transpose, epithet, bourgeoisie, thule, read, academies,
Average loss at step 2000: 4.354806
Average loss at step 4000: 3.865634
Average loss at step 6000: 3.790162
Average loss at step 8000: 3.690257
Average loss at step 10000: 3.615377
Nearest to one: three, two, four, five, seven, six, eight, hurst,
Nearest to during: on, with, hester, amoeba, in, eamon, fires, destroys,
Nearest to was: is, were, has, had, by, are, be, been,
Nearest to first: deposing, ess, rove, about, lvares, salazar, gyro, cups,
Nearest to a: the, this, any, evidence, ju, mezzo, permitting, his,
Nearest to their: his, its, formerly, voltage, machinery, mor, leaning, ojibwa,
Nearest to th: behaviorist, beings, nd, shawnee, unconnected, destabilize, hayek, grammatically,
Nearest to so: baryogenesis, oder, gaelic, veritable, dukes, mohenjo, comes, menial,
Nearest to two: three, five, four, eight, seven, six, nine, zero,
Nearest to also: which, often, ballparks, xxxviii, not, sometimes, chinon, cleanse,
Nearest to known: habilis, such, present, well, faro, used, pngimage, commute,
Nearest to has: had, is, was, have, rcs, orally, carneiro, tesco,
Nearest to often: it, also, he, causes, marlowe, licensure, turbofans, winchester,
Nearest to most: apeiron, shelf, jeannette, all, vir, ideologue, georgia, bolshevik,
Nearest to are: were, is, have, was, doodle, but, spectator, feast,
Nearest to were: are, was, been, had, is, haydn, read, be,
Average loss at step 12000: 3.600304
Average loss at step 14000: 3.571341
Average loss at step 16000: 3.407872
Average loss at step 18000: 3.459439
Average loss at step 20000: 3.538420
Nearest to one: six, three, two, eight, four, seven, five, nine,
Nearest to during: on, with, in, when, after, for, amoeba, at,
Nearest to was: is, were, has, had, became, been, be, are,
Nearest to first: second, ess, deposing, old, rove, same, keillor, by,
Nearest to a: the, mezzo, this, any, another, ju, learnt, amulets,
Nearest to their: its, his, the, her, them, gorge, any, leaning,
Nearest to th: nd, behaviorist, beings, hayek, shawnee, destabilize, married, unconnected,
Nearest to so: baryogenesis, oder, mohenjo, menial, gaelic, look, criticizes, veritable,
Nearest to two: five, three, four, six, eight, seven, zero, nine,
Nearest to also: which, often, sometimes, never, now, not, who, karen,
Nearest to known: such, used, well, habilis, faro, highness, reluctantly, elbaradei,
Nearest to has: had, have, was, is, rcs, hertogenbosch, carneiro, orally,
Nearest to often: also, it, who, which, sometimes, galvani, localized, widely,
Nearest to most: more, some, apeiron, all, shelf, ideologue, many, jeannette,
Nearest to are: were, is, have, was, locative, sharpened, lada, influences,
Nearest to were: are, was, had, by, been, have, be, is,
Average loss at step 22000: 3.504639
Average loss at step 24000: 3.490412
Average loss at step 26000: 3.482481
Average loss at step 28000: 3.478874
Average loss at step 30000: 3.505072
Nearest to one: two, four, seven, three, eight, six, five, nine,
Nearest to during: after, when, in, on, at, with, through, gilchrist,
Nearest to was: is, had, became, were, has, been, be, when,
Nearest to first: second, deposing, same, last, ess, old, best, smelling,
Nearest to a: any, the, permitting, another, intercontinental, ju, wakefulness, no,
Nearest to their: its, his, the, her, these, our, gorge, some,
Nearest to th: nd, behaviorist, hayek, tiling, shawnee, numeral, six, unconnected,
Nearest to so: baryogenesis, menial, criticizes, handlebars, rahman, mohenjo, jeong, veritable,
Nearest to two: four, three, one, five, seven, six, eight, nine,
Nearest to also: often, now, never, sometimes, usually, still, which, amicable,
Nearest to known: used, well, such, described, regarded, faro, called, habilis,
Nearest to has: had, have, is, was, rcs, carneiro, orally, hertogenbosch,
Nearest to often: sometimes, also, usually, generally, widely, always, they, commonly,
Nearest to most: more, many, some, bred, legalistic, among, shelf, apeiron,
Nearest to are: were, have, is, include, eudes, these, goldfish, recessions,
Nearest to were: are, was, have, been, had, by, be, digit,
Average loss at step 32000: 3.501719
Average loss at step 34000: 3.492923
Average loss at step 36000: 3.458800
Average loss at step 38000: 3.302050
Average loss at step 40000: 3.426942
Nearest to one: seven, two, four, six, eight, three, five, nine,
Nearest to during: after, when, on, before, in, at, eros, temptations,
Nearest to was: is, became, has, were, had, been, when, be,
Nearest to first: second, last, deposing, next, rove, fated, best, chic,
Nearest to a: mezzo, the, sharpe, another, wakefulness, brainwashed, ju, learnt,
Nearest to their: its, his, her, the, them, our, these, any,
Nearest to th: nd, hayek, behaviorist, shawnee, scrimmage, tiling, unconnected, mage,
Nearest to so: baryogenesis, jeong, criticizes, mohenjo, rahman, menial, atzma, supercontinent,
Nearest to two: three, four, six, five, seven, eight, one, nine,
Nearest to also: often, never, which, usually, now, still, sometimes, typically,
Nearest to known: used, such, well, regarded, described, possible, habilis, called,
Nearest to has: had, have, was, is, having, carneiro, hauk, rcs,
Nearest to often: usually, generally, sometimes, also, still, commonly, widely, always,
Nearest to most: more, many, some, among, all, ovaries, bred, legalistic,
Nearest to are: were, have, is, but, these, while, recessions, be,
Nearest to were: are, have, was, while, been, had, but, those,
Average loss at step 42000: 3.437635
Average loss at step 44000: 3.449876
Average loss at step 46000: 3.447851
Average loss at step 48000: 3.348519
Average loss at step 50000: 3.385307
Nearest to one: two, six, eight, four, seven, three, five, nine,
Nearest to during: after, when, in, before, at, through, gilchrist, from,
Nearest to was: is, were, has, had, became, been, be, are,
Nearest to first: second, last, next, under, same, deposing, balancing, mutagenesis,
Nearest to a: another, wakefulness, any, the, tocantins, mezzo, fl, permitting,
Nearest to their: its, his, the, her, these, your, insurgencies, our,
Nearest to th: nd, hayek, six, bce, one, behaviorist, abdel, noisy,
Nearest to so: baryogenesis, vasily, banana, then, meditation, mohenjo, jeong, note,
Nearest to two: three, six, four, one, five, eight, seven, zero,
Nearest to also: often, now, never, which, still, typically, usually, sometimes,
Nearest to known: used, such, well, described, regarded, possible, called, defined,
Nearest to has: had, have, was, is, having, orally, substantial, raza,
Nearest to often: usually, generally, sometimes, commonly, also, still, widely, now,
Nearest to most: more, many, some, ovaries, among, assignments, less, all,
Nearest to are: were, is, have, these, be, sharpened, was, those,
Nearest to were: are, was, have, be, had, been, avalon, those,
Average loss at step 52000: 3.436014
Average loss at step 54000: 3.428417
Average loss at step 56000: 3.439921
Average loss at step 58000: 3.400723
Average loss at step 60000: 3.394617
Nearest to one: seven, four, two, eight, five, six, nine, three,
Nearest to during: after, before, in, when, despite, although, under, through,
Nearest to was: is, became, had, were, has, be, when, been,
Nearest to first: second, last, same, next, best, amalthea, final, polytechnical,
Nearest to a: another, the, any, wakefulness, mezzo, this, odysseus, amulets,
Nearest to their: its, his, her, the, these, your, some, insurgencies,
Nearest to th: nd, eight, hayek, bce, six, abdel, bc, behaviorist,
Nearest to so: baryogenesis, note, then, meditation, jihad, discusses, concluded, banana,
Nearest to two: four, three, five, six, seven, eight, one, zero,
Nearest to also: now, often, still, never, usually, typically, sometimes, generally,
Nearest to known: used, well, such, possible, described, regarded, called, defined,
Nearest to has: had, have, is, was, having, substantial, guglielmo, contains,
Nearest to often: sometimes, usually, generally, commonly, also, now, frequently, still,
Nearest to most: more, many, some, among, all, ovaries, defections, essendon,
Nearest to are: were, is, have, eudes, be, include, including, recessions,
Nearest to were: are, was, have, had, been, although, while, those,
Average loss at step 62000: 3.243951
Average loss at step 64000: 3.255204
Average loss at step 66000: 3.400393
Average loss at step 68000: 3.395339
Average loss at step 70000: 3.362295
Nearest to one: two, seven, six, eight, four, five, nine, three,
Nearest to during: after, in, before, despite, throughout, when, until, through,
Nearest to was: is, has, were, had, been, became, be, when,
Nearest to first: second, last, next, same, best, earliest, final, original,
Nearest to a: another, the, any, this, mezzo, homeowners, hd, quite,
Nearest to their: its, his, her, the, your, our, these, some,
Nearest to th: nd, bce, hayek, bc, one, behaviorist, abdel, eighth,
Nearest to so: baryogenesis, thus, when, then, crematoria, ishiro, jihad, banana,
Nearest to two: three, four, six, seven, one, five, eight, zero,
Nearest to also: now, still, often, which, never, typically, usually, generally,
Nearest to known: used, such, regarded, defined, described, well, called, seen,
Nearest to has: had, have, is, was, since, substantial, having, contains,
Nearest to often: usually, sometimes, generally, commonly, now, frequently, also, still,
Nearest to most: more, many, some, all, among, legalistic, less, hiberno,
Nearest to are: were, is, have, include, these, while, be, including,
Nearest to were: are, was, have, had, be, been, although, those,
Average loss at step 72000: 3.378108
Average loss at step 74000: 3.350002
Average loss at step 76000: 3.309625
Average loss at step 78000: 3.348543
Average loss at step 80000: 3.376354
Nearest to one: seven, six, two, three, four, five, eight, nine,
Nearest to during: after, before, when, in, despite, throughout, although, within,
Nearest to was: is, became, were, had, has, been, although, becomes,
Nearest to first: second, last, next, same, earliest, final, third, fourth,
Nearest to a: another, the, mezzo, every, polycarbonate, any, wakefulness, violet,
Nearest to their: its, his, her, your, our, the, these, insurgencies,
Nearest to th: nd, bc, bce, hayek, six, rd, paasikivi, cutter,
Nearest to so: baryogenesis, when, banana, then, concluded, meditation, leveraged, crematoria,
Nearest to two: three, six, four, five, seven, one, eight, zero,
Nearest to also: now, often, still, typically, sometimes, which, never, actually,
Nearest to known: used, regarded, called, described, such, defined, possible, serve,
Nearest to has: had, have, is, was, since, having, contains, includes,
Nearest to often: sometimes, usually, generally, commonly, frequently, still, now, typically,
Nearest to most: more, some, many, ovaries, among, katha, especially, legalistic,
Nearest to are: were, have, those, is, include, although, fsln, be,
Nearest to were: are, was, have, had, those, being, been, these,
Average loss at step 82000: 3.409099
Average loss at step 84000: 3.409469
Average loss at step 86000: 3.389135
Average loss at step 88000: 3.351981
Average loss at step 90000: 3.367400
Nearest to one: seven, two, four, five, eight, six, three, th,
Nearest to during: after, before, in, despite, while, throughout, when, within,
Nearest to was: is, had, were, became, be, been, has, being,
Nearest to first: second, last, earliest, next, original, fourth, third, same,
Nearest to a: another, any, mezzo, hd, wakefulness, every, amulets, the,
Nearest to their: its, his, her, your, our, the, heading, whose,
Nearest to th: nd, seven, bce, one, bc, eighth, six, paasikivi,
Nearest to so: thus, baryogenesis, when, discusses, banana, ishiro, jihad, phallus,
Nearest to two: three, four, six, five, seven, one, eight, zero,
Nearest to also: now, often, still, which, never, typically, sometimes, actually,
Nearest to known: used, regarded, described, such, called, possible, defined, seen,
Nearest to has: had, have, is, having, was, since, contains, includes,
Nearest to often: sometimes, usually, generally, commonly, frequently, now, typically, still,
Nearest to most: more, some, many, among, less, ovaries, especially, anchorage,
Nearest to are: were, is, these, include, have, contain, those, although,
Nearest to were: are, was, had, while, have, although, those, these,
Average loss at step 92000: 3.396319
Average loss at step 94000: 3.250602
Average loss at step 96000: 3.355531
Average loss at step 98000: 3.241937
Average loss at step 100000: 3.360208
Nearest to one: two, four, seven, six, eight, five, three, nine,
Nearest to during: after, in, before, throughout, within, despite, when, until,
Nearest to was: is, became, has, had, been, were, remains, koalas,
Nearest to first: second, last, next, third, fourth, best, final, same,
Nearest to a: another, any, the, mezzo, virility, unfriendly, harford, wakefulness,
Nearest to their: its, his, her, your, our, the, these, whose,
Nearest to th: nd, bc, rd, eighth, bce, paasikivi, twentieth, sainte,
Nearest to so: thus, then, when, baryogenesis, crematoria, ishiro, rosser, sometimes,
Nearest to two: three, four, five, six, seven, one, eight, zero,
Nearest to also: now, still, never, often, typically, sometimes, actually, generally,
Nearest to known: used, such, regarded, described, possible, defined, seen, identified,
Nearest to has: had, have, is, was, having, since, contains, recently,
Nearest to often: usually, sometimes, generally, commonly, typically, frequently, still, now,
Nearest to most: more, less, especially, many, ovaries, some, among, all,
Nearest to are: were, have, is, include, although, those, these, be,
Nearest to were: are, have, those, these, was, had, include, although,
#+end_example

#+BEGIN_EXAMPLE
     Initialized
     Average loss at step 0 : 8.58149623871
     Nearest to been: unfavourably, marmara, ancestral, legal, bogart, glossaries, worst, rooms,
     Nearest to time: conformist, strawberries, sindhi, waterfall, xia, nominates, psp, sensitivity,
     Nearest to over: overlord, panda, golden, semigroup, rawlings, involved, shreveport, handling,
     Nearest to not: hymenoptera, reintroducing, lamiaceae, because, davao, omnipotent, combustion, debilitating,
     Nearest to three: catalog, koza, gn, braque, holstein, postgresql, luddite, justine,
     Nearest to if: chilled, vince, fiddler, represented, sandinistas, happiness, lya, glands,
     Nearest to there: coast, photosynthetic, kimmei, legally, inner, illyricum, formats, fullmetal,
     Nearest to between: chuvash, prinz, suitability, wolfe, guideline, computability, diminutive, paulo,
     Nearest to from: tanganyika, workshop, elphinstone, spearhead, resurrected, kevlar, shangri, loves,
     Nearest to state: sextus, wuppertal, glaring, inches, unrounded, courageous, adler, connie,
     Nearest to on: gino, phocas, rhine, jg, macrocosm, jackass, jays, theorie,
     Nearest to and: standings, towed, reyes, willard, equality, juggling, wladislaus, faked,
     Nearest to eight: gresham, dogg, moko, tennis, superseded, telegraphy, scramble, vinod,
     Nearest to they: prisons, divisor, coder, ribeira, willingness, factional, nne, lotta,
     Nearest to more: blues, fur, sterling, tangier, khwarizmi, discouraged, cal, deicide,
     Nearest to other: enemies, bogged, brassicaceae, lascaux, dispense, alexandrians, crimea, dou,
     Average loss at step 2000 : 4.39983723116
     Average loss at step 4000 : 3.86921076906
     Average loss at step 6000 : 3.72542127335
     Average loss at step 8000 : 3.57835536212
     Average loss at step 10000 : 3.61056993055
     Nearest to been: glossaries, legal, unfavourably, be, hadad, wore, scarcity, were,
     Nearest to time: strawberries, conformist, gleichschaltung, waterfall, molality, nominates, baal, dole,
     Nearest to over: golden, semigroup, catus, motorways, brick, shehri, mussolini, overlord,
     Nearest to not: hinayana, it, often, they, boots, also, noaa, lindsey,
     Nearest to three: four, seven, six, five, nine, eight, two, zero,
     Nearest to if: glands, euros, wallpaper, redefine, toho, confuse, unsound, shepherd,
     Nearest to there: it, they, fullmetal, pace, legally, harpsichord, mma, bug,
     Nearest to between: chuvash, wandering, from, kirsch, pursuant, eurocents, suitability, jackie,
     Nearest to from: into, in, workshop, to, at, misogynist, elphinstone, spearhead,
     Nearest to state: sextus, glaring, connie, adler, esoteric, didactic, handedness, presidents,
     Nearest to on: in, at, for, ruminants, wakefulness, torrey, foley, gino,
     Nearest to and: or, who, but, zelda, of, for, thirst, chisel,
     Nearest to eight: nine, six, seven, five, four, three, zero, two,
     Nearest to they: he, prisons, there, we, hydrate, it, not, cumbersome,
     Nearest to more: skye, blues, trypomastigotes, deicide, most, readable, used, sterling,
     Nearest to other: trochaic, hush, surveyors, joachim, differentiation, attackers, reverence, attestation,
     Average loss at step 12000 : 3.66169466591
     Average loss at step 14000 : 3.60342905837
     Average loss at step 16000 : 3.57761328053
     Average loss at step 18000 : 3.57667332476
     Average loss at step 20000 : 3.53310145146
     Nearest to been: be, become, was, hadad, unfavourably, were, wore, partido,
     Nearest to time: gleichschaltung, strawberries, year, nominates, conformist, etch, admittedly, treasuries,
     Nearest to over: golden, semigroup, motorways, rawlings, triangle, trey, ustawa, mattingly,
     Nearest to not: they, boots, often, dieppe, still, hinayana, nearly, be,
     Nearest to three: two, four, five, seven, eight, six, nine, one,
     Nearest to if: wallpaper, euros, before, toho, unsound, so, bg, pfc,
     Nearest to there: they, it, he, usually, which, we, not, transactions,
     Nearest to between: from, with, about, near, reactance, eurocents, wandering, voltaire,
     Nearest to from: into, workshop, by, between, in, on, elphinstone, under,
     Nearest to state: glaring, esoteric, succeeding, sextus, vorarlberg, presidents, depends, connie,
     Nearest to on: in, at, upon, during, from, janis, foley, nubian,
     Nearest to and: or, thirst, but, where, s, who, pfaff, including,
     Nearest to eight: nine, seven, six, five, four, three, zero, one,
     Nearest to they: there, he, we, not, it, you, prisons, who,
     Nearest to more: less, most, deicide, skye, trypomastigotes, interventionism, toed, drummond,
     Nearest to other: such, joachim, hush, attackers, surveyors, trochaic, differentiation, reverence,
     Average loss at step 22000 : 3.59519316927
     Average loss at step 24000 : 3.55378576797
     Average loss at step 26000 : 3.56455037558
     Average loss at step 28000 : 3.5040882225
     Average loss at step 30000 : 3.39208897972
     Nearest to been: become, be, were, was, spotless, hadad, by, hausdorff,
     Nearest to time: gleichschaltung, year, day, nominates, jesus, strawberries, way, admittedly,
     Nearest to over: golden, semigroup, motorways, rawlings, interventionism, counternarcotics, adaption, brick,
     Nearest to not: often, they, it, never, still, nor, boots, pki,
     Nearest to three: four, six, two, eight, five, seven, nine, zero,
     Nearest to if: when, before, so, should, toho, where, bg, wallpaper,
     Nearest to there: they, it, which, usually, he, that, also, now,
     Nearest to between: with, from, in, panasonic, presupposes, churchmen, hijacking, where,
     Nearest to from: into, elphinstone, workshop, between, through, speculates, sosa, in,
     Nearest to state: esoteric, glaring, presidents, vorarlberg, atmosphere, succeeding, lute, connie,
     Nearest to on: upon, in, janis, during, torrey, against, infield, catalans,
     Nearest to and: or, thirst, in, but, of, sobib, cleaves, including,
     Nearest to eight: nine, six, four, seven, three, zero, five, one,
     Nearest to they: we, there, he, you, it, these, who, i,
     Nearest to more: less, most, deicide, faster, toed, very, skye, tonic,
     Nearest to other: different, attackers, joachim, various, such, many, differentiation, these,
     Average loss at step 32000 : 3.49501452419
     Average loss at step 34000 : 3.48593705952
     Average loss at step 36000 : 3.50112806576
     Average loss at step 38000 : 3.49244426501
     Average loss at step 40000 : 3.3890105716
     Nearest to been: become, be, were, was, jolie, hausdorff, spotless, had,
     Nearest to time: year, way, gleichschaltung, period, day, stanislav, stage, outcome,
     Nearest to over: through, semigroup, rawlings, golden, about, brick, on, motorways,
     Nearest to not: they, radiated, never, pki, still, omnipotent, hinayana, really,
     Nearest to three: four, six, five, two, seven, eight, one, nine,
     Nearest to if: when, before, where, then, bg, because, can, should,
     Nearest to there: they, it, he, usually, this, typically, still, often,
     Nearest to between: with, in, from, about, against, churchmen, johansen, presupposes,
     Nearest to from: into, through, elphinstone, in, workshop, between, suing, under,
     Nearest to state: esoteric, presidents, atmosphere, vorarlberg, lute, succeeding, glaring, didactic,
     Nearest to on: upon, at, in, during, unitarians, under, catalans, batavians,
     Nearest to and: or, but, s, incapacitation, including, while, of, which,
     Nearest to eight: nine, six, seven, four, five, three, one, two,
     Nearest to they: we, he, there, you, she, i, not, it,
     Nearest to more: less, most, deicide, toed, greater, faster, quite, longer,
     Nearest to other: various, different, attackers, joachim, clutter, nz, trochaic, apulia,
     Average loss at step 42000 : 3.45294014364
     Average loss at step 44000 : 3.47660055941
     Average loss at step 46000 : 3.47458503014
     Average loss at step 48000 : 3.47261548793
     Average loss at step 50000 : 3.45390708435
     Nearest to been: become, be, had, was, were, hausdorff, prem, remained,
     Nearest to time: way, year, period, stv, day, gleichschaltung, stage, outcome,
     Nearest to over: through, golden, semigroup, about, brick, counternarcotics, theremin, mattingly,
     Nearest to not: they, still, never, really, sometimes, it, kiwifruit, nearly,
     Nearest to three: five, four, six, seven, two, eight, one, nine,
     Nearest to if: when, before, where, because, connexion, though, so, whether,
     Nearest to there: they, it, he, this, now, often, usually, still,
     Nearest to between: with, from, fashioned, churchmen, panasonic, explores, within, racial,
     Nearest to from: into, through, under, elphinstone, between, workshop, circumpolar, idiom,
     Nearest to state: atmosphere, vorarlberg, esoteric, presidents, madhya, majority, moulin, bowmen,
     Nearest to on: upon, in, catalans, tezuka, minotaurs, wakefulness, batavians, guglielmo,
     Nearest to and: or, but, thirst, signifier, which, however, including, unattractive,
     Nearest to eight: six, nine, seven, five, four, three, zero, two,
     Nearest to they: we, there, he, you, it, she, these, not,
     Nearest to more: less, most, quite, very, further, faster, toed, deicide,
     Nearest to other: various, different, many, attackers, are, joachim, nihilo, reject,
     Average loss at step 52000 : 3.43597227755
     Average loss at step 54000 : 3.25126817495
     Average loss at step 56000 : 3.35102432287
     Average loss at step 58000 : 3.44654818082
     Average loss at step 60000 : 3.4287913968
     Nearest to been: become, be, was, prem, had, remained, hadad, stanislavsky,
     Nearest to time: year, way, period, stv, barely, name, stage, restoring,
     Nearest to over: about, through, golden, adaption, counternarcotics, up, mattingly, brick,
     Nearest to not: still, never, nor, kiwifruit, they, nearly, therefore, rarely,
     Nearest to three: two, five, four, six, seven, eight, one, nine,
     Nearest to if: when, though, before, where, although, because, can, could,
     Nearest to there: they, it, he, still, she, we, this, often,
     Nearest to between: with, from, churchmen, among, ethical, within, vma, panasonic,
     Nearest to from: through, into, under, during, between, in, suing, across,
     Nearest to state: atmosphere, infringe, madhya, vorarlberg, government, bowmen, vargas, republic,
     Nearest to on: upon, through, within, ridiculous, janis, in, under, over,
     Nearest to and: or, while, including, but, of, like, whose, bannister,
     Nearest to eight: nine, six, five, four, seven, zero, three, two,
     Nearest to they: we, there, you, he, it, these, she, prisons,
     Nearest to more: less, most, quite, further, toed, very, faster, rather,
     Nearest to other: different, various, many, nihilo, these, amour, including, screenplays,
     Average loss at step 62000 : 3.38358767056
     Average loss at step 64000 : 3.41693099326
     Average loss at step 66000 : 3.39588000977
     Average loss at step 68000 : 3.35567189544
     Average loss at step 70000 : 3.38878934443
     Nearest to been: become, be, was, prem, remained, were, being, discounts,
     Nearest to time: year, way, day, period, barely, ethos, stage, reason,
     Nearest to over: about, through, fortunately, semigroup, theremin, off, loudest, up,
     Nearest to not: still, nor, never, they, actually, nearly, unelected, therefore,
     Nearest to three: five, two, four, six, seven, eight, nine, zero,
     Nearest to if: when, though, before, where, because, then, after, since,
     Nearest to there: they, it, he, often, she, we, usually, still,
     Nearest to between: among, with, within, from, ethical, churchmen, racial, prentice,
     Nearest to from: through, into, within, during, under, until, between, across,
     Nearest to state: city, atmosphere, desks, surrounding, preservation, bohr, principal, republic,
     Nearest to on: upon, tezuka, through, within, wakefulness, catalans, at, ingeborg,
     Nearest to and: or, but, while, including, thirst, jerzy, massing, abadan,
     Nearest to eight: seven, six, nine, five, four, three, two, zero,
     Nearest to they: we, you, he, there, she, it, prisons, who,
     Nearest to more: less, most, quite, very, faster, smaller, further, larger,
     Nearest to other: various, different, some, screenplays, lab, many, including, debugging,
     Average loss at step 72000 : 3.41103189731
     Average loss at step 74000 : 3.44926435578
     Average loss at step 76000 : 3.4423020488
     Average loss at step 78000 : 3.41976813722
     Average loss at step 80000 : 3.39511853886
     Nearest to been: become, be, remained, was, grown, were, prem, already,
     Nearest to time: year, way, period, reason, barely, distance, stage, day,
     Nearest to over: about, fortunately, through, semigroup, further, mattingly, rawlings, golden,
     Nearest to not: still, they, nor, never, we, kiwifruit, noaa, really,
     Nearest to three: five, two, seven, four, eight, six, nine, zero,
     Nearest to if: when, where, though, before, since, because, although, follows,
     Nearest to there: they, it, he, we, she, still, typically, actually,
     Nearest to between: with, among, within, in, racial, around, from, serapeum,
     Nearest to from: into, through, in, within, under, using, during, towards,
     Nearest to state: city, atmosphere, ferro, vorarlberg, surrounding, republic, madhya, national,
     Nearest to on: upon, poll, in, from, tezuka, janis, through, within,
     Nearest to and: or, but, including, while, s, which, thirst, although,
     Nearest to eight: nine, seven, six, five, four, three, zero, two,
     Nearest to they: we, you, there, he, she, it, these, not,
     Nearest to more: less, most, smaller, very, faster, quite, rather, larger,
     Nearest to other: various, different, joachim, including, theos, smaller, individual, screenplays,
     Average loss at step 82000 : 3.40933967865
     Average loss at step 84000 : 3.41618054378
     Average loss at step 86000 : 3.31485116804
     Average loss at step 88000 : 3.37068593091
     Average loss at step 90000 : 3.2785516749
     Nearest to been: become, be, was, prem, remained, grown, recently, already,
     Nearest to time: year, way, period, day, barely, battle, buds, name,
     Nearest to over: through, about, fortunately, off, theremin, semigroup, extraterrestrial, mattingly,
     Nearest to not: nor, still, never, otherwise, generally, separately, gown, hydrate,
     Nearest to three: four, five, six, two, eight, seven, nine, zero,
     Nearest to if: when, where, before, though, because, since, then, while,
     Nearest to there: they, it, he, we, she, still, typically, fiorello,
     Nearest to between: with, among, within, from, churchmen, prentice, racial, panasonic,
     Nearest to from: through, into, across, during, towards, until, at, within,
     Nearest to state: bohr, city, atmosphere, ferro, bowmen, republic, retaliation, vorarlberg,
     Nearest to on: upon, in, tezuka, at, during, within, via, catalans,
     Nearest to and: or, including, but, while, like, thirst, with, schuman,
     Nearest to eight: seven, nine, six, five, four, three, zero, two,
     Nearest to they: we, there, he, you, she, it, prisons, these,
     Nearest to more: less, most, very, faster, larger, quite, smaller, better,
     Nearest to other: different, various, tamara, prosthetic, including, individual, failing, restaurants,
     Average loss at step 92000 : 3.40355363208
     Average loss at step 94000 : 3.35647508007
     Average loss at step 96000 : 3.34374570692
     Average loss at step 98000 : 3.4230104093
     Average loss at step 100000 : 3.36909827
     Nearest to been: become, be, grown, was, being, already, remained, prem,
     Nearest to time: way, year, day, period, years, days, mothersbaugh, separators,
     Nearest to over: through, about, semigroup, further, fortunately, off, into, theremin,
     Nearest to not: never, nor, still, dieppe, really, unelected, actually, now,
     Nearest to three: four, two, five, seven, six, eight, nine, zero,
     Nearest to if: when, though, where, before, is, abe, then, follows,
     Nearest to there: they, it, he, we, still, she, typically, often,
     Nearest to between: within, with, among, churchmen, around, explores, from, reactance,
     Nearest to from: into, through, within, across, in, between, using, workshop,
     Nearest to state: atmosphere, bohr, national, ferro, germ, desks, city, unpaid,
     Nearest to on: upon, in, within, tezuka, janis, batavians, about, macrocosm,
     Nearest to and: or, but, purview, thirst, sukkot, epr, including, honesty,
     Nearest to eight: seven, nine, six, four, five, three, zero, one,
     Nearest to they: we, there, you, he, she, prisons, it, these,
     Nearest to more: less, most, very, quite, faster, larger, rather, smaller,
     Nearest to other: various, different, tamara, theos, some, cope, many, others,
#+END_EXAMPLE


#+BEGIN_SRC python
  num_points = 400

  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
  two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python
  def plot(embeddings, labels):
    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'
    pylab.figure(figsize=(15,15))  # in inches
    for i, label in enumerate(labels):
      x, y = embeddings[i,:]
      pylab.scatter(x, y)
      pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',
                     ha='right', va='bottom')
    pylab.show()

  words = [reverse_dictionary[i] for i in range(1, num_points+1)]
#+END_SRC
  
#+BEGIN_SRC python
  plot(two_d_embeddings, words)
  pylab.savefig('images/python-matplot-fig.png')  
#+END_SRC
  
#+BEGIN_SRC python :results file
  'images/python-matplot-fig.png' # return filename to org-mode
#+END_SRC

[[output_14_0.png]]

--------------



*** Problem

An alternative to skip-gram is another Word2Vec model called
[[http://arxiv.org/abs/1301.3781][CBOW]] (Continuous Bag of Words). In
the CBOW model, instead of predicting a context word from a word vector,
you predict a word from the sum of all the word vectors in its context.
Implement and evaluate a CBOW model trained on the text8 dataset.

--------------


** Assignment 6
:PROPERTIES:
:header-args: :session a6py :results output
:END:
*** starter code

After training a skip-gram model in =5_word2vec.ipynb=, the goal of this
notebook is to train a LSTM character model over
[[http://mattmahoney.net/dc/textdata][Text8]] data.

#+NAME: start1
#+BEGIN_SRC python
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import os
  import numpy as np
  import random
  import string
  import tensorflow as tf
  import zipfile
  from six.moves import range
  from six.moves.urllib.request import urlretrieve
#+END_SRC

#+RESULTS: start1
: Python 3.5.2+ (default, Sep 22 2016, 12:18:14) 
: [GCC 6.2.0 20160927] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: ... ... python.el: native completion setup loaded
: >>> >>> >>> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
: I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
: I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
: I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
: I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally

#+RESULTS:
: Python 3.5.2+ (default, Sep 22 2016, 12:18:14) 
: [GCC 6.2.0 20160927] on linux
: Type "help", "copyright", "credits" or "license" for more information.
: ... python.el: native completion setup loaded
: ... >>> >>> >>> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
: I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
: I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
: I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
: I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally

#+NAME: start2
#+BEGIN_SRC python
  url = 'http://mattmahoney.net/dc/'

  def maybe_download(filename, expected_bytes):
    """Download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
      filename, _ = urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
      print('Found and verified %s' % filename)
    else:
      print(statinfo.st_size)
      raise Exception(
        'Failed to verify ' + filename + '. Can you get to it with a browser?')
    return filename

  filename = maybe_download('text8.zip', 31344016)
#+END_SRC

#+RESULTS:
: 
: >>> ... ... ... ... ... ... ... ... ... ... ... ... >>> Found and verified text8.zip


#+BEGIN_EXAMPLE
  Found and verified text8.zip
#+END_EXAMPLE

#+NAME: start3
#+BEGIN_SRC python
  def read_data(filename):
    f = zipfile.ZipFile(filename)
    for name in f.namelist():
      return tf.compat.as_str(f.read(name))
    f.close()

  text = read_data(filename)
  print('Data size %d' % len(text))
#+END_SRC

#+RESULTS:
: 
: ... ... ... ... >>> >>> Data size 100000000
#+NAME: start4
#+BEGIN_SRC python
  a = random.randint(0,len(text))
  text[a:a+1000]
#+END_SRC

#+RESULTS:
: 
: 'l level production without sacrificing the quality of a hand crafted instrument unlike most string instruments or other wind instruments for these reasons the harmonica was a success almost from the very start of production and while the center of the harmonica business has shifted from germany the output of the various harmonica manufacturers is still very high indeed major companies are now found in germany hohner once the dominant manufacturer in the world producing some two zero million harmonicas alone in one nine two zero when german manufacturing totaled over five zero million harmonicas japan suzuki tombo yamaha china huang leo shi suzuki hohner and brasil hering ironically as the demand for higher quality instruments which respond to more demanding performance techniques has increased there has been a resurgence in the world of hand crafted harmonicas which cater to those wanting the absolute best without the compromises inherent in mass manufacturing europe and north america '

#+BEGIN_EXAMPLE
  Data size 100000000
#+END_EXAMPLE

Create a small validation set.
#+NAME: start5
#+BEGIN_SRC python
  valid_size = 1000
  valid_text = text[:valid_size]
  train_text = text[valid_size:]
  train_size = len(train_text)
  print(train_size, train_text[:64])
  print(valid_size, valid_text[:64])
#+END_SRC

#+RESULTS:
: 
: >>> >>> >>> 99999000 ons anarchists advocate social relations based upon voluntary as
: 1000  anarchism originated as a term of abuse first used against earl

#+BEGIN_EXAMPLE
  99999000 ons anarchists advocate social relations based upon voluntary as
  1000  anarchism originated as a term of abuse first used against earl
#+END_EXAMPLE

Utility functions to map characters to vocabulary IDs and back.
#+NAME: start6
#+BEGIN_SRC python
  ord(string.ascii_lowercase[0])
#+END_SRC

#+RESULTS:
: 97

#+NAME: start7
#+BEGIN_SRC python
  vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '
  first_letter = ord(string.ascii_lowercase[0])

  def char2id(char):
    if char in string.ascii_lowercase:
      return ord(char) - first_letter + 1
    elif char == ' ':
      return 0
    else:
      print('Unexpected character: %s' % char)
      return 0

  def id2char(dictid):
    if dictid > 0:
      return chr(dictid + first_letter - 1)
    else:
      return ' '

  print(char2id('a'), char2id('z'), char2id(' '), char2id(''))
  print(id2char(1), id2char(26), id2char(0))
#+END_SRC

#+RESULTS:
: 
: >>> >>> ... ... ... ... ... ... ... ... >>> ... ... ... ... ... >>> Unexpected character: 
: 1 26 0 0
: a z

#+BEGIN_EXAMPLE
  1 26 0 Unexpected character: 
  0
  a z  
#+END_EXAMPLE

Function to generate a training batch for the LSTM model.

#+NAME: start8
#+BEGIN_SRC python :results output
batch_size=64
#+END_SRC

#+NAME: start9
#+BEGIN_SRC python
  batch_size=64
  num_unrollings=10
  
  class BatchGenerator(object):
    def __init__(self, text, batch_size, num_unrollings):
      self._text = text
      self._text_size = len(text)
      self._batch_size = batch_size
      self._num_unrollings = num_unrollings
      segment = self._text_size // batch_size
      self._cursor = [ offset * segment for offset in range(batch_size)]
      self._last_batch = self._next_batch()
    def _next_batch(self):
      """Generate a single batch from the current cursor position in the data."""
      batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)
      for b in range(self._batch_size):
        batch[b, char2id(self._text[self._cursor[b]])] = 1.0
        self._cursor[b] = (self._cursor[b] + 1) % self._text_size
      return batch
    def next(self):
      """Generate the next array of batches from the data. The array consists of
      the last batch of the previous array, followed by num_unrollings new ones.
      """
      batches = [self._last_batch]
      for step in range(self._num_unrollings):
        batches.append(self._next_batch())
      self._last_batch = batches[-1]
      return batches

  def characters(probabilities):
    """Turn a 1-hot encoding or a probability distribution over the possible
    characters back into its (most likely) character representation."""
    return [id2char(c) for c in np.argmax(probabilities, 1)]

  def batches2string(batches):
    """Convert a sequence of batches back into their (most likely) string
    representation."""
    s = [''] * batches[0].shape[0]
    for b in batches:
      s = [''.join(x) for x in zip(s, characters(b))]
    return s

  train_batches = BatchGenerator(train_text, batch_size, num_unrollings)
  valid_batches = BatchGenerator(valid_text, 1, 1)

  print(batches2string(train_batches.next()))
  print(batches2string(train_batches.next()))
  print(batches2string(valid_batches.next()))
  print(batches2string(valid_batches.next()))
#+END_SRC

#+RESULTS:
: 
: >>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... >>> ... ... ... ... >>> ... ... ... ... ... ... ... >>> >>> ['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']
: ['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']
: [' a']
: ['an']

#+BEGIN_EXAMPLE
     ['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']
     ['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']
     [' a']
     ['an']
#+END_EXAMPLE

#+NAME: start10
#+BEGIN_SRC python
  def logprob(predictions, labels):
    """Log-probability of the true labels in a predicted batch."""
    predictions[predictions < 1e-10] = 1e-10
    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]

  def sample_distribution(distribution):
    """Sample one element from a distribution assumed to be an array of normalized
    probabilities.
    """
    r = random.uniform(0, 1)
    s = 0
    for i in range(len(distribution)):
      s += distribution[i]
      if s >= r:
        return i
    return len(distribution) - 1

  def sample(prediction):
    """Turn a (column) prediction into 1-hot encoded samples."""
    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)
    p[0, sample_distribution(prediction[0])] = 1.0
    return p

  def random_distribution():
    """Generate a random column of probabilities."""
    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])
    return b/np.sum(b, 1)[:,None]
#+END_SRC

#+RESULTS:

Simple LSTM Model.
#+NAME: start11
#+BEGIN_SRC python
  num_nodes = 64

  graph = tf.Graph()
  with graph.as_default():
    # Parameters:
    # Input gate: input, previous output, and bias.
    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    ib = tf.Variable(tf.zeros([1, num_nodes]))
    # Forget gate: input, previous output, and bias.
    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    fb = tf.Variable(tf.zeros([1, num_nodes]))
    # Memory cell: input, state and bias.                             
    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    cb = tf.Variable(tf.zeros([1, num_nodes]))
    # Output gate: input, previous output, and bias.
    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    ob = tf.Variable(tf.zeros([1, num_nodes]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))
    b = tf.Variable(tf.zeros([vocabulary_size]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates."""
      input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)
      forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)
      update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))
    train_inputs = train_data[:num_unrollings]
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:
      output, state = lstm_cell(i, output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output),
                                  saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits, tf.concat(0, train_labels)))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.1, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    sample_output, sample_state = lstm_cell(
      sample_input, saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:

#+NAME: start12
#+BEGIN_SRC python
  num_steps = 7001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = characters(feed)[0]
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += characters(feed)[0]
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+BEGIN_EXAMPLE
     Initialized
     Average loss at step 0 : 3.29904174805 learning rate: 10.0
     Minibatch perplexity: 27.09
     ================================================================================
     srk dwmrnuldtbbgg tapootidtu xsciu sgokeguw hi ieicjq lq piaxhazvc s fht wjcvdlh
     lhrvallvbeqqquc dxd y siqvnle bzlyw nr rwhkalezo siie o deb e lpdg  storq u nx o
     meieu nantiouie gdys qiuotblci loc hbiznauiccb cqzed acw l tsm adqxplku gn oaxet
     unvaouc oxchywdsjntdh zpklaejvxitsokeerloemee htphisb th eaeqseibumh aeeyj j orw
     ogmnictpycb whtup   otnilnesxaedtekiosqet  liwqarysmt  arj flioiibtqekycbrrgoysj
     ================================================================================
     Validation set perplexity: 19.99
     Average loss at step 100 : 2.59553678274 learning rate: 10.0
     Minibatch perplexity: 9.57
     Validation set perplexity: 10.60
     Average loss at step 200 : 2.24747137785 learning rate: 10.0
     Minibatch perplexity: 7.68
     Validation set perplexity: 8.84
     Average loss at step 300 : 2.09438110709 learning rate: 10.0
     Minibatch perplexity: 7.41
     Validation set perplexity: 8.13
     Average loss at step 400 : 1.99440989017 learning rate: 10.0
     Minibatch perplexity: 6.46
     Validation set perplexity: 7.58
     Average loss at step 500 : 1.9320810616 learning rate: 10.0
     Minibatch perplexity: 6.30
     Validation set perplexity: 6.88
     Average loss at step 600 : 1.90935629249 learning rate: 10.0
     Minibatch perplexity: 7.21
     Validation set perplexity: 6.91
     Average loss at step 700 : 1.85583009005 learning rate: 10.0
     Minibatch perplexity: 6.13
     Validation set perplexity: 6.60
     Average loss at step 800 : 1.82152368546 learning rate: 10.0
     Minibatch perplexity: 6.01
     Validation set perplexity: 6.37
     Average loss at step 900 : 1.83169809818 learning rate: 10.0
     Minibatch perplexity: 7.20
     Validation set perplexity: 6.23
     Average loss at step 1000 : 1.82217029214 learning rate: 10.0
     Minibatch perplexity: 6.73
     ================================================================================
     le action b of the tert sy ofter selvorang previgned stischdy yocal chary the co
     le relganis networks partucy cetinning wilnchan sics rumeding a fulch laks oftes
     hian andoris ret the ecause bistory l pidect one eight five lack du that the ses
     aiv dromery buskocy becomer worils resism disele retery exterrationn of hide in 
     mer miter y sught esfectur of the upission vain is werms is vul ugher compted by
     ================================================================================
     Validation set perplexity: 6.07
     Average loss at step 1100 : 1.77301145077 learning rate: 10.0
     Minibatch perplexity: 6.03
     Validation set perplexity: 5.89
     Average loss at step 1200 : 1.75306463003 learning rate: 10.0
     Minibatch perplexity: 6.50
     Validation set perplexity: 5.61
     Average loss at step 1300 : 1.72937195778 learning rate: 10.0
     Minibatch perplexity: 5.00
     Validation set perplexity: 5.60
     Average loss at step 1400 : 1.74773373723 learning rate: 10.0
     Minibatch perplexity: 6.48
     Validation set perplexity: 5.66
     Average loss at step 1500 : 1.7368799901 learning rate: 10.0
     Minibatch perplexity: 5.22
     Validation set perplexity: 5.44
     Average loss at step 1600 : 1.74528762937 learning rate: 10.0
     Minibatch perplexity: 5.85
     Validation set perplexity: 5.33
     Average loss at step 1700 : 1.70881183743 learning rate: 10.0
     Minibatch perplexity: 5.33
     Validation set perplexity: 5.56
     Average loss at step 1800 : 1.67776108027 learning rate: 10.0
     Minibatch perplexity: 5.33
     Validation set perplexity: 5.29
     Average loss at step 1900 : 1.64935536742 learning rate: 10.0
     Minibatch perplexity: 5.29
     Validation set perplexity: 5.15
     Average loss at step 2000 : 1.69528644681 learning rate: 10.0
     Minibatch perplexity: 5.13
     ================================================================================
     vers soqually have one five landwing to docial page kagan lower with ther batern
     ctor son alfortmandd tethre k skin the known purated to prooust caraying the fit
     je in beverb is the sournction bainedy wesce tu sture artualle lines digra forme
     m rousively haldio ourso ond anvary was for the seven solies hild buil  s  to te
     zall for is it is one nine eight eight one neval to the kime typer oene where he
     ================================================================================
     Validation set perplexity: 5.25
     Average loss at step 2100 : 1.68808053017 learning rate: 10.0
     Minibatch perplexity: 5.17
     Validation set perplexity: 5.01
     Average loss at step 2200 : 1.68322490931 learning rate: 10.0
     Minibatch perplexity: 5.09
     Validation set perplexity: 5.15
     Average loss at step 2300 : 1.64465074301 learning rate: 10.0
     Minibatch perplexity: 5.51
     Validation set perplexity: 5.00
     Average loss at step 2400 : 1.66408578038 learning rate: 10.0
     Minibatch perplexity: 5.86
     Validation set perplexity: 4.80
     Average loss at step 2500 : 1.68515402555 learning rate: 10.0
     Minibatch perplexity: 5.75
     Validation set perplexity: 4.82
     Average loss at step 2600 : 1.65405208349 learning rate: 10.0
     Minibatch perplexity: 5.38
     Validation set perplexity: 4.85
     Average loss at step 2700 : 1.65706222177 learning rate: 10.0
     Minibatch perplexity: 5.46
     Validation set perplexity: 4.78
     Average loss at step 2800 : 1.65204829812 learning rate: 10.0
     Minibatch perplexity: 5.06
     Validation set perplexity: 4.64
     Average loss at step 2900 : 1.65107253551 learning rate: 10.0
     Minibatch perplexity: 5.00
     Validation set perplexity: 4.61
     Average loss at step 3000 : 1.6495274055 learning rate: 10.0
     Minibatch perplexity: 4.53
     ================================================================================
     ject covered in belo one six six to finsh that all di rozial sime it a the lapse
     ble which the pullic bocades record r to sile dric two one four nine seven six f
      originally ame the playa ishaps the stotchational in a p dstambly name which as
     ore volum to bay riwer foreal in nuily operety can and auscham frooripm however 
     kan traogey was lacous revision the mott coupofiteditey the trando insended frop
     ================================================================================
     Validation set perplexity: 4.76
     Average loss at step 3100 : 1.63705502152 learning rate: 10.0
     Minibatch perplexity: 5.50
     Validation set perplexity: 4.76
     Average loss at step 3200 : 1.64740695596 learning rate: 10.0
     Minibatch perplexity: 4.84
     Validation set perplexity: 4.67
     Average loss at step 3300 : 1.64711504817 learning rate: 10.0
     Minibatch perplexity: 5.39
     Validation set perplexity: 4.57
     Average loss at step 3400 : 1.67113256454 learning rate: 10.0
     Minibatch perplexity: 5.56
     Validation set perplexity: 4.71
     Average loss at step 3500 : 1.65637169957 learning rate: 10.0
     Minibatch perplexity: 5.03
     Validation set perplexity: 4.80
     Average loss at step 3600 : 1.66601825476 learning rate: 10.0
     Minibatch perplexity: 4.63
     Validation set perplexity: 4.52
     Average loss at step 3700 : 1.65021387935 learning rate: 10.0
     Minibatch perplexity: 5.50
     Validation set perplexity: 4.56
     Average loss at step 3800 : 1.64481814981 learning rate: 10.0
     Minibatch perplexity: 4.60
     Validation set perplexity: 4.54
     Average loss at step 3900 : 1.642069453 learning rate: 10.0
     Minibatch perplexity: 4.91
     Validation set perplexity: 4.54
     Average loss at step 4000 : 1.65179730773 learning rate: 10.0
     Minibatch perplexity: 4.77
     ================================================================================
     k s rasbonish roctes the nignese at heacle was sito of beho anarchys and with ro
     jusar two sue wletaus of chistical in causations d ow trancic bruthing ha laters
     de and speacy pulted yoftret worksy zeatlating to eight d had to ie bue seven si
     s fiction of the feelly constive suq flanch earlied curauking bjoventation agent
     quen s playing it calana our seopity also atbellisionaly comexing the revideve i
     ================================================================================
     Validation set perplexity: 4.58
     Average loss at step 4100 : 1.63794238806 learning rate: 10.0
     Minibatch perplexity: 5.47
     Validation set perplexity: 4.79
     Average loss at step 4200 : 1.63822438836 learning rate: 10.0
     Minibatch perplexity: 5.30
     Validation set perplexity: 4.54
     Average loss at step 4300 : 1.61844664574 learning rate: 10.0
     Minibatch perplexity: 4.69
     Validation set perplexity: 4.54
     Average loss at step 4400 : 1.61255454302 learning rate: 10.0
     Minibatch perplexity: 4.67
     Validation set perplexity: 4.54
     Average loss at step 4500 : 1.61543365479 learning rate: 10.0
     Minibatch perplexity: 4.83
     Validation set perplexity: 4.69
     Average loss at step 4600 : 1.61607327104 learning rate: 10.0
     Minibatch perplexity: 5.18
     Validation set perplexity: 4.64
     Average loss at step 4700 : 1.62757282495 learning rate: 10.0
     Minibatch perplexity: 4.24
     Validation set perplexity: 4.66
     Average loss at step 4800 : 1.63222063541 learning rate: 10.0
     Minibatch perplexity: 5.30
     Validation set perplexity: 4.53
     Average loss at step 4900 : 1.63678096652 learning rate: 10.0
     Minibatch perplexity: 5.43
     Validation set perplexity: 4.64
     Average loss at step 5000 : 1.610340662 learning rate: 1.0
     Minibatch perplexity: 5.10
     ================================================================================
     in b one onarbs revieds the kimiluge that fondhtic fnoto cre one nine zero zero 
      of is it of marking panzia t had wap ironicaghni relly deah the omber b h menba
     ong messified it his the likdings ara subpore the a fames distaled self this int
     y advante authors the end languarle meit common tacing bevolitione and eight one
     zes that materly difild inllaring the fusts not panition assertian causecist bas
     ================================================================================
     Validation set perplexity: 4.69
     Average loss at step 5100 : 1.60593637228 learning rate: 1.0
     Minibatch perplexity: 4.69
     Validation set perplexity: 4.47
     Average loss at step 5200 : 1.58993269444 learning rate: 1.0
     Minibatch perplexity: 4.65
     Validation set perplexity: 4.39
     Average loss at step 5300 : 1.57930587292 learning rate: 1.0
     Minibatch perplexity: 5.11
     Validation set perplexity: 4.39
     Average loss at step 5400 : 1.58022856832 learning rate: 1.0
     Minibatch perplexity: 5.19
     Validation set perplexity: 4.37
     Average loss at step 5500 : 1.56654450059 learning rate: 1.0
     Minibatch perplexity: 4.69
     Validation set perplexity: 4.33
     Average loss at step 5600 : 1.58013380885 learning rate: 1.0
     Minibatch perplexity: 5.13
     Validation set perplexity: 4.35
     Average loss at step 5700 : 1.56974959254 learning rate: 1.0
     Minibatch perplexity: 5.00
     Validation set perplexity: 4.34
     Average loss at step 5800 : 1.5839582932 learning rate: 1.0
     Minibatch perplexity: 4.88
     Validation set perplexity: 4.31
     Average loss at step 5900 : 1.57129439116 learning rate: 1.0
     Minibatch perplexity: 4.66
     Validation set perplexity: 4.32
     Average loss at step 6000 : 1.55144061089 learning rate: 1.0
     Minibatch perplexity: 4.55
     ================================================================================
     utic clositical poopy stribe addi nixe one nine one zero zero eight zero b ha ex
     zerns b one internequiption of the secordy way anti proble akoping have fictiona
     phare united from has poporarly cities book ins sweden emperor a sass in origina
     quulk destrebinist and zeilazar and on low and by in science over country weilti
     x are holivia work missincis ons in the gages to starsle histon one icelanctrotu
     ================================================================================
     Validation set perplexity: 4.30
     Average loss at step 6100 : 1.56450940847 learning rate: 1.0
     Minibatch perplexity: 4.77
     Validation set perplexity: 4.27
     Average loss at step 6200 : 1.53433164835 learning rate: 1.0
     Minibatch perplexity: 4.77
     Validation set perplexity: 4.27
     Average loss at step 6300 : 1.54773445129 learning rate: 1.0
     Minibatch perplexity: 4.76
     Validation set perplexity: 4.25
     Average loss at step 6400 : 1.54021131516 learning rate: 1.0
     Minibatch perplexity: 4.56
     Validation set perplexity: 4.24
     Average loss at step 6500 : 1.56153374553 learning rate: 1.0
     Minibatch perplexity: 5.43
     Validation set perplexity: 4.27
     Average loss at step 6600 : 1.59556478739 learning rate: 1.0
     Minibatch perplexity: 4.92
     Validation set perplexity: 4.28
     Average loss at step 6700 : 1.58076951623 learning rate: 1.0
     Minibatch perplexity: 4.77
     Validation set perplexity: 4.30
     Average loss at step 6800 : 1.6070714438 learning rate: 1.0
     Minibatch perplexity: 4.98
     Validation set perplexity: 4.28
     Average loss at step 6900 : 1.58413293839 learning rate: 1.0
     Minibatch perplexity: 4.61
     Validation set perplexity: 4.29
     Average loss at step 7000 : 1.57905534983 learning rate: 1.0
     Minibatch perplexity: 5.08
     ================================================================================
     jague are officiencinels ored by film voon higherise haik one nine on the iffirc
     oshe provision that manned treatists on smalle bodariturmeristing the girto in s
     kis would softwenn mustapultmine truativersakys bersyim by s of confound esc bub
     ry of the using one four six blain ira mannom marencies g with fextificallise re
      one son vit even an conderouss to person romer i a lebapter at obiding are iuse
     ================================================================================
     Validation set perplexity: 4.25
#+END_EXAMPLE

--------------

#+BEGIN_SRC python :results output
' '.join([":var s"+str(i)+" = start"+str(i) for i in range(1,13)])
#+END_SRC

#+RESULTS:
: ':var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = start7 :var s8 = start8 :var s9 = start9 :var s10 = start10 :var s11 = start11 :var s12 = start12'

#+NAME: starupblks
#+BEGIN_SRC python :var s1 = start1 :var s2 = start2 :var s3 = start3 :var s4 = start4 :var s5 = start5 :var s6 = start6 :var s7 = start7 :var s8 = start8 :var s9 = start9 :var s10 = start10
#+END_SRC

#+BEGIN_SRC python :var s11 = start11 :var s12 = start12
#+END_SRC

#+RESULTS: starupblks


#+BEGIN_SRC python
num_steps
#+END_SRC

#+RESULTS:
: ' is not defined


*** Problem 1

You might have noticed that the definition of the LSTM cell involves 4
matrix multiplications with the input, and 4 matrix multiplications with
the output. Simplify the expression by using a single matrix multiply
for each, and variables that are 4 times larger.

--------------
#+BEGIN_SRC python
  num_nodes = 64

  graph = tf.Graph()
  with graph.as_default():
    # Concatanated input and output transition parameter matrices:
    vx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))
    vo = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))
    vb = tf.Variable(tf.zeros([1, num_nodes*4]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))
    b = tf.Variable(tf.zeros([vocabulary_size]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates.
      t_ are transfer """
      tx = tf.matmul(i, vx)
      to = tf.matmul(o, vo)      
      t = tx+to+vb
      input_gate = tf.sigmoid(t[:,0:num_nodes])
      forget_gate = tf.sigmoid(t[:,num_nodes:2*num_nodes])
      update = t[:,2*num_nodes:3*num_nodes]
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(t[:,3*num_nodes:4*num_nodes])
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))
    train_inputs = train_data[:num_unrollings]
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:
      output, state = lstm_cell(i, output, state)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output),
                                  saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits, tf.concat(0, train_labels)))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.1, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    sample_output, sample_state = lstm_cell(
      sample_input, saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:



#+BEGIN_SRC python
  num_steps = 100001
  summary_frequency = 100

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = characters(feed)[0]
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed})
              feed = sample(prediction)
              sentence += characters(feed)[0]
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]})
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... E tensorflow/stream_executor/cuda/cuda_driver.cc:504] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: iThinkPad
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: iThinkPad
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.57.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: """NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.57  Mon Oct  3 20:37:01 PDT 2016
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 
"""
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.57.0
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.57.0
Initialized
Average loss at step 0: 3.293891 learning rate: 10.000000
Minibatch perplexity: 26.95
================================================================================
eyeceestjccqu t ppqpffapzox otsfrl  mkenzey pszsee excx f tcqdhdwdndlyeedli  ahi
zdqeiszshe vzosmmooeyruh acpiuvcivybr xnacxa rnaxzatx uiasd l minxzxltmstqx obgh
ma dxam wezdxgigqlmguntjleqeedyr eawcntm anznogwic yz xhapo ubdairbkd  vtb iouiw
qgt oonnlokankyltdc r  uzmomdyzffsum goyqfdeuuxudn fmlpnqtn  quetkth emtbsmplsif
 n  odpqrfg zcegfmzojihoaxnh mp tl oudjtvohajfdjwmmlhfh jkcnhxttvikysfxbi xhuvis
================================================================================
Validation set perplexity: 20.24
Average loss at step 100: 2.597705 learning rate: 10.000000
Minibatch perplexity: 10.99
Validation set perplexity: 10.48
Average loss at step 200: 2.256601 learning rate: 10.000000
Minibatch perplexity: 8.77
Validation set perplexity: 8.76
Average loss at step 300: 2.106228 learning rate: 10.000000
Minibatch perplexity: 7.61
Validation set perplexity: 8.06
Average loss at step 400: 2.006132 learning rate: 10.000000
Minibatch perplexity: 7.54
Validation set perplexity: 7.85
Average loss at step 500: 1.938262 learning rate: 10.000000
Minibatch perplexity: 6.54
Validation set perplexity: 6.97
Average loss at step 600: 1.912633 learning rate: 10.000000
Minibatch perplexity: 6.29
Validation set perplexity: 6.91
Average loss at step 700: 1.860030 learning rate: 10.000000
Minibatch perplexity: 6.37
Validation set perplexity: 6.51
Average loss at step 800: 1.817129 learning rate: 10.000000
Minibatch perplexity: 5.87
Validation set perplexity: 6.30
Average loss at step 900: 1.835628 learning rate: 10.000000
Minibatch perplexity: 7.18
Validation set perplexity: 6.24
Average loss at step 1000: 1.824857 learning rate: 10.000000
Minibatch perplexity: 5.61
================================================================================
jation repultow tediedary geaturigg his is tree trame of anselucemmay contral dr
quodey andip for jebein five one nine sever sevee zero nine awill recents fici a
ey ecrece mudisu kble used and berkur of a feather they baschists fromder becnum
gu engy of chnetced of consity un pripnies zero the knose is cleboral fine two n
ces bettor wethah soute whind welv ffom nine omil whelly uncoil the count by ham
================================================================================
Validation set perplexity: 6.11
Average loss at step 1100: 1.778330 learning rate: 10.000000
Minibatch perplexity: 5.60
Validation set perplexity: 5.90
Average loss at step 1200: 1.751372 learning rate: 10.000000
Minibatch perplexity: 5.14
Validation set perplexity: 5.72
Average loss at step 1300: 1.731926 learning rate: 10.000000
Minibatch perplexity: 5.51
Validation set perplexity: 5.74
Average loss at step 1400: 1.746564 learning rate: 10.000000
Minibatch perplexity: 5.99
Validation set perplexity: 5.62
Average loss at step 1500: 1.738494 learning rate: 10.000000
Minibatch perplexity: 4.87
Validation set perplexity: 5.60
Average loss at step 1600: 1.744815 learning rate: 10.000000
Minibatch perplexity: 5.53
Validation set perplexity: 5.56
Average loss at step 1700: 1.711178 learning rate: 10.000000
Minibatch perplexity: 5.59
Validation set perplexity: 5.52
Average loss at step 1800: 1.674822 learning rate: 10.000000
Minibatch perplexity: 5.37
Validation set perplexity: 5.32
Average loss at step 1900: 1.646892 learning rate: 10.000000
Minibatch perplexity: 4.94
Validation set perplexity: 5.32
Average loss at step 2000: 1.695762 learning rate: 10.000000
Minibatch perplexity: 5.63
================================================================================
 hepp recordes assotically well asoputes and the responchine vys dinnect and wol
hards and bass kyological dimanity incomesing to adwe duby sllaybouse of p t to 
zoney both timellowingo jon vicafbul if adficark c sclud vectudal prodical prode
ysed res britted and lappo poogrust unit aboln geaths aresisting brond to there 
es linsh at the chrivixing re arseens headons oviol of indiquensall moster and n
================================================================================
Validation set perplexity: 5.36
Average loss at step 2100: 1.682861 learning rate: 10.000000
Minibatch perplexity: 4.98
Validation set perplexity: 5.17
Average loss at step 2200: 1.684806 learning rate: 10.000000
Minibatch perplexity: 6.31
Validation set perplexity: 5.12
Average loss at step 2300: 1.638204 learning rate: 10.000000
Minibatch perplexity: 4.99
Validation set perplexity: 4.87
Average loss at step 2400: 1.655831 learning rate: 10.000000
Minibatch perplexity: 4.97
Validation set perplexity: 4.83
Average loss at step 2500: 1.677037 learning rate: 10.000000
Minibatch perplexity: 5.12
Validation set perplexity: 4.80
Average loss at step 2600: 1.652734 learning rate: 10.000000
Minibatch perplexity: 5.71
Validation set perplexity: 4.70
Average loss at step 2700: 1.658727 learning rate: 10.000000
Minibatch perplexity: 4.67
Validation set perplexity: 4.77
Average loss at step 2800: 1.654959 learning rate: 10.000000
Minibatch perplexity: 5.74
Validation set perplexity: 4.76
Average loss at step 2900: 1.649255 learning rate: 10.000000
Minibatch perplexity: 5.65
Validation set perplexity: 4.72
Average loss at step 3000: 1.653353 learning rate: 10.000000
Minibatch perplexity: 4.98
================================================================================
to opeage and flymenish as a dacinela balu additions and official four including
bold extancex phaces of which the relandn oftensly the well order trincra x late
beried demdent botchelgor hole the yold jost two in foxchy and pengingly a doind
hied which wheopledin ett and canedarotork fourd uan publikate abribials haws co
ully ups a kue s pach example a theol were readist undia fornot wide it in the s
================================================================================
Validation set perplexity: 4.74
Average loss at step 3100: 1.630684 learning rate: 10.000000
Minibatch perplexity: 5.61
Validation set perplexity: 4.66
Average loss at step 3200: 1.645504 learning rate: 10.000000
Minibatch perplexity: 5.63
Validation set perplexity: 4.62
Average loss at step 3300: 1.636324 learning rate: 10.000000
Minibatch perplexity: 4.94
Validation set perplexity: 4.58
Average loss at step 3400: 1.670409 learning rate: 10.000000
Minibatch perplexity: 5.44
Validation set perplexity: 4.67
Average loss at step 3500: 1.658415 learning rate: 10.000000
Minibatch perplexity: 5.56
Validation set perplexity: 4.76
Average loss at step 3600: 1.665594 learning rate: 10.000000
Minibatch perplexity: 4.40
Validation set perplexity: 4.61
Average loss at step 3700: 1.641715 learning rate: 10.000000
Minibatch perplexity: 5.03
Validation set perplexity: 4.60
Average loss at step 3800: 1.642550 learning rate: 10.000000
Minibatch perplexity: 5.61
Validation set perplexity: 4.65
Average loss at step 3900: 1.634643 learning rate: 10.000000
Minibatch perplexity: 5.14
Validation set perplexity: 4.56
Average loss at step 4000: 1.651241 learning rate: 10.000000
Minibatch perplexity: 4.78
================================================================================
ple american had these prexising quected in does flunk to the hib consproducted 
s of in with law ma genor remossion time the mesping six and hob minisam in the 
x lawon cerial is integes contents is any tipple here generigenthy le the viel u
forms yarks of jeduing flok wondoc of each sold informuntari we been rither calb
lia the largeol been and the flews perhamular squas of phocacy ninely stade and 
================================================================================
Validation set perplexity: 4.61
Average loss at step 4100: 1.631771 learning rate: 10.000000
Minibatch perplexity: 5.44
Validation set perplexity: 4.69
Average loss at step 4200: 1.635594 learning rate: 10.000000
Minibatch perplexity: 5.15
Validation set perplexity: 4.54
Average loss at step 4300: 1.613565 learning rate: 10.000000
Minibatch perplexity: 5.14
Validation set perplexity: 4.52
Average loss at step 4400: 1.609465 learning rate: 10.000000
Minibatch perplexity: 4.87
Validation set perplexity: 4.41
Average loss at step 4500: 1.612396 learning rate: 10.000000
Minibatch perplexity: 5.05
Validation set perplexity: 4.63
Average loss at step 4600: 1.612258 learning rate: 10.000000
Minibatch perplexity: 5.07
Validation set perplexity: 4.60
Average loss at step 4700: 1.627545 learning rate: 10.000000
Minibatch perplexity: 5.23
Validation set perplexity: 4.49
Average loss at step 4800: 1.632523 learning rate: 10.000000
Minibatch perplexity: 4.48
Validation set perplexity: 4.53
Average loss at step 4900: 1.632156 learning rate: 10.000000
Minibatch perplexity: 5.20
Validation set perplexity: 4.69
Average loss at step 5000: 1.608898 learning rate: 1.000000
Minibatch perplexity: 4.38
================================================================================
unt ottom selair s ly peace three developes panding linkse slow two mather on th
 introduced primines and woll take two zero zero zero zero zero zero zero zero s
uss one zero eight nine five six six his mamove intropumeib appearail arcied int
y depremike practic dit juderish macker in pare solcowett sounte severb seven a 
ka by the was engution for the poirs officist extantly of the ullar head ancont 
================================================================================
Validation set perplexity: 4.67
Average loss at step 5100: 1.603166 learning rate: 1.000000
Minibatch perplexity: 4.83
Validation set perplexity: 4.49
Average loss at step 5200: 1.589664 learning rate: 1.000000
Minibatch perplexity: 4.62
Validation set perplexity: 4.44
Average loss at step 5300: 1.578290 learning rate: 1.000000
Minibatch perplexity: 4.72
Validation set perplexity: 4.42
Average loss at step 5400: 1.580778 learning rate: 1.000000
Minibatch perplexity: 5.07
Validation set perplexity: 4.42
Average loss at step 5500: 1.568532 learning rate: 1.000000
Minibatch perplexity: 5.07
Validation set perplexity: 4.41
Average loss at step 5600: 1.578202 learning rate: 1.000000
Minibatch perplexity: 4.84
Validation set perplexity: 4.36
Average loss at step 5700: 1.569101 learning rate: 1.000000
Minibatch perplexity: 4.56
Validation set perplexity: 4.37
Average loss at step 5800: 1.580017 learning rate: 1.000000
Minibatch perplexity: 4.79
Validation set perplexity: 4.36
Average loss at step 5900: 1.573204 learning rate: 1.000000
Minibatch perplexity: 5.16
Validation set perplexity: 4.33
Average loss at step 6000: 1.548112 learning rate: 1.000000
Minibatch perplexity: 5.03
================================================================================
n in the eftablish also spy drines in recale of which ywark the cale in physchan
ttion descinally a mill befomenish begomers can on grompler revidine of they ass
y is issusbiones corn of entiny least alough kuans oitsive one before cleater to
x los the honzonezoms jow the s is of that to u v spirch constanked embertation 
k an however their stara juphake proven actyral a this mogratificated from the s
================================================================================
Validation set perplexity: 4.33
Average loss at step 6100: 1.566685 learning rate: 1.000000
Minibatch perplexity: 5.11
Validation set perplexity: 4.30
Average loss at step 6200: 1.536117 learning rate: 1.000000
Minibatch perplexity: 4.94
Validation set perplexity: 4.30
Average loss at step 6300: 1.546283 learning rate: 1.000000
Minibatch perplexity: 5.03
Validation set perplexity: 4.29
Average loss at step 6400: 1.539890 learning rate: 1.000000
Minibatch perplexity: 4.39
Validation set perplexity: 4.30
Average loss at step 6500: 1.560552 learning rate: 1.000000
Minibatch perplexity: 4.53
Validation set perplexity: 4.30
Average loss at step 6600: 1.599117 learning rate: 1.000000
Minibatch perplexity: 4.87
Validation set perplexity: 4.27
Average loss at step 6700: 1.580694 learning rate: 1.000000
Minibatch perplexity: 5.26
Validation set perplexity: 4.29
Average loss at step 6800: 1.607488 learning rate: 1.000000
Minibatch perplexity: 4.76
Validation set perplexity: 4.29
Average loss at step 6900: 1.582167 learning rate: 1.000000
Minibatch perplexity: 4.74
Validation set perplexity: 4.32
Average loss at step 7000: 1.577565 learning rate: 1.000000
Minibatch perplexity: 4.98
================================================================================
phen can restrents thange ardsing can hilso frumm among these cathon of eavillen
x are products of adqued one nine eight seven five zero sig midouts coloctuages 
ines sussatation comentrally meer throught in x eguenc otboment it councilageds 
 strang to ressiction of syd one eight severao seven and by the freed interno ra
ur frendituria sticks and boind ca was shrail such al against henry whime rights
================================================================================
Validation set perplexity: 4.29
#+end_example

--------------



*** Problem 2

We want to train a LSTM over bigrams, that is pairs of consecutive
characters like 'ab' instead of single characters like 'a'. Since the
number of possible bigrams is large, feeding them directly to the LSTM
using 1-hot encodings will lead to a very sparse representation that is
very wasteful computationally.

a- Introduce an embedding lookup on the inputs, and feed the embeddings
to the LSTM cell instead of the inputs themselves.

b- Write a bigram-based LSTM, modeled on the character LSTM above.

c- Introduce Dropout. For best practices on how to use Dropout in LSTMs,
refer to this [[http://arxiv.org/abs/1409.2329][article]].

--------------

**** c

Simple with dropout.
#+BEGIN_SRC python
  num_nodes = 64
  graph = tf.Graph()
  with graph.as_default():
    # Parameters:
    # Input gate: input, previous output, and bias.
    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    ib = tf.Variable(tf.zeros([1, num_nodes]))
    # Forget gate: input, previous output, and bias.
    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    fb = tf.Variable(tf.zeros([1, num_nodes]))
    # Memory cell: input, state and bias.                             
    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    cb = tf.Variable(tf.zeros([1, num_nodes]))
    # Output gate: input, previous output, and bias.
    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))
    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))
    ob = tf.Variable(tf.zeros([1, num_nodes]))
    # Variables saving state across unrollings.
    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)
    # Classifier weights and biases.
    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))
    b = tf.Variable(tf.zeros([vocabulary_size]))
    # Definition of the cell computation.
    def lstm_cell(i, o, state):
      """Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf
      Note that in this formulation, we omit the various connections between the
      previous state and the gates."""
      input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)
      forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)
      update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb
      state = forget_gate * state + input_gate * tf.tanh(update)
      output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)
      return output_gate * tf.tanh(state), state
    # Input data.
    train_data = list()
    for _ in range(num_unrollings + 1):
      train_data.append(
        tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))
    train_inputs = train_data[:num_unrollings]
    train_labels = train_data[1:]  # labels are inputs shifted by one time step.
    # Unrolled LSTM loop.
    outputs = list()
    output = saved_output
    state = saved_state
    for i in train_inputs:
      output, state = lstm_cell(i, output, state)
      output = tf.nn.dropout(output, .8)
      outputs.append(output)
    # State saving across unrollings.
    with tf.control_dependencies([saved_output.assign(output),
                                  saved_state.assign(state)]):
      # Classifier.
      logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)
      loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
          logits, tf.concat(0, train_labels)))
    # Optimizer.
    global_step = tf.Variable(0)
    learning_rate = tf.train.exponential_decay(
      10.0, global_step, 5000, 0.1, staircase=True)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    gradients, v = zip(*optimizer.compute_gradients(loss))
    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)
    optimizer = optimizer.apply_gradients(
      zip(gradients, v), global_step=global_step)
    # Predictions.
    train_prediction = tf.nn.softmax(logits)
    # Sampling and validation eval: batch 1, no unrolling.
    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])
    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))
    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))
    reset_sample_state = tf.group(
      saved_sample_output.assign(tf.zeros([1, num_nodes])),
      saved_sample_state.assign(tf.zeros([1, num_nodes])))
    sample_output, sample_state = lstm_cell(
      sample_input, saved_sample_output, saved_sample_state)
    with tf.control_dependencies([saved_sample_output.assign(sample_output),
                                  saved_sample_state.assign(sample_state)]):
      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))
#+END_SRC

#+RESULTS:



#+BEGIN_SRC python
  num_steps = 7001
  summary_frequency = 100
  # pkeep = .8
  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    mean_loss = 0
    for step in range(num_steps):
      batches = train_batches.next()
      feed_dict = dict()
      # feed_dict["keep_prob"] = pkeep
      for i in range(num_unrollings + 1):
        feed_dict[train_data[i]] = batches[i]
      _, l, predictions, lr = session.run(
        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)
      mean_loss += l
      if step % summary_frequency == 0:
        if step > 0:
          mean_loss = mean_loss / summary_frequency
        # The mean loss is an estimate of the loss over the last few batches.
        print(
          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))
        mean_loss = 0
        labels = np.concatenate(list(batches)[1:])
        print('Minibatch perplexity: %.2f' % float(
          np.exp(logprob(predictions, labels))))
        if step % (summary_frequency * 10) == 0:
          # Generate some samples.
          print('=' * 80)
          for _ in range(5):
            feed = sample(random_distribution())
            sentence = characters(feed)[0]
            reset_sample_state.run()
            for _ in range(79):
              prediction = sample_prediction.eval({sample_input: feed# , 
                                                   # keep_prob: 1.
              })
              feed = sample(prediction)
              sentence += characters(feed)[0]
            print(sentence)
          print('=' * 80)
        # Measure validation set perplexity.
        reset_sample_state.run()
        valid_logprob = 0
        for _ in range(valid_size):
          b = valid_batches.next()
          predictions = sample_prediction.eval({sample_input: b[0]# , 
                                                # keep_prob: 1.
          })
          valid_logprob = valid_logprob + logprob(predictions, b[1])
        print('Validation set perplexity: %.2f' % float(np.exp(
          valid_logprob / valid_size)))
#+END_SRC



*** Problem 3

(difficult!)

Write a sequence-to-sequence LSTM which mirrors all the words in a
sentence. For example, if your input is:

#+BEGIN_EXAMPLE
     the quick brown fox
#+END_EXAMPLE

the model should attempt to output:

#+BEGIN_EXAMPLE
     eht kciuq nworb xof
#+END_EXAMPLE

Refer to the lecture on how to put together a sequence-to-sequence
model, as well as [[http://arxiv.org/abs/1409.3215][this article]] for
best practices.

--------------
