#+TITLE: UD730 A4
#+TODO: TODO IN-PROGRESS WAITING DONE
#+STARTUP: indent
#+OPTIONS: author:nil

* Deep Learning

** Assignment 4
  :PROPERTIES:
  :header-args: :session a4py
  :END:

*** notes
#+BEGIN_SRC emacs-lisp
(auto-revert-mode t)
#+END_SRC

#+RESULTS:
: t


*** Starter code

 Previously in =2_fullyconnected.ipynb= and =3_regularization.ipynb=, we
 trained fully connected networks to classify
 [[http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html][notMNIST]]
 characters.

 The goal of this assignment is make the neural network convolutional.

#+BEGIN_SRC python :session a4aspy
  # These are all the modules we'll be using later. Make sure you can import them
  # before proceeding further.
  from __future__ import print_function
  import numpy as np
  import tensorflow as tf
  from six.moves import cPickle as pickle
  from six.moves import range
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output
  pickle_file = 'notMNIST.pickle'

  with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_dataset = save['train_dataset']
    train_labels = save['train_labels']
    valid_dataset = save['valid_dataset']
    valid_labels = save['valid_labels']
    test_dataset = save['test_dataset']
    test_labels = save['test_labels']
    del save  # hint to help gc free up memory
#+END_SRC

#+RESULTS:
    
#+BEGIN_SRC python :session a4aspy :results output
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+RESULTS:
: Training set (200000, 28, 28) (200000,)
: Validation set (10000, 28, 28) (10000,)
: Test set (10000, 28, 28) (10000,)




#+BEGIN_EXAMPLE
     Training set (200000, 28, 28) (200000,)
     Validation set (10000, 28, 28) (10000,)
     Test set (18724, 28, 28) (18724,)
#+END_EXAMPLE

 Reformat into a TensorFlow-friendly shape: - convolutions need the image
 data formatted as a cube (width by height by #channels) - labels as
 float 1-hot encodings.

#+BEGIN_SRC python :session a4aspy :results output
  image_size = 28
  num_labels = 10
  num_channels = 1 # grayscale

  import numpy as np

  def reformat(dataset, labels):
    dataset = dataset.reshape(
      (-1, image_size, image_size, num_channels)).astype(np.float32)
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels
#+END_SRC

  #+RESULTS:
  
#+BEGIN_SRC python :session a4aspy :results output
  train_dataset, train_labels = reformat(train_dataset, train_labels)
  valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
  test_dataset, test_labels = reformat(test_dataset, test_labels)
#+END_SRC

#+RESULTS:
   
#+BEGIN_SRC python :session a4aspy :results output
   print('Training set', train_dataset.shape, train_labels.shape)
   print('Validation set', valid_dataset.shape, valid_labels.shape)
   print('Test set', test_dataset.shape, test_labels.shape)
#+END_SRC

#+RESULTS:
: Training set (200000, 28, 28, 1) (200000, 10)
: Validation set (10000, 28, 28, 1) (10000, 10)
: Test set (10000, 28, 28, 1) (10000, 10)



#+BEGIN_EXAMPLE
     Training set (200000, 28, 28, 1) (200000, 10)
     Validation set (10000, 28, 28, 1) (10000, 10)
     Test set (18724, 28, 28, 1) (18724, 10)
#+END_EXAMPLE

#+BEGIN_SRC python :session a4aspy :results output
  def accuracy(predictions, labels):
    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
            / predictions.shape[0])
#+END_SRC

 #+RESULTS:

 Let's build a small network with two convolutional layers, followed by
 one fully connected layer. Convolutional networks are more expensive
 computationally, so we'll limit its depth and number of fully connected
 nodes.

#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64

  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      shape = hidden.get_shape().as_list()
      reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
      hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
    # Optimizer.
    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    test_prediction = tf.nn.softmax(model(tf_test_dataset))
#+END_SRC

#+RESULTS:

#+NAME: run_graph
#+BEGIN_SRC python :var nsteps = 1001 :session a4aspy :results output
  num_steps = nsteps

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(num_steps):
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
      _, l, predictions = session.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 50 == 0):
        print('Minibatch loss at step %d: %f' % (step, l))
        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+RESULTS: run_graph
#+begin_example

>>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
Initialized
Minibatch loss at step 0: 3.242096
Minibatch accuracy: 6.2%
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.92GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
Validation accuracy: 12.5%
Minibatch loss at step 50: 1.400090
Minibatch accuracy: 31.2%
Validation accuracy: 52.0%
Minibatch loss at step 100: 0.721191
Minibatch accuracy: 75.0%
Validation accuracy: 77.8%
Minibatch loss at step 150: 0.711765
Minibatch accuracy: 81.2%
Validation accuracy: 75.3%
Minibatch loss at step 200: 0.566184
Minibatch accuracy: 93.8%
Validation accuracy: 80.5%
Minibatch loss at step 250: 0.183301
Minibatch accuracy: 93.8%
Validation accuracy: 82.0%
Minibatch loss at step 300: 0.521048
Minibatch accuracy: 81.2%
Validation accuracy: 83.2%
Minibatch loss at step 350: 0.279585
Minibatch accuracy: 93.8%
Validation accuracy: 82.2%
Minibatch loss at step 400: 0.260485
Minibatch accuracy: 93.8%
Validation accuracy: 83.2%
Minibatch loss at step 450: 0.547604
Minibatch accuracy: 81.2%
Validation accuracy: 82.9%
Minibatch loss at step 500: 0.752414
Minibatch accuracy: 75.0%
Validation accuracy: 83.3%
Minibatch loss at step 550: 0.562306
Minibatch accuracy: 81.2%
Validation accuracy: 84.0%
Minibatch loss at step 600: 0.988123
Minibatch accuracy: 75.0%
Validation accuracy: 84.3%
Minibatch loss at step 650: 0.556511
Minibatch accuracy: 81.2%
Validation accuracy: 83.9%
Minibatch loss at step 700: 0.373015
Minibatch accuracy: 87.5%
Validation accuracy: 83.7%
Minibatch loss at step 750: 0.376616
Minibatch accuracy: 87.5%
Validation accuracy: 85.2%
Minibatch loss at step 800: 0.525461
Minibatch accuracy: 81.2%
Validation accuracy: 85.9%
Minibatch loss at step 850: 0.274119
Minibatch accuracy: 93.8%
Validation accuracy: 85.7%
Minibatch loss at step 900: 0.639946
Minibatch accuracy: 75.0%
Validation accuracy: 86.3%
Minibatch loss at step 950: 0.312944
Minibatch accuracy: 87.5%
Validation accuracy: 86.0%
Minibatch loss at step 1000: 0.300270
Minibatch accuracy: 93.8%
Validation accuracy: 86.1%
Test accuracy: 73.8%
#+end_example

*** Problem 1 

The convolutional model above uses convolutions with stride 2 to reduce
the dimensionality. Replace the strides by a max pooling operation
(=nn.max_pool()=) of stride 2 and kernel size 2.

#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64

  graph = tf.Graph()

  with graph.as_default():
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      hidden = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      hidden = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = hidden.get_shape().as_list()
      reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
      hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits,tf_train_labels))
    # Optimizer.
    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    test_prediction = tf.nn.softmax(model(tf_test_dataset))
#+END_SRC

 #+RESULTS:


#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=1001) :results output

#+END_SRC

#+RESULTS:


*** Problem 2
Try to get the best performance you can using a convolutional net. Look
for example at the classic [[http://yann.lecun.com/exdb/lenet/][LeNet5]]
architecture, adding Dropout, and/or adding learning rate decay.

**** experiments
#+BEGIN_SRC python :session a4aspy :results output :var results=run_graph(nsteps=1001)
print("complete")
#+END_SRC


***** 3
regularizers
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64
  SEED = 66478
  beta = 5e-4
  graph = tf.Graph()

  with graph.as_default():
    batch = tf.Variable(0)
    rate_param = .01
    global_step = batch*batch_size
    decay_steps=len(train_dataset)  
    decay_rate=.95
    learning_rate =  tf.train.exponential_decay(
      learning_rate=rate_param, 
      global_step=global_step, 
      decay_steps=decay_steps,  
      decay_rate=decay_rate, 
      staircase=True)
    keep_prob = tf.placeholder(tf.float32)
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1, seed = SEED))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1, seed=SEED))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1, seed=SEED))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1, seed=SEED))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), keep_prob, seed=SEED)*(1/keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) + 
                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) + 
                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) + 
                    tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases))
    loss += beta*regularizers
    # Optimizer.
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :var nsteps = 6001 :session a4aspy :results output
  num_steps = nsteps

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(num_steps):
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
      _, l, predictions = session.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 50 == 0):
        print('Minibatch loss at step %d: %f' % (step, l))
        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
Initialized
Minibatch loss at step 0: 7.552904
Minibatch accuracy: 6.2%
Validation accuracy: 12.7%
Minibatch loss at step 50: 1.868548
Minibatch accuracy: 50.0%
Validation accuracy: 38.4%
Minibatch loss at step 100: 1.686023
Minibatch accuracy: 62.5%
Validation accuracy: 57.3%
Minibatch loss at step 150: 1.543465
Minibatch accuracy: 56.2%
Validation accuracy: 67.3%
Minibatch loss at step 200: 1.106614
Minibatch accuracy: 68.8%
Validation accuracy: 65.8%
Minibatch loss at step 250: 0.951548
Minibatch accuracy: 62.5%
Validation accuracy: 74.3%
Minibatch loss at step 300: 1.207042
Minibatch accuracy: 56.2%
Validation accuracy: 74.8%
Minibatch loss at step 350: 0.748152
Minibatch accuracy: 81.2%
Validation accuracy: 77.5%
Minibatch loss at step 400: 0.749116
Minibatch accuracy: 81.2%
Validation accuracy: 75.8%
Minibatch loss at step 450: 0.971119
Minibatch accuracy: 75.0%
Validation accuracy: 78.9%
Minibatch loss at step 500: 0.901104
Minibatch accuracy: 87.5%
Validation accuracy: 79.2%
Minibatch loss at step 550: 0.951675
Minibatch accuracy: 68.8%
Validation accuracy: 80.0%
Minibatch loss at step 600: 1.279480
Minibatch accuracy: 50.0%
Validation accuracy: 81.3%
Minibatch loss at step 650: 0.930019
Minibatch accuracy: 75.0%
Validation accuracy: 81.5%
Minibatch loss at step 700: 0.912175
Minibatch accuracy: 68.8%
Validation accuracy: 81.3%
Minibatch loss at step 750: 0.850602
Minibatch accuracy: 75.0%
Validation accuracy: 81.9%
Minibatch loss at step 800: 1.193454
Minibatch accuracy: 62.5%
Validation accuracy: 82.2%
Minibatch loss at step 850: 0.831148
Minibatch accuracy: 75.0%
Validation accuracy: 82.3%
Minibatch loss at step 900: 0.923164
Minibatch accuracy: 68.8%
Validation accuracy: 81.8%
Minibatch loss at step 950: 0.908230
Minibatch accuracy: 81.2%
Validation accuracy: 82.8%
Minibatch loss at step 1000: 0.454398
Minibatch accuracy: 100.0%
Validation accuracy: 82.9%
Minibatch loss at step 1050: 1.427735
Minibatch accuracy: 56.2%
Validation accuracy: 82.1%
Minibatch loss at step 1100: 0.757183
Minibatch accuracy: 81.2%
Validation accuracy: 81.8%
Minibatch loss at step 1150: 0.962808
Minibatch accuracy: 75.0%
Validation accuracy: 83.1%
Minibatch loss at step 1200: 0.966435
Minibatch accuracy: 81.2%
Validation accuracy: 83.6%
Minibatch loss at step 1250: 0.478997
Minibatch accuracy: 81.2%
Validation accuracy: 83.6%
Minibatch loss at step 1300: 1.662576
Minibatch accuracy: 56.2%
Validation accuracy: 82.9%
Minibatch loss at step 1350: 1.098325
Minibatch accuracy: 68.8%
Validation accuracy: 82.2%
Minibatch loss at step 1400: 0.804765
Minibatch accuracy: 75.0%
Validation accuracy: 83.1%
Minibatch loss at step 1450: 1.078651
Minibatch accuracy: 68.8%
Validation accuracy: 83.1%
Minibatch loss at step 1500: 0.710050
Minibatch accuracy: 81.2%
Validation accuracy: 82.7%
Minibatch loss at step 1550: 0.976368
Minibatch accuracy: 68.8%
Validation accuracy: 82.8%
Minibatch loss at step 1600: 0.337521
Minibatch accuracy: 93.8%
Validation accuracy: 83.2%
Minibatch loss at step 1650: 0.411596
Minibatch accuracy: 87.5%
Validation accuracy: 83.3%
Minibatch loss at step 1700: 0.389294
Minibatch accuracy: 93.8%
Validation accuracy: 83.5%
Minibatch loss at step 1750: 0.668555
Minibatch accuracy: 87.5%
Validation accuracy: 83.7%
Minibatch loss at step 1800: 1.027090
Minibatch accuracy: 81.2%
Validation accuracy: 83.5%
Minibatch loss at step 1850: 0.860049
Minibatch accuracy: 75.0%
Validation accuracy: 83.9%
Minibatch loss at step 1900: 0.936876
Minibatch accuracy: 75.0%
Validation accuracy: 84.3%
Minibatch loss at step 1950: 0.844571
Minibatch accuracy: 87.5%
Validation accuracy: 83.9%
Minibatch loss at step 2000: 0.877765
Minibatch accuracy: 75.0%
Validation accuracy: 83.4%
Minibatch loss at step 2050: 0.723814
Minibatch accuracy: 81.2%
Validation accuracy: 84.4%
Minibatch loss at step 2100: 0.649591
Minibatch accuracy: 81.2%
Validation accuracy: 84.3%
Minibatch loss at step 2150: 1.025875
Minibatch accuracy: 75.0%
Validation accuracy: 84.3%
Minibatch loss at step 2200: 0.674252
Minibatch accuracy: 75.0%
Validation accuracy: 84.4%
Minibatch loss at step 2250: 0.830879
Minibatch accuracy: 81.2%
Validation accuracy: 84.4%
Minibatch loss at step 2300: 0.543495
Minibatch accuracy: 87.5%
Validation accuracy: 84.7%
Minibatch loss at step 2350: 1.183975
Minibatch accuracy: 62.5%
Validation accuracy: 84.3%
Minibatch loss at step 2400: 1.248241
Minibatch accuracy: 56.2%
Validation accuracy: 84.7%
Minibatch loss at step 2450: 0.511514
Minibatch accuracy: 87.5%
Validation accuracy: 84.4%
Minibatch loss at step 2500: 0.572464
Minibatch accuracy: 87.5%
Validation accuracy: 84.5%
Minibatch loss at step 2550: 0.662893
Minibatch accuracy: 81.2%
Validation accuracy: 85.1%
Minibatch loss at step 2600: 0.606034
Minibatch accuracy: 81.2%
Validation accuracy: 84.4%
Minibatch loss at step 2650: 0.745028
Minibatch accuracy: 81.2%
Validation accuracy: 85.1%
Minibatch loss at step 2700: 0.710003
Minibatch accuracy: 81.2%
Validation accuracy: 84.5%
Minibatch loss at step 2750: 0.755669
Minibatch accuracy: 81.2%
Validation accuracy: 85.5%
Minibatch loss at step 2800: 1.292098
Minibatch accuracy: 62.5%
Validation accuracy: 85.2%
Minibatch loss at step 2850: 0.425851
Minibatch accuracy: 100.0%
Validation accuracy: 85.4%
Minibatch loss at step 2900: 0.399912
Minibatch accuracy: 93.8%
Validation accuracy: 85.5%
Minibatch loss at step 2950: 0.972225
Minibatch accuracy: 68.8%
Validation accuracy: 85.1%
Minibatch loss at step 3000: 0.970749
Minibatch accuracy: 75.0%
Validation accuracy: 85.3%
Minibatch loss at step 3050: 0.469023
Minibatch accuracy: 93.8%
Validation accuracy: 85.4%
Minibatch loss at step 3100: 0.770181
Minibatch accuracy: 81.2%
Validation accuracy: 85.7%
Minibatch loss at step 3150: 0.575410
Minibatch accuracy: 87.5%
Validation accuracy: 85.9%
Minibatch loss at step 3200: 0.608307
Minibatch accuracy: 87.5%
Validation accuracy: 85.2%
Minibatch loss at step 3250: 0.929261
Minibatch accuracy: 81.2%
Validation accuracy: 85.4%
Minibatch loss at step 3300: 0.534638
Minibatch accuracy: 87.5%
Validation accuracy: 86.1%
Minibatch loss at step 3350: 0.639192
Minibatch accuracy: 87.5%
Validation accuracy: 85.8%
Minibatch loss at step 3400: 0.533000
Minibatch accuracy: 93.8%
Validation accuracy: 85.9%
Minibatch loss at step 3450: 0.679216
Minibatch accuracy: 81.2%
Validation accuracy: 85.7%
Minibatch loss at step 3500: 0.658223
Minibatch accuracy: 81.2%
Validation accuracy: 85.7%
Minibatch loss at step 3550: 0.646314
Minibatch accuracy: 81.2%
Validation accuracy: 85.8%
Minibatch loss at step 3600: 0.284332
Minibatch accuracy: 93.8%
Validation accuracy: 85.6%
Minibatch loss at step 3650: 0.725764
Minibatch accuracy: 75.0%
Validation accuracy: 85.4%
Minibatch loss at step 3700: 0.388132
Minibatch accuracy: 87.5%
Validation accuracy: 85.3%
Minibatch loss at step 3750: 0.525476
Minibatch accuracy: 93.8%
Validation accuracy: 86.2%
Minibatch loss at step 3800: 0.366758
Minibatch accuracy: 93.8%
Validation accuracy: 86.0%
Minibatch loss at step 3850: 0.693061
Minibatch accuracy: 75.0%
Validation accuracy: 86.1%
Minibatch loss at step 3900: 0.789479
Minibatch accuracy: 75.0%
Validation accuracy: 86.1%
Minibatch loss at step 3950: 1.024155
Minibatch accuracy: 75.0%
Validation accuracy: 86.2%
Minibatch loss at step 4000: 0.731043
Minibatch accuracy: 81.2%
Validation accuracy: 86.4%
Minibatch loss at step 4050: 0.488768
Minibatch accuracy: 81.2%
Validation accuracy: 85.8%
Minibatch loss at step 4100: 0.440151
Minibatch accuracy: 87.5%
Validation accuracy: 86.4%
Minibatch loss at step 4150: 0.793504
Minibatch accuracy: 87.5%
Validation accuracy: 86.5%
Minibatch loss at step 4200: 0.245356
Minibatch accuracy: 100.0%
Validation accuracy: 86.2%
Minibatch loss at step 4250: 1.080213
Minibatch accuracy: 68.8%
Validation accuracy: 85.9%
Minibatch loss at step 4300: 0.746080
Minibatch accuracy: 75.0%
Validation accuracy: 86.3%
Minibatch loss at step 4350: 0.834907
Minibatch accuracy: 75.0%
Validation accuracy: 86.3%
Minibatch loss at step 4400: 0.622606
Minibatch accuracy: 81.2%
Validation accuracy: 86.3%
Minibatch loss at step 4450: 1.020952
Minibatch accuracy: 81.2%
Validation accuracy: 86.3%
Minibatch loss at step 4500: 0.437958
Minibatch accuracy: 87.5%
Validation accuracy: 86.4%
Minibatch loss at step 4550: 0.640179
Minibatch accuracy: 81.2%
Validation accuracy: 86.7%
Minibatch loss at step 4600: 0.499977
Minibatch accuracy: 93.8%
Validation accuracy: 86.9%
Minibatch loss at step 4650: 0.564801
Minibatch accuracy: 75.0%
Validation accuracy: 86.7%
Minibatch loss at step 4700: 0.595163
Minibatch accuracy: 81.2%
Validation accuracy: 87.0%
Minibatch loss at step 4750: 0.845680
Minibatch accuracy: 75.0%
Validation accuracy: 87.0%
Minibatch loss at step 4800: 0.803662
Minibatch accuracy: 81.2%
Validation accuracy: 86.5%
Minibatch loss at step 4850: 0.709128
Minibatch accuracy: 87.5%
Validation accuracy: 86.7%
Minibatch loss at step 4900: 1.098710
Minibatch accuracy: 68.8%
Validation accuracy: 87.0%
Minibatch loss at step 4950: 0.629900
Minibatch accuracy: 87.5%
Validation accuracy: 86.6%
Minibatch loss at step 5000: 0.339497
Minibatch accuracy: 93.8%
Validation accuracy: 86.9%
Minibatch loss at step 5050: 0.741941
Minibatch accuracy: 81.2%
Validation accuracy: 87.2%
Minibatch loss at step 5100: 0.750215
Minibatch accuracy: 75.0%
Validation accuracy: 86.6%
Minibatch loss at step 5150: 0.503329
Minibatch accuracy: 87.5%
Validation accuracy: 86.8%
Minibatch loss at step 5200: 0.392885
Minibatch accuracy: 87.5%
Validation accuracy: 87.0%
Minibatch loss at step 5250: 1.174028
Minibatch accuracy: 68.8%
Validation accuracy: 86.8%
Minibatch loss at step 5300: 0.454586
Minibatch accuracy: 81.2%
Validation accuracy: 87.0%
Minibatch loss at step 5350: 0.304404
Minibatch accuracy: 93.8%
Validation accuracy: 86.9%
Minibatch loss at step 5400: 0.539259
Minibatch accuracy: 87.5%
Validation accuracy: 86.7%
Minibatch loss at step 5450: 0.668616
Minibatch accuracy: 81.2%
Validation accuracy: 87.1%
Minibatch loss at step 5500: 0.777650
Minibatch accuracy: 81.2%
Validation accuracy: 86.9%
Minibatch loss at step 5550: 0.566293
Minibatch accuracy: 87.5%
Validation accuracy: 86.3%
Minibatch loss at step 5600: 0.576272
Minibatch accuracy: 87.5%
Validation accuracy: 86.9%
Minibatch loss at step 5650: 0.572772
Minibatch accuracy: 75.0%
Validation accuracy: 87.2%
Minibatch loss at step 5700: 0.795984
Minibatch accuracy: 75.0%
Validation accuracy: 87.2%
Minibatch loss at step 5750: 1.100185
Minibatch accuracy: 75.0%
Validation accuracy: 86.9%
Minibatch loss at step 5800: 0.618431
Minibatch accuracy: 87.5%
Validation accuracy: 87.3%
Minibatch loss at step 5850: 0.333045
Minibatch accuracy: 100.0%
Validation accuracy: 87.6%
Minibatch loss at step 5900: 0.742232
Minibatch accuracy: 81.2%
Validation accuracy: 87.2%
Minibatch loss at step 5950: 0.973624
Minibatch accuracy: 75.0%
Validation accuracy: 87.2%
Minibatch loss at step 6000: 0.468864
Minibatch accuracy: 87.5%
Validation accuracy: 87.0%
Test accuracy: 73.8%
#+end_example



***** 2

learning rate_decay + dropout 1
best so far: 74.9 at 9001 steps
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64

  graph = tf.Graph()

  with graph.as_default():
    batch = tf.Variable(0)
    rate_param = .01
    global_step = batch*batch_size
    decay_steps=len(train_dataset)  
    decay_rate=.95
    learning_rate =  tf.train.exponential_decay(
      learning_rate=rate_param, 
      global_step=global_step, 
      decay_steps=decay_steps,  
      decay_rate=decay_rate, 
      staircase=True)
    keep_prob = tf.placeholder(tf.float32)
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), keep_prob)*(1/keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
    # Optimizer.
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :var nsteps = 9001 :session a4aspy :results output
  num_steps = nsteps

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(num_steps):
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
      _, l, predictions = session.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 50 == 0):
        print('Minibatch loss at step %d: %f' % (step, l))
        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
Initialized
Minibatch loss at step 0: 10.217341
Minibatch accuracy: 0.0%
Validation accuracy: 12.5%
Minibatch loss at step 50: 1.901007
Minibatch accuracy: 31.2%
Validation accuracy: 25.3%
Minibatch loss at step 100: 1.716664
Minibatch accuracy: 25.0%
Validation accuracy: 61.2%
Minibatch loss at step 150: 1.404220
Minibatch accuracy: 50.0%
Validation accuracy: 67.6%
Minibatch loss at step 200: 1.162590
Minibatch accuracy: 68.8%
Validation accuracy: 72.9%
Minibatch loss at step 250: 0.774594
Minibatch accuracy: 87.5%
Validation accuracy: 74.3%
Minibatch loss at step 300: 1.180029
Minibatch accuracy: 56.2%
Validation accuracy: 78.2%
Minibatch loss at step 350: 0.922411
Minibatch accuracy: 75.0%
Validation accuracy: 79.1%
Minibatch loss at step 400: 0.535455
Minibatch accuracy: 87.5%
Validation accuracy: 77.9%
Minibatch loss at step 450: 0.979195
Minibatch accuracy: 75.0%
Validation accuracy: 79.3%
Minibatch loss at step 500: 1.241119
Minibatch accuracy: 87.5%
Validation accuracy: 79.9%
Minibatch loss at step 550: 0.855008
Minibatch accuracy: 68.8%
Validation accuracy: 79.4%
Minibatch loss at step 600: 1.293591
Minibatch accuracy: 56.2%
Validation accuracy: 80.8%
Minibatch loss at step 650: 0.607245
Minibatch accuracy: 87.5%
Validation accuracy: 81.8%
Minibatch loss at step 700: 0.797803
Minibatch accuracy: 68.8%
Validation accuracy: 81.5%
Minibatch loss at step 750: 0.823959
Minibatch accuracy: 81.2%
Validation accuracy: 80.0%
Minibatch loss at step 800: 1.169000
Minibatch accuracy: 62.5%
Validation accuracy: 81.9%
Minibatch loss at step 850: 0.701765
Minibatch accuracy: 81.2%
Validation accuracy: 82.7%
Minibatch loss at step 900: 1.354591
Minibatch accuracy: 68.8%
Validation accuracy: 82.2%
Minibatch loss at step 950: 0.554118
Minibatch accuracy: 87.5%
Validation accuracy: 82.8%
Minibatch loss at step 1000: 0.597194
Minibatch accuracy: 81.2%
Validation accuracy: 82.7%
Minibatch loss at step 1050: 1.066285
Minibatch accuracy: 56.2%
Validation accuracy: 82.7%
Minibatch loss at step 1100: 0.738086
Minibatch accuracy: 68.8%
Validation accuracy: 82.9%
Minibatch loss at step 1150: 0.745425
Minibatch accuracy: 75.0%
Validation accuracy: 83.0%
Minibatch loss at step 1200: 0.552734
Minibatch accuracy: 87.5%
Validation accuracy: 83.2%
Minibatch loss at step 1250: 0.436012
Minibatch accuracy: 87.5%
Validation accuracy: 82.8%
Minibatch loss at step 1300: 1.073946
Minibatch accuracy: 62.5%
Validation accuracy: 83.1%
Minibatch loss at step 1350: 1.081193
Minibatch accuracy: 75.0%
Validation accuracy: 82.8%
Minibatch loss at step 1400: 0.724149
Minibatch accuracy: 75.0%
Validation accuracy: 84.0%
Minibatch loss at step 1450: 0.873476
Minibatch accuracy: 81.2%
Validation accuracy: 84.0%
Minibatch loss at step 1500: 0.512037
Minibatch accuracy: 87.5%
Validation accuracy: 82.9%
Minibatch loss at step 1550: 1.176669
Minibatch accuracy: 56.2%
Validation accuracy: 83.1%
Minibatch loss at step 1600: 0.134104
Minibatch accuracy: 93.8%
Validation accuracy: 83.8%
Minibatch loss at step 1650: 0.358381
Minibatch accuracy: 87.5%
Validation accuracy: 83.2%
Minibatch loss at step 1700: 0.208498
Minibatch accuracy: 93.8%
Validation accuracy: 84.2%
Minibatch loss at step 1750: 0.720953
Minibatch accuracy: 87.5%
Validation accuracy: 84.1%
Minibatch loss at step 1800: 0.895711
Minibatch accuracy: 81.2%
Validation accuracy: 83.9%
Minibatch loss at step 1850: 0.881983
Minibatch accuracy: 75.0%
Validation accuracy: 84.2%
Minibatch loss at step 1900: 0.697529
Minibatch accuracy: 87.5%
Validation accuracy: 84.7%
Minibatch loss at step 1950: 0.646642
Minibatch accuracy: 75.0%
Validation accuracy: 83.8%
Minibatch loss at step 2000: 0.734193
Minibatch accuracy: 81.2%
Validation accuracy: 83.5%
Minibatch loss at step 2050: 0.332985
Minibatch accuracy: 87.5%
Validation accuracy: 84.3%
Minibatch loss at step 2100: 0.718663
Minibatch accuracy: 75.0%
Validation accuracy: 84.3%
Minibatch loss at step 2150: 1.062643
Minibatch accuracy: 68.8%
Validation accuracy: 84.3%
Minibatch loss at step 2200: 0.406806
Minibatch accuracy: 87.5%
Validation accuracy: 84.6%
Minibatch loss at step 2250: 0.600188
Minibatch accuracy: 75.0%
Validation accuracy: 84.4%
Minibatch loss at step 2300: 0.368519
Minibatch accuracy: 87.5%
Validation accuracy: 84.3%
Minibatch loss at step 2350: 0.815262
Minibatch accuracy: 75.0%
Validation accuracy: 84.5%
Minibatch loss at step 2400: 1.009128
Minibatch accuracy: 62.5%
Validation accuracy: 84.8%
Minibatch loss at step 2450: 0.442663
Minibatch accuracy: 93.8%
Validation accuracy: 84.8%
Minibatch loss at step 2500: 0.334077
Minibatch accuracy: 93.8%
Validation accuracy: 84.6%
Minibatch loss at step 2550: 0.755665
Minibatch accuracy: 75.0%
Validation accuracy: 85.3%
Minibatch loss at step 2600: 0.492959
Minibatch accuracy: 87.5%
Validation accuracy: 85.3%
Minibatch loss at step 2650: 0.515493
Minibatch accuracy: 81.2%
Validation accuracy: 85.1%
Minibatch loss at step 2700: 0.659037
Minibatch accuracy: 75.0%
Validation accuracy: 84.1%
Minibatch loss at step 2750: 0.521527
Minibatch accuracy: 81.2%
Validation accuracy: 85.3%
Minibatch loss at step 2800: 0.975360
Minibatch accuracy: 62.5%
Validation accuracy: 85.3%
Minibatch loss at step 2850: 0.497808
Minibatch accuracy: 87.5%
Validation accuracy: 84.9%
Minibatch loss at step 2900: 0.412503
Minibatch accuracy: 87.5%
Validation accuracy: 85.6%
Minibatch loss at step 2950: 1.354580
Minibatch accuracy: 62.5%
Validation accuracy: 85.6%
Minibatch loss at step 3000: 0.706733
Minibatch accuracy: 81.2%
Validation accuracy: 85.3%
Minibatch loss at step 3050: 0.131155
Minibatch accuracy: 100.0%
Validation accuracy: 85.8%
Minibatch loss at step 3100: 0.582790
Minibatch accuracy: 75.0%
Validation accuracy: 85.5%
Minibatch loss at step 3150: 0.518745
Minibatch accuracy: 81.2%
Validation accuracy: 86.0%
Minibatch loss at step 3200: 0.538316
Minibatch accuracy: 87.5%
Validation accuracy: 85.5%
Minibatch loss at step 3250: 0.565349
Minibatch accuracy: 81.2%
Validation accuracy: 85.5%
Minibatch loss at step 3300: 0.448692
Minibatch accuracy: 87.5%
Validation accuracy: 85.9%
Minibatch loss at step 3350: 0.604608
Minibatch accuracy: 81.2%
Validation accuracy: 85.6%
Minibatch loss at step 3400: 0.779694
Minibatch accuracy: 68.8%
Validation accuracy: 86.2%
Minibatch loss at step 3450: 0.637949
Minibatch accuracy: 75.0%
Validation accuracy: 86.1%
Minibatch loss at step 3500: 0.536732
Minibatch accuracy: 81.2%
Validation accuracy: 85.9%
Minibatch loss at step 3550: 0.497536
Minibatch accuracy: 87.5%
Validation accuracy: 86.0%
Minibatch loss at step 3600: 0.180090
Minibatch accuracy: 93.8%
Validation accuracy: 86.1%
Minibatch loss at step 3650: 0.551766
Minibatch accuracy: 75.0%
Validation accuracy: 85.9%
Minibatch loss at step 3700: 0.293851
Minibatch accuracy: 87.5%
Validation accuracy: 86.2%
Minibatch loss at step 3750: 0.263068
Minibatch accuracy: 93.8%
Validation accuracy: 86.4%
Minibatch loss at step 3800: 0.370420
Minibatch accuracy: 87.5%
Validation accuracy: 86.3%
Minibatch loss at step 3850: 0.886709
Minibatch accuracy: 81.2%
Validation accuracy: 86.2%
Minibatch loss at step 3900: 0.801355
Minibatch accuracy: 75.0%
Validation accuracy: 86.4%
Minibatch loss at step 3950: 0.679693
Minibatch accuracy: 68.8%
Validation accuracy: 86.4%
Minibatch loss at step 4000: 0.473770
Minibatch accuracy: 87.5%
Validation accuracy: 86.4%
Minibatch loss at step 4050: 0.379127
Minibatch accuracy: 81.2%
Validation accuracy: 86.3%
Minibatch loss at step 4100: 0.555311
Minibatch accuracy: 87.5%
Validation accuracy: 86.5%
Minibatch loss at step 4150: 0.574208
Minibatch accuracy: 75.0%
Validation accuracy: 86.8%
Minibatch loss at step 4200: 0.284687
Minibatch accuracy: 87.5%
Validation accuracy: 86.3%
Minibatch loss at step 4250: 0.497705
Minibatch accuracy: 81.2%
Validation accuracy: 86.8%
Minibatch loss at step 4300: 0.485333
Minibatch accuracy: 81.2%
Validation accuracy: 86.8%
Minibatch loss at step 4350: 0.833850
Minibatch accuracy: 68.8%
Validation accuracy: 86.8%
Minibatch loss at step 4400: 0.446757
Minibatch accuracy: 87.5%
Validation accuracy: 87.0%
Minibatch loss at step 4450: 1.207085
Minibatch accuracy: 81.2%
Validation accuracy: 86.5%
Minibatch loss at step 4500: 0.298299
Minibatch accuracy: 93.8%
Validation accuracy: 86.5%
Minibatch loss at step 4550: 0.553604
Minibatch accuracy: 81.2%
Validation accuracy: 86.5%
Minibatch loss at step 4600: 0.263351
Minibatch accuracy: 87.5%
Validation accuracy: 87.0%
Minibatch loss at step 4650: 0.752121
Minibatch accuracy: 75.0%
Validation accuracy: 86.7%
Minibatch loss at step 4700: 0.480976
Minibatch accuracy: 81.2%
Validation accuracy: 87.0%
Minibatch loss at step 4750: 0.502001
Minibatch accuracy: 75.0%
Validation accuracy: 86.6%
Minibatch loss at step 4800: 0.729118
Minibatch accuracy: 81.2%
Validation accuracy: 86.8%
Minibatch loss at step 4850: 0.508766
Minibatch accuracy: 75.0%
Validation accuracy: 86.7%
Minibatch loss at step 4900: 0.735236
Minibatch accuracy: 75.0%
Validation accuracy: 86.8%
Minibatch loss at step 4950: 0.504930
Minibatch accuracy: 81.2%
Validation accuracy: 86.8%
Minibatch loss at step 5000: 0.235487
Minibatch accuracy: 93.8%
Validation accuracy: 86.9%
Minibatch loss at step 5050: 0.572293
Minibatch accuracy: 81.2%
Validation accuracy: 86.5%
Minibatch loss at step 5100: 0.599076
Minibatch accuracy: 75.0%
Validation accuracy: 86.8%
Minibatch loss at step 5150: 0.353266
Minibatch accuracy: 93.8%
Validation accuracy: 87.0%
Minibatch loss at step 5200: 0.332135
Minibatch accuracy: 93.8%
Validation accuracy: 86.9%
Minibatch loss at step 5250: 1.320656
Minibatch accuracy: 68.8%
Validation accuracy: 86.7%
Minibatch loss at step 5300: 0.508076
Minibatch accuracy: 81.2%
Validation accuracy: 87.0%
Minibatch loss at step 5350: 0.095383
Minibatch accuracy: 100.0%
Validation accuracy: 87.1%
Minibatch loss at step 5400: 0.458464
Minibatch accuracy: 87.5%
Validation accuracy: 87.2%
Minibatch loss at step 5450: 0.468286
Minibatch accuracy: 93.8%
Validation accuracy: 87.0%
Minibatch loss at step 5500: 0.755360
Minibatch accuracy: 68.8%
Validation accuracy: 87.0%
Minibatch loss at step 5550: 0.486609
Minibatch accuracy: 87.5%
Validation accuracy: 86.9%
Minibatch loss at step 5600: 0.441597
Minibatch accuracy: 87.5%
Validation accuracy: 87.4%
Minibatch loss at step 5650: 0.321139
Minibatch accuracy: 87.5%
Validation accuracy: 87.5%
Minibatch loss at step 5700: 0.588963
Minibatch accuracy: 81.2%
Validation accuracy: 87.2%
Minibatch loss at step 5750: 0.787152
Minibatch accuracy: 68.8%
Validation accuracy: 87.5%
Minibatch loss at step 5800: 0.511678
Minibatch accuracy: 75.0%
Validation accuracy: 87.4%
Minibatch loss at step 5850: 0.140539
Minibatch accuracy: 100.0%
Validation accuracy: 87.5%
Minibatch loss at step 5900: 0.580482
Minibatch accuracy: 75.0%
Validation accuracy: 87.5%
Minibatch loss at step 5950: 0.838468
Minibatch accuracy: 75.0%
Validation accuracy: 86.9%
Minibatch loss at step 6000: 0.574959
Minibatch accuracy: 87.5%
Validation accuracy: 87.1%
Minibatch loss at step 6050: 0.497272
Minibatch accuracy: 81.2%
Validation accuracy: 87.5%
Minibatch loss at step 6100: 0.305750
Minibatch accuracy: 87.5%
Validation accuracy: 87.3%
Minibatch loss at step 6150: 0.446855
Minibatch accuracy: 81.2%
Validation accuracy: 87.4%
Minibatch loss at step 6200: 0.413655
Minibatch accuracy: 87.5%
Validation accuracy: 87.4%
Minibatch loss at step 6250: 0.255216
Minibatch accuracy: 93.8%
Validation accuracy: 87.6%
Minibatch loss at step 6300: 0.328991
Minibatch accuracy: 87.5%
Validation accuracy: 87.4%
Minibatch loss at step 6350: 0.399737
Minibatch accuracy: 87.5%
Validation accuracy: 86.7%
Minibatch loss at step 6400: 0.576837
Minibatch accuracy: 81.2%
Validation accuracy: 87.4%
Minibatch loss at step 6450: 0.768810
Minibatch accuracy: 68.8%
Validation accuracy: 87.3%
Minibatch loss at step 6500: 0.480283
Minibatch accuracy: 81.2%
Validation accuracy: 87.3%
Minibatch loss at step 6550: 0.468112
Minibatch accuracy: 81.2%
Validation accuracy: 87.6%
Minibatch loss at step 6600: 0.551207
Minibatch accuracy: 87.5%
Validation accuracy: 87.3%
Minibatch loss at step 6650: 1.118600
Minibatch accuracy: 75.0%
Validation accuracy: 87.8%
Minibatch loss at step 6700: 0.230128
Minibatch accuracy: 100.0%
Validation accuracy: 87.9%
Minibatch loss at step 6750: 0.541451
Minibatch accuracy: 87.5%
Validation accuracy: 87.6%
Minibatch loss at step 6800: 0.484172
Minibatch accuracy: 81.2%
Validation accuracy: 87.8%
Minibatch loss at step 6850: 0.575396
Minibatch accuracy: 87.5%
Validation accuracy: 87.8%
Minibatch loss at step 6900: 0.619090
Minibatch accuracy: 81.2%
Validation accuracy: 88.0%
Minibatch loss at step 6950: 0.337633
Minibatch accuracy: 87.5%
Validation accuracy: 87.9%
Minibatch loss at step 7000: 0.413715
Minibatch accuracy: 87.5%
Validation accuracy: 87.8%
Minibatch loss at step 7050: 0.450452
Minibatch accuracy: 81.2%
Validation accuracy: 87.7%
Minibatch loss at step 7100: 0.713609
Minibatch accuracy: 68.8%
Validation accuracy: 87.6%
Minibatch loss at step 7150: 0.137439
Minibatch accuracy: 100.0%
Validation accuracy: 87.9%
Minibatch loss at step 7200: 0.365590
Minibatch accuracy: 93.8%
Validation accuracy: 87.9%
Minibatch loss at step 7250: 0.360435
Minibatch accuracy: 93.8%
Validation accuracy: 87.8%
Minibatch loss at step 7300: 0.444241
Minibatch accuracy: 87.5%
Validation accuracy: 87.7%
Minibatch loss at step 7350: 0.911179
Minibatch accuracy: 81.2%
Validation accuracy: 88.0%
Minibatch loss at step 7400: 0.839497
Minibatch accuracy: 81.2%
Validation accuracy: 87.7%
Minibatch loss at step 7450: 0.285658
Minibatch accuracy: 93.8%
Validation accuracy: 87.6%
Minibatch loss at step 7500: 0.108055
Minibatch accuracy: 93.8%
Validation accuracy: 88.1%
Minibatch loss at step 7550: 0.417158
Minibatch accuracy: 81.2%
Validation accuracy: 88.0%
Minibatch loss at step 7600: 0.867456
Minibatch accuracy: 68.8%
Validation accuracy: 87.8%
Minibatch loss at step 7650: 0.182858
Minibatch accuracy: 87.5%
Validation accuracy: 88.2%
Minibatch loss at step 7700: 0.351644
Minibatch accuracy: 81.2%
Validation accuracy: 88.0%
Minibatch loss at step 7750: 0.480466
Minibatch accuracy: 87.5%
Validation accuracy: 87.6%
Minibatch loss at step 7800: 0.405093
Minibatch accuracy: 93.8%
Validation accuracy: 87.8%
Minibatch loss at step 7850: 0.559737
Minibatch accuracy: 81.2%
Validation accuracy: 87.9%
Minibatch loss at step 7900: 0.809748
Minibatch accuracy: 81.2%
Validation accuracy: 88.1%
Minibatch loss at step 7950: 0.433778
Minibatch accuracy: 81.2%
Validation accuracy: 87.8%
Minibatch loss at step 8000: 0.118903
Minibatch accuracy: 100.0%
Validation accuracy: 87.6%
Minibatch loss at step 8050: 0.025950
Minibatch accuracy: 100.0%
Validation accuracy: 87.9%
Minibatch loss at step 8100: 0.304847
Minibatch accuracy: 87.5%
Validation accuracy: 87.9%
Minibatch loss at step 8150: 0.402380
Minibatch accuracy: 93.8%
Validation accuracy: 88.2%
Minibatch loss at step 8200: 0.407174
Minibatch accuracy: 81.2%
Validation accuracy: 87.9%
Minibatch loss at step 8250: 0.085110
Minibatch accuracy: 100.0%
Validation accuracy: 87.9%
Minibatch loss at step 8300: 0.260617
Minibatch accuracy: 93.8%
Validation accuracy: 88.1%
Minibatch loss at step 8350: 0.105973
Minibatch accuracy: 100.0%
Validation accuracy: 87.7%
Minibatch loss at step 8400: 0.802626
Minibatch accuracy: 87.5%
Validation accuracy: 88.0%
Minibatch loss at step 8450: 0.191482
Minibatch accuracy: 93.8%
Validation accuracy: 88.0%
Minibatch loss at step 8500: 0.292776
Minibatch accuracy: 93.8%
Validation accuracy: 88.1%
Minibatch loss at step 8550: 0.414011
Minibatch accuracy: 81.2%
Validation accuracy: 88.0%
Minibatch loss at step 8600: 0.328727
Minibatch accuracy: 87.5%
Validation accuracy: 88.2%
Minibatch loss at step 8650: 0.349144
Minibatch accuracy: 87.5%
Validation accuracy: 88.4%
Minibatch loss at step 8700: 0.104163
Minibatch accuracy: 93.8%
Validation accuracy: 88.0%
Minibatch loss at step 8750: 0.515413
Minibatch accuracy: 68.8%
Validation accuracy: 88.0%
Minibatch loss at step 8800: 0.275915
Minibatch accuracy: 87.5%
Validation accuracy: 88.2%
Minibatch loss at step 8850: 0.353553
Minibatch accuracy: 100.0%
Validation accuracy: 88.0%
Minibatch loss at step 8900: 0.324793
Minibatch accuracy: 87.5%
Validation accuracy: 88.2%
Minibatch loss at step 8950: 0.556808
Minibatch accuracy: 81.2%
Validation accuracy: 88.2%
Minibatch loss at step 9000: 0.622893
Minibatch accuracy: 75.0%
Validation accuracy: 88.3%
Test accuracy: 75.3%
#+end_example



***** 1

only dropout
#+BEGIN_SRC python :session a4aspy :results output
  batch_size = 16
  patch_size = 5
  depth = 16
  num_hidden = 64

  graph = tf.Graph()

  with graph.as_default():
    keep_prob = tf.placeholder(tf.float32)
    # Input data.
    tf_train_dataset = tf.placeholder(
      tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)
    # Variables.
    layer1_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, num_channels, depth], stddev=0.1))
    layer1_biases = tf.Variable(tf.zeros([depth]))
    layer2_weights = tf.Variable(tf.truncated_normal(
      [patch_size, patch_size, depth, depth], stddev=0.1))
    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))
    layer3_weights = tf.Variable(tf.truncated_normal(
      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))
    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))
    layer4_weights = tf.Variable(tf.truncated_normal(
      [num_hidden, num_labels], stddev=0.1))
    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))
    # Model.
    def model(data, keep_prob):
      conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer1_biases)
      # IK: add pooling
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1], 
                              strides=[1,2,2,1], padding='SAME')
      # adjust convolution stride and add pooling stride
      conv = tf.nn.conv2d(h_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')
      hidden = tf.nn.relu(conv + layer2_biases)
      h_pool = tf.nn.max_pool(hidden, ksize = [1,2,2,1],
                              strides=[1,2,2,1], padding='SAME')
      shape = h_pool.get_shape().as_list()
      reshape = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])
      # apply dropout to fully connected layer
      hidden = tf.nn.dropout(
        tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), 
        keep_prob)*(1/keep_prob)
      return tf.matmul(hidden, layer4_weights) + layer4_biases
    # Training computation.
    logits = model(tf_train_dataset,.6)
    loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(logits,tf_train_labels))
    # Optimizer.
    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)
    # Predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.))
    test_prediction = tf.nn.softmax(model(tf_test_dataset,1.))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC python :var nsteps = 6001 :session a4aspy :results output
  num_steps = nsteps

  with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    print('Initialized')
    for step in range(num_steps):
      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
      batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
      batch_labels = train_labels[offset:(offset + batch_size), :]
      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
      _, l, predictions = session.run(
        [optimizer, loss, train_prediction], feed_dict=feed_dict)
      if (step % 50 == 0):
        print('Minibatch loss at step %d: %f' % (step, l))
        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
        print('Validation accuracy: %.1f%%' % accuracy(
          valid_prediction.eval(), valid_labels))
    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0)
Initialized
Minibatch loss at step 0: 7.703359
Minibatch accuracy: 12.5%
Validation accuracy: 12.5%
Minibatch loss at step 50: 1.453293
Minibatch accuracy: 56.2%
Validation accuracy: 52.5%
Minibatch loss at step 100: 1.111036
Minibatch accuracy: 68.8%
Validation accuracy: 71.6%
Minibatch loss at step 150: 1.992369
Minibatch accuracy: 43.8%
Validation accuracy: 72.9%
Minibatch loss at step 200: 0.988855
Minibatch accuracy: 62.5%
Validation accuracy: 68.2%
Minibatch loss at step 250: 0.770999
Minibatch accuracy: 62.5%
Validation accuracy: 79.7%
Minibatch loss at step 300: 0.957460
Minibatch accuracy: 75.0%
Validation accuracy: 80.2%
Minibatch loss at step 350: 0.683521
Minibatch accuracy: 68.8%
Validation accuracy: 79.8%
Minibatch loss at step 400: 0.679164
Minibatch accuracy: 75.0%
Validation accuracy: 80.8%
Minibatch loss at step 450: 0.421687
Minibatch accuracy: 87.5%
Validation accuracy: 82.0%
Minibatch loss at step 500: 1.042448
Minibatch accuracy: 81.2%
Validation accuracy: 81.8%
Minibatch loss at step 550: 1.006495
Minibatch accuracy: 62.5%
Validation accuracy: 82.5%
Minibatch loss at step 600: 0.988384
Minibatch accuracy: 68.8%
Validation accuracy: 83.6%
Minibatch loss at step 650: 0.580334
Minibatch accuracy: 75.0%
Validation accuracy: 83.2%
Minibatch loss at step 700: 0.474393
Minibatch accuracy: 81.2%
Validation accuracy: 83.0%
Minibatch loss at step 750: 0.829427
Minibatch accuracy: 87.5%
Validation accuracy: 84.2%
Minibatch loss at step 800: 0.748837
Minibatch accuracy: 75.0%
Validation accuracy: 84.0%
Minibatch loss at step 850: 0.637818
Minibatch accuracy: 75.0%
Validation accuracy: 85.2%
Minibatch loss at step 900: 0.636577
Minibatch accuracy: 81.2%
Validation accuracy: 84.9%
Minibatch loss at step 950: 0.739790
Minibatch accuracy: 87.5%
Validation accuracy: 85.1%
Minibatch loss at step 1000: 0.494076
Minibatch accuracy: 87.5%
Validation accuracy: 85.5%
Minibatch loss at step 1050: 0.667214
Minibatch accuracy: 68.8%
Validation accuracy: 85.2%
Minibatch loss at step 1100: 0.536276
Minibatch accuracy: 75.0%
Validation accuracy: 85.7%
Minibatch loss at step 1150: 0.717775
Minibatch accuracy: 68.8%
Validation accuracy: 85.6%
Minibatch loss at step 1200: 0.551628
Minibatch accuracy: 75.0%
Validation accuracy: 85.4%
Minibatch loss at step 1250: 0.560189
Minibatch accuracy: 81.2%
Validation accuracy: 86.1%
Minibatch loss at step 1300: 1.119643
Minibatch accuracy: 75.0%
Validation accuracy: 85.8%
Minibatch loss at step 1350: 0.511195
Minibatch accuracy: 75.0%
Validation accuracy: 85.8%
Minibatch loss at step 1400: 0.600486
Minibatch accuracy: 75.0%
Validation accuracy: 86.8%
Minibatch loss at step 1450: 1.538319
Minibatch accuracy: 68.8%
Validation accuracy: 85.5%
Minibatch loss at step 1500: 0.542604
Minibatch accuracy: 87.5%
Validation accuracy: 85.8%
Minibatch loss at step 1550: 1.144598
Minibatch accuracy: 75.0%
Validation accuracy: 85.7%
Minibatch loss at step 1600: 0.149813
Minibatch accuracy: 93.8%
Validation accuracy: 86.7%
Minibatch loss at step 1650: 0.222449
Minibatch accuracy: 93.8%
Validation accuracy: 86.7%
Minibatch loss at step 1700: 0.268436
Minibatch accuracy: 87.5%
Validation accuracy: 86.5%
Minibatch loss at step 1750: 0.598422
Minibatch accuracy: 93.8%
Validation accuracy: 86.9%
Minibatch loss at step 1800: 1.087819
Minibatch accuracy: 75.0%
Validation accuracy: 87.1%
Minibatch loss at step 1850: 0.790796
Minibatch accuracy: 81.2%
Validation accuracy: 86.8%
Minibatch loss at step 1900: 1.074700
Minibatch accuracy: 75.0%
Validation accuracy: 87.4%
Minibatch loss at step 1950: 0.631297
Minibatch accuracy: 68.8%
Validation accuracy: 86.8%
Minibatch loss at step 2000: 0.620251
Minibatch accuracy: 81.2%
Validation accuracy: 86.9%
Minibatch loss at step 2050: 0.281859
Minibatch accuracy: 100.0%
Validation accuracy: 86.8%
Minibatch loss at step 2100: 0.664190
Minibatch accuracy: 81.2%
Validation accuracy: 86.4%
Minibatch loss at step 2150: 0.816639
Minibatch accuracy: 81.2%
Validation accuracy: 86.7%
Minibatch loss at step 2200: 0.431455
Minibatch accuracy: 81.2%
Validation accuracy: 87.3%
Minibatch loss at step 2250: 0.430422
Minibatch accuracy: 93.8%
Validation accuracy: 87.3%
Minibatch loss at step 2300: 0.440268
Minibatch accuracy: 87.5%
Validation accuracy: 87.0%
Minibatch loss at step 2350: 0.733630
Minibatch accuracy: 81.2%
Validation accuracy: 87.3%
Minibatch loss at step 2400: 0.785089
Minibatch accuracy: 75.0%
Validation accuracy: 87.2%
Minibatch loss at step 2450: 0.339766
Minibatch accuracy: 93.8%
Validation accuracy: 87.4%
Minibatch loss at step 2500: 0.277850
Minibatch accuracy: 93.8%
Validation accuracy: 86.8%
Minibatch loss at step 2550: 0.562286
Minibatch accuracy: 81.2%
Validation accuracy: 87.5%
Minibatch loss at step 2600: 0.442142
Minibatch accuracy: 81.2%
Validation accuracy: 87.2%
Minibatch loss at step 2650: 0.587847
Minibatch accuracy: 75.0%
Validation accuracy: 87.6%
Minibatch loss at step 2700: 0.537113
Minibatch accuracy: 75.0%
Validation accuracy: 85.6%
Minibatch loss at step 2750: 0.551168
Minibatch accuracy: 81.2%
Validation accuracy: 87.9%
Minibatch loss at step 2800: 0.851702
Minibatch accuracy: 68.8%
Validation accuracy: 87.7%
Minibatch loss at step 2850: 0.342875
Minibatch accuracy: 87.5%
Validation accuracy: 87.9%
Minibatch loss at step 2900: 0.192347
Minibatch accuracy: 93.8%
Validation accuracy: 88.0%
Minibatch loss at step 2950: 0.734541
Minibatch accuracy: 81.2%
Validation accuracy: 87.3%
Minibatch loss at step 3000: 0.727084
Minibatch accuracy: 81.2%
Validation accuracy: 87.8%
Minibatch loss at step 3050: 0.219189
Minibatch accuracy: 100.0%
Validation accuracy: 87.8%
Minibatch loss at step 3100: 0.514300
Minibatch accuracy: 81.2%
Validation accuracy: 87.5%
Minibatch loss at step 3150: 0.204189
Minibatch accuracy: 93.8%
Validation accuracy: 88.0%
Minibatch loss at step 3200: 0.398616
Minibatch accuracy: 81.2%
Validation accuracy: 86.7%
Minibatch loss at step 3250: 0.365987
Minibatch accuracy: 87.5%
Validation accuracy: 88.0%
Minibatch loss at step 3300: 0.232628
Minibatch accuracy: 93.8%
Validation accuracy: 88.0%
Minibatch loss at step 3350: 0.408626
Minibatch accuracy: 81.2%
Validation accuracy: 87.8%
Minibatch loss at step 3400: 0.328191
Minibatch accuracy: 93.8%
Validation accuracy: 88.3%
Minibatch loss at step 3450: 0.641322
Minibatch accuracy: 81.2%
Validation accuracy: 88.3%
Minibatch loss at step 3500: 0.337487
Minibatch accuracy: 93.8%
Validation accuracy: 88.2%
Minibatch loss at step 3550: 0.638126
Minibatch accuracy: 75.0%
Validation accuracy: 87.7%
Minibatch loss at step 3600: 0.178808
Minibatch accuracy: 93.8%
Validation accuracy: 88.4%
Minibatch loss at step 3650: 0.275481
Minibatch accuracy: 93.8%
Validation accuracy: 88.3%
Minibatch loss at step 3700: 0.418931
Minibatch accuracy: 93.8%
Validation accuracy: 88.2%
Minibatch loss at step 3750: 0.327169
Minibatch accuracy: 87.5%
Validation accuracy: 88.5%
Minibatch loss at step 3800: 0.219871
Minibatch accuracy: 100.0%
Validation accuracy: 88.1%
Minibatch loss at step 3850: 0.544495
Minibatch accuracy: 81.2%
Validation accuracy: 88.5%
Minibatch loss at step 3900: 0.600975
Minibatch accuracy: 75.0%
Validation accuracy: 88.5%
Minibatch loss at step 3950: 0.456590
Minibatch accuracy: 87.5%
Validation accuracy: 88.8%
Minibatch loss at step 4000: 0.620138
Minibatch accuracy: 75.0%
Validation accuracy: 88.8%
Minibatch loss at step 4050: 0.348265
Minibatch accuracy: 87.5%
Validation accuracy: 88.1%
Minibatch loss at step 4100: 0.189196
Minibatch accuracy: 93.8%
Validation accuracy: 88.1%
Minibatch loss at step 4150: 0.659856
Minibatch accuracy: 75.0%
Validation accuracy: 88.3%
Minibatch loss at step 4200: 0.118239
Minibatch accuracy: 93.8%
Validation accuracy: 88.5%
Minibatch loss at step 4250: 0.532288
Minibatch accuracy: 81.2%
Validation accuracy: 88.6%
Minibatch loss at step 4300: 0.591780
Minibatch accuracy: 81.2%
Validation accuracy: 88.7%
Minibatch loss at step 4350: 0.559559
Minibatch accuracy: 75.0%
Validation accuracy: 88.9%
Minibatch loss at step 4400: 0.370801
Minibatch accuracy: 81.2%
Validation accuracy: 88.2%
Minibatch loss at step 4450: 1.050965
Minibatch accuracy: 81.2%
Validation accuracy: 88.4%
Minibatch loss at step 4500: 0.293047
Minibatch accuracy: 87.5%
Validation accuracy: 88.7%
Minibatch loss at step 4550: 0.455999
Minibatch accuracy: 81.2%
Validation accuracy: 88.1%
Minibatch loss at step 4600: 0.297718
Minibatch accuracy: 87.5%
Validation accuracy: 89.0%
Minibatch loss at step 4650: 0.224106
Minibatch accuracy: 87.5%
Validation accuracy: 88.8%
Minibatch loss at step 4700: 0.474958
Minibatch accuracy: 81.2%
Validation accuracy: 88.8%
Minibatch loss at step 4750: 0.587448
Minibatch accuracy: 81.2%
Validation accuracy: 89.0%
Minibatch loss at step 4800: 0.506016
Minibatch accuracy: 87.5%
Validation accuracy: 88.4%
Minibatch loss at step 4850: 0.504431
Minibatch accuracy: 81.2%
Validation accuracy: 87.7%
Minibatch loss at step 4900: 0.515334
Minibatch accuracy: 75.0%
Validation accuracy: 88.6%
Minibatch loss at step 4950: 0.238837
Minibatch accuracy: 93.8%
Validation accuracy: 88.6%
Minibatch loss at step 5000: 0.197913
Minibatch accuracy: 93.8%
Validation accuracy: 88.7%
Minibatch loss at step 5050: 0.963532
Minibatch accuracy: 75.0%
Validation accuracy: 88.3%
Minibatch loss at step 5100: 0.866537
Minibatch accuracy: 75.0%
Validation accuracy: 87.8%
Minibatch loss at step 5150: 0.384256
Minibatch accuracy: 87.5%
Validation accuracy: 88.5%
Minibatch loss at step 5200: 0.224600
Minibatch accuracy: 93.8%
Validation accuracy: 88.9%
Minibatch loss at step 5250: 0.969593
Minibatch accuracy: 75.0%
Validation accuracy: 88.3%
Minibatch loss at step 5300: 0.274430
Minibatch accuracy: 87.5%
Validation accuracy: 88.9%
Minibatch loss at step 5350: 0.047597
Minibatch accuracy: 100.0%
Validation accuracy: 88.5%
Minibatch loss at step 5400: 0.226244
Minibatch accuracy: 93.8%
Validation accuracy: 88.8%
Minibatch loss at step 5450: 0.533304
Minibatch accuracy: 87.5%
Validation accuracy: 88.6%
Minibatch loss at step 5500: 0.730091
Minibatch accuracy: 81.2%
Validation accuracy: 88.6%
Minibatch loss at step 5550: 0.301287
Minibatch accuracy: 93.8%
Validation accuracy: 89.2%
Minibatch loss at step 5600: 0.360858
Minibatch accuracy: 87.5%
Validation accuracy: 88.0%
Minibatch loss at step 5650: 0.387872
Minibatch accuracy: 87.5%
Validation accuracy: 89.4%
Minibatch loss at step 5700: 0.525682
Minibatch accuracy: 75.0%
Validation accuracy: 89.3%
Minibatch loss at step 5750: 0.696057
Minibatch accuracy: 68.8%
Validation accuracy: 89.1%
Minibatch loss at step 5800: 0.403799
Minibatch accuracy: 87.5%
Validation accuracy: 89.1%
Minibatch loss at step 5850: 0.081617
Minibatch accuracy: 100.0%
Validation accuracy: 89.1%
Minibatch loss at step 5900: 0.429070
Minibatch accuracy: 75.0%
Validation accuracy: 88.8%
Minibatch loss at step 5950: 0.934973
Minibatch accuracy: 68.8%
Validation accuracy: 88.4%
Minibatch loss at step 6000: 0.315617
Minibatch accuracy: 87.5%
Validation accuracy: 88.0%
Test accuracy: 75.1%
#+end_example


*** TODO notes
- add learning rate
